<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A post on Asymmetric Numeral Systems coding">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Lossless Compression with Asymmetric Numeral Systems | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../model-explanability-with-shapley-additive-explanations-shap/" title="Model Explainability with SHapley Additive exPlanations (SHAP)" type="text/html">
<link rel="next" href="../lossless-compression-with-latent-variable-models-using-bits-back-coding/" title="Lossless Compression with Latent Variable Models using Bits-Back Coding" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Lossless Compression with Asymmetric Numeral Systems">
<meta property="og:url" content="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/">
<meta property="og:description" content="A post on Asymmetric Numeral Systems coding">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-09-26T10:37:43-04:00">
<meta property="article:tag" content="Arithmetic Coding">
<meta property="article:tag" content="asymmetric numeral systems">
<meta property="article:tag" content="compression">
<meta property="article:tag" content="entropy">
<meta property="article:tag" content="Huffman coding">
<meta property="article:tag" content="mathjax">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Lossless Compression with Asymmetric Numeral Systems</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2020-09-26T10:37:43-04:00" itemprop="datePublished" title="2020-09-26 10:37">2020-09-26 10:37</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>During my undergraduate days, one of the most interesting courses I took was on
coding and compression.  Here was a course that combined algorithms,
probability and secret messages, what's not to like? <a class="footnote-reference brackets" href="#id2" id="id1">1</a> I ended up not going
down that career path, at least partially because communications systems had
its heyday around the 2000s with companies like Nortel and Blackberry and its
predecessors (some like to joke that all the major theoretical breakthroughs
were done by Shannon and his discovery of information theory around 1950).  Fortunately, I
eventually wound up studying industrial applications of classical AI techniques
and then machine learning, which has really grown like crazy in the last 10
years or so.  Which is exactly why I was so surprised that a <em>new</em> and <em>better</em>
method of lossless compression was developed in 2009 <em>after</em> I finished my
undergraduate degree when I was well into my PhD.  It's a bit mind boggling that
something as well-studied as entropy-based lossless compression still had
(have?) totally new methods to discover, but I digress.</p>
<p>In this post, I'm going to write about a relatively new entropy based encoding
method called Asymmetrical Numeral Systems (ANS) developed by Jaroslaw (Jarek)
Duda [2].  If you've ever heard of Arithmetic Coding (probably best known for
its use in JPEG compression), ANS runs in a very similar vein.  It can
generate codes that are close to the theoretical compression limit
(similar to Arithmetic coding) but is <em>much</em> more efficient.  It's been used in
modern compression algorithms since 2014 including compressors developed
by Facebook, Apple and Google [3].  As usual, I'm going to go over some
background, some math, some examples to help with intuition, and finally some
experiments with a toy ANS implementation I wrote.  I hope you're as
excited as I am, let's begin!</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Background: Data Coding and Compression </h4>
<p>Most modern digital communication is done via transmitting <strong>bits</strong>
(i.e. 0/1 values) with the goals of both reliability and efficiency.
A <strong>symbol</strong> is unit of information that is transmitted, which can be the bits itself
or a higher level concept represented by a sequence of bits (e.g. "a", "b", "c", "d" etc.).
An <strong>alphabet</strong> is the set of all symbols that you can transmit.
A <strong>message</strong> is a sequence of symbols that you want to transmit.</p>
<p>Often you will want to <strong>code</strong> a message (such as a file) for storage or
transmission on a communication channel by specifying some rules on how to
transform it into another form.  The usual reasons why you want to code a message is
compression (fewer bits), redundancy (error correction/detection), or
encryption (confidentiality).  For compression, we generally have two main
approaches: lossy and lossless.  In lossy schemes, you drop some non-essential
details in the message (think image compression) to trade-off a greater
reduction in size.  This trick is used extensively in things like image
(e.g. JPEG) or audio (e.g. MP3) compression.  Lossless schemes on the other
hand aim to retain the exact message reducing the file size by exploiting some
statistical redundancies in the data.  We're mainly talking about lossless
compression schemes today.</p>
<p>Lossless schemes come in many forms such as
<a class="reference external" href="https://en.wikipedia.org/wiki/Run-length_encoding">Run Length Encoding</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/LZ77_and_LZ78">Lempel-Ziv compression</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a>, and
<a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_coding">Arithmetic coding</a>,
which probably constitute the most popular ones (aside from ANS, which we'll be
discussing today).  Most of the ones above work by reading one or more symbols
(think bytes) from the data stream and replacing it with some compact bit
representation.  For example, in Huffman Coding, the most frequent symbol is
replaced with a single bit, or Run Length Encoding, which replaces a repeated
sequence of a character by the character and how many times it repeats.  In both
these examples, it works on a subset of the sequence and replaces them.  The
other variant which both Arithmetic coding and Asymmetrical Numeral Systems
fall under is where then <em>entire</em> message is encoded as a single number (the
smaller or less precise the number, the shorter the message).  This allows you
to get closer to the theoretical compression limit.</p>
<p>Speaking of theoretical compression limits, according to Shannon's
<a class="reference external" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">source coding theorem</a>
the theoretical limit you can (losslessly) compress data is equal
to its entropy.  In other words, the average number of bits per symbol of your
message cannot be smaller than:</p>
<div class="math">
\begin{equation*}
H(X) = -\sum_{i=1}^n p_i \log_2(p_i)  \tag{1}
\end{equation*}
</div>
<p>where it's presumed that you know the <span class="math">\(p_i\)</span> distribution of each of your <span class="math">\(n\)</span> symbols
ahead of time.  Note that the logarithm is base 2, which naturally allows us to
talk in terms of "bits".
I wrote some details on how to think about entropy in my previous post on
<a class="reference external" href="../maximum-entropy-distributions/">maximum entropy distributions</a> so I won't
go into much detail now.</p>
<div class="admonition admonition-example-1-entropy-of-a-discrete-probability-distribution">
<p class="admonition-title">Example 1: Entropy of a Discrete Probability Distribution</p>
<p>Imagine we have an alphabet with 3 symbols: <span class="math">\(\{a, b, c\}\)</span>.
Let random variable <span class="math">\(X\)</span> represent the probability of seeing a symbol
in a message, and is given by the distribution:
<span class="math">\(p_a = \frac{4}{7}, p_b=\frac{2}{7}, p_c=\frac{1}{7}\)</span>.</p>
<p>The entropy and minimum average number of bits per symbol we can achieve
for messages with this distribution (according the the source coding
theorem) is:</p>
<div class="math">
\begin{align*}
H(X) &amp;= -\sum_{i=1}^n p_i \log_2(p_i)  \\
     &amp;= -p_a\log_2(p_a)  - p_b\log_2(p_b) - p_c\log_2(p_c) \\
     &amp;= -\frac{4}{7}\log_2(\frac{4}{7})
        - \frac{2}{7}\log_2(\frac{2}{7})
        - \frac{1}{7}\log_2(\frac{1}{7}) \\
     &amp;\approx 1.3788 bits \\
     \tag{2}
\end{align*}
</div>
<p>Contrast that to naively encoding each symbol using 2-bits (vs. 1.3788),
for example, representing "a" as 00, "b" as 01, and "c" as 10 (leaving 11
unassigned).</p>
<p>This can also be contrasted to assuming that we had a uniform distribution
(<span class="math">\(p_a=p_b=p_c=\frac{1}{3}\)</span>), which would yield us an entropy of
<span class="math">\(H(X)=-3\cdot\frac{1}{3}log_2(\frac{1}{3}) = 1.5850\)</span> vs. 1.3788 with
a more skewed distribution.  This shows a larger idea that uniform
distributions are the "hardest" to compress (i.e. have the highest entropy)
because you can't really exploit any asymmetry in the symbol distribution
-- all of them are equally likely.</p>
</div>
<p>One class of lossless compression schemes is called
<a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_encoding">entropy encoders</a> and they
exploit the estimated statistical properties of your message in order to get
pretty close to the theoretical compression limit.  Huffman coding, Arithmetic
coding, and Asymmetric Numeral Systems all are entropy encoders.</p>
<p>Finally, the metric we'll be using is compression ratio, defined as:</p>
<div class="math">
\begin{equation*}
\text{Compression Ratio} = \frac{Uncompressed Size}{Compressed Size}
\end{equation*}
</div>
<p><br></p>
<h4> Asymmetric Numeral Systems </h4>
<p>Asymmetric Numeral Systems (ANS) is a entropy encoding method used in data
compression developed by Jaroslaw Duda [2] in 2009.  It has a really simple
idea: take a message as a sequence of symbols and <em>encode it as a single
natural number</em> <span class="math">\(x\)</span>.
If <span class="math">\(x\)</span> is small, it requires fewer bits to represent; if <span class="math">\(x\)</span> is
large, then it requires more bits to represent.  Or to think about it the other
way, if I can exploit the statistical properties of my message so that: (a) the
most likely messages get mapped to small natural numbers, and (b) the least likely
messages get mapped to larger natural numbers, then I will have achieved good
compression.  Let's explore this idea a bit more.</p>
<p></p>
<h5> Encoding a Binary String to a Natural Number </h5>
<p>First off, let's discuss how we can even map a sequence of symbols to a natural
number.  We can start with the simplest case: a sequence of binary symbols (0s and 1s).
We all know how to convert a binary string to a natural number, but let's break
it down into its fundamental parts.  We are particularly interested in how
to <em>incrementally</em> build up to the natural number by reading one bit at a time.</p>
<p>Suppose we have already converted some binary string <span class="math">\(b_1 b_2 b_3 \ldots b_i\)</span>
(<span class="math">\(b_1\)</span> being the most significant digit) into a natural number
<span class="math">\(x_i\)</span> via the typical method of converting (unsigned) binary numbers to
natural numbers.  If we get another another binary digit <span class="math">\(b_{i+1}\)</span>, we
want to derive a coding function such that <span class="math">\(x_{i+1} = C(x_i, b_{i+1})\)</span>
generates the natural number representation of <span class="math">\(b_1 b_2 b_3 \ldots b_{i+1}\)</span>.
If you remember your discrete math courses, it should really just be
multiplying the original number by 2 (shifting up a digit in binary), and then
adding the new binary digit, which is just:</p>
<div class="math">
\begin{equation*}
C(x_i, b_{i+1}) := 2x_i + b_{i+1} \tag{3}
\end{equation*}
</div>
<p>If we start with <span class="math">\(x_0=0\)</span>, you can see that we'll be able to convert any
binary string iteratively (from MSB to LSB) to its natural number
representation.  Inversely, we can convert from any natural number to
iteratively recover both the binary digit <span class="math">\(b_{i+1}\)</span> and the next
resulting natural number without that digit using the following decoding
function:</p>
<div class="math">
\begin{equation*}
(x_i, b_{i+1}) = D(x_{i+1}) := (\lfloor\frac{x_{i+1}}{2}\rfloor, x_{i+1} \bmod 2) \tag{4}
\end{equation*}
</div>
<p>Nothing really new here but let's make a few observations:</p>
<ul class="simple">
<li><p>We shouldn't start with <span class="math">\(x_0=0\)</span>, because we won't be able to
distinguish between "0", "00", "000" etc. because they all map to <span class="math">\(0\)</span>.
Instead, let's start at <span class="math">\(x_0=1\)</span>, which effectively adds a leading "1"
to each message we generate but now "0" and "00" can be distinguished as
("10" and "100").</p></li>
<li><p>Let's look at how we're using <span class="math">\(b_{i+1}\)</span>.  In Equation 3, if
<span class="math">\(b_{i+1}\)</span> is odd, then we add 1, else if even we add 0.  In Equation 4,
we're doing the reverse, if the number <span class="math">\(x_{i+1}\)</span> is odd, we know we can
recover a "1", else when even, we recover an "0".  We'll use this idea in
order to extend to more complicated cases.</p></li>
<li><p>Finally, the encoding using Equations 3 and 4 are optimal if we have a uniform
distribution of "0"s and "1"s (i.e. <span class="math">\(p_0=p_1=\frac{1}{2}\)</span>).  Notice
that the entropy <span class="math">\(H(x) = -2 \cdot \frac{1}{2}\log_2(\frac{1}{2}) = 1\)</span>,
which results in 1 bit per binary digit, which is exactly what these
equations generate  (if you exclude the fact that we start at 1).</p></li>
</ul>
<p>The last two points are relevant because it gives us a hint as to how we might
extend this to non-uniform binary messages.  Our encoding is optimal because we
were able to spread the evens and odds (over any given range) in proportion to
their probability.  We'll explore this idea a bit more in the next section.</p>
<div class="admonition admonition-example-2-encoding-a-binary-string-to-from-a-natural-number">
<p class="admonition-title">Example 2: Encoding a Binary String to/from a Natural Number</p>
<p>Using Equation 3 and 4, let's convert binary string
<span class="math">\(b_1 b_2 b_3 b_4 b_5 = 10011\)</span> to a natural number.
Starting with <span class="math">\(x_0=1\)</span>, we have:</p>
<div class="math">
\begin{align*}
x_1 &amp;= C(x_0, b_1) = 2x_0 + b_1 = 2(1) + 1 = 3 \\
x_2 &amp;= C(x_1, b_2) = 2x_1 + b_2 = 2(3) + 0 = 6 \\
x_3 &amp;= C(x_2, b_3) = 2x_2 + b_3 = 2(6) + 0 = 12 \\
x_4 &amp;= C(x_3, b_4) = 2x_3 + b_4 = 2(12) + 1 = 25 \\
x_5 &amp;= C(x_4, b_5) = 2x_4 + b_5 = 2(25) + 1 = 51 \\
\tag{5}
\end{align*}
</div>
<p>To recover our original message, we can use <span class="math">\(D(x_{i+1})\)</span>:</p>
<div class="math">
\begin{align*}
(x_4, b_5) &amp;= D(x_5) = (\lfloor\frac{x_{5}}{2}\rfloor, x_{5} \bmod 2) =
    (\lfloor \frac{51}{2} \rfloor, 51 \bmod 2) = (25, 1) \\
(x_3, b_4) &amp;= D(x_4) = (\lfloor\frac{x_{4}}{2}\rfloor, x_{4} \bmod 2) =
    (\lfloor \frac{25}{2} \rfloor, 25 \bmod 2) = (12, 1) \\
(x_2, b_3) &amp;= D(x_3) = (\lfloor\frac{x_{3}}{2}\rfloor, x_{3} \bmod 2) =
    (\lfloor \frac{12}{2} \rfloor, 12 \bmod 2) = (6, 0) \\
(x_1, b_2) &amp;= D(x_2) = (\lfloor\frac{x_{2}}{2}\rfloor, x_{2} \bmod 2) =
    (\lfloor \frac{6}{2} \rfloor, 6 \bmod 2) = (3, 0) \\
(x_0, b_1) &amp;= D(x_1) = (\lfloor\frac{x_{1}}{2}\rfloor, x_{1} \bmod 2) =
    (\lfloor \frac{3}{2} \rfloor, 3 \bmod 2) = (1, 1) \\
\tag{6}
\end{align*}
</div>
<p>Notice that we recovered our original message in the reverse order.
The number of bits needed to represent our natural number is <span class="math">\(\lceil
\log_2(51) \rceil = 6\)</span> bits, which is just 1 bit above our ideal entropy of
5 bits (assuming a uniform distribution).</p>
</div>
<p></p>
<h5> Redefining the Odds (and Evens) </h5>
<p>Let's think about why the naive encoding in the previous section might result
in an optimal code for a uniform distribution.  For one, it spreads even and odd
numbers (binary strings ending in "0"'s and "1"'s respectively) uniformly across
any natural number range.  This kind of makes sense since they are uniformly
distributed.  What's the analogy for a non-uniform distribution?</p>
<p>If we were going to map a non-uniform distribution with
<span class="math">\(p_1=p &lt; 1-p = p_0\)</span>, then we would want the more frequent symbol
(0 in this case) to appear more often in any given mapped natural number range.
More precisely, we would want even numbers to be mapped in a given range
roughly <span class="math">\(\frac{1-p}{p}\)</span> more often than odd numbers.  Or stated another
way, in a given mapped natural number range from <span class="math">\([1, N]\)</span> we would want
to see roughly <span class="math">\(N\cdot p\)</span> evens and <span class="math">\(N\cdot (1-p)\)</span> odds.
This is the right intuition but doesn't really show how it might generate an
optimal code.  Let's work backwards from an optimal compression scheme and
figure out what we would need.</p>
<p>We are trying to define the encoding function <span class="math">\(x_{i+1} = C(x_i, b_{i+1})\)</span>
(similarly to Equation 3) such that each incremental bit generates the minimal
amount of entropy.  Assuming that <span class="math">\(x_i\)</span> has <span class="math">\(\log_2 (x_i)\)</span> bits of
information, and we want to encode <span class="math">\(b_{i+1}\)</span> optimally with
<span class="math">\(-\log_2(p_{b_{i+1}})\)</span> bits, we have (with a bit of abuse of entropy notation):</p>
<div class="math">
\begin{align*}
H(x_{i+1}) &amp;= H(C_{\text{opt}}(x_i, b_{i+1})) \\
           &amp;= H(x_i) + H(b_{i+1})\\
           &amp;= \log_2(x_i) - \log_2(p_{b_{i+1}})\\
           &amp;= \log_2(\frac{x_i}{p_{b_{i+1}}}) \\
&amp;\implies C_{\text{opt}}(x_i, b_{i+1}) \approx \frac{x_i}{p_{b_{i+1}}}
    \tag{8}
\end{align*}
</div>
<p>Therefore, if we can define <span class="math">\(C(x_i, b_{i+1}) \approx \frac{x_i}{p_{b_{i+1}}}\)</span>
then we will have achieved an optimal code!  Let's try to understand what this
mapping means.</p>
<p>From Equation 8, if we are starting at some <span class="math">\(x_i\)</span> and get a new bit
<span class="math">\(b_{i+1}=1\)</span> (an odd number), then <span class="math">\(x_{i+1}\approx\frac{x_i}{p}\)</span>.
But we know <span class="math">\(x_i\)</span> can be any natural number, so this implies that
odd numbers will be placed at (roughly), <span class="math">\(\frac{1}{p}, \frac{2}{p},
\frac{3}{p}, \ldots\)</span> intervals for any given natural number.
This also means, we'll see an odd number
(roughly) every <span class="math">\(\frac{1}{p}\)</span> natural numbers.
But if we take a closer look, this is precisely the condition of having roughly
<span class="math">\(N\cdot p\)</span> for the first <span class="math">\(N\)</span> natural numbers (<span class="math">\(\text{# of
Odds} = N / \frac{1}{p} = N\cdot p\)</span>).  Similarly, we'll see even numbers
(roughly) every <span class="math">\(\frac{1}{1-p}\)</span>, which also means we'll see (roughly)
<span class="math">\(N \cdot (1-p)\)</span> in the first <span class="math">\(N\)</span> natural numbers.  So our intuition
does lead us towards the solution of an optimal code after all!</p>
<div class="figure align-center">
<img alt="Distribution of Evens and Odds for Various :math:`p`" src="../../images/ans_even_odd.png" style="height: 200px;"><p class="caption"><strong>Figure 1: Distribution of Evens and Odds for Various :math:`p`</strong></p>
</div>
<p>Thinking about this code a bit differently, we are essentially redefining the
frequency of evens and odds with this new mapping.  We can see this more
clearly in Figure 1.  For different values of <span class="math">\(p\)</span>, we can see a repeating
pattern of where the evens and odds fall.  When <span class="math">\(p=1/2\)</span>, we see an
alternating pattern (never mind that <span class="math">\(2\)</span> is mapped to an odd, this is an
unimportant quirk of the implementation) as we usually expect.  However,
when we go to non-uniform distributions, we can see repeating but
non-alternating patterns.  One thing you may notice is that the above equations
are in <span class="math">\(\mathbb{R}\)</span> but we need them to mapped to natural numbers!
Figure 1 implicitly does some of the required rounding and we'll see more of
that in the implementations below.</p>
<p>In summary:</p>
<ul class="simple">
<li><p>A binary message encoded and decoded to a single natural number.</p></li>
<li><p>Using this method, we can build an entropy encoder by defining a mapping of
even and odd binary numbers (those ending in "1'/"0"s) in proportion to their
probabilities (<span class="math">\(p, 1-p\)</span>) in a message.</p></li>
<li><p>We can incrementally generate this number bit by bit by using a coding
function <span class="math">\(x_{i+1} = C(x_i, b_{i+1})\)</span>
(decoding function <span class="math">\((x_i, b_{i+1}) = D(x_{i+1})\)</span>) that will
iteratively generate a mapped natural number from (to) the previous mapped number
and the next bit.</p></li>
<li><p>If we can guarantee our coding function <span class="math">\(C(x_i, b_{i+1}) \approx \frac{x_i}{p_{b_{i+1}}}\)</span>
then we will have achieved an optimal code.</p></li>
</ul>
<p></p>
<h5> Uniform Binary Variant (uABS) </h5>
<p>Without loss of generality, let's use a binary alphabet with odds ending in "1" and evens
ending in "0", and <span class="math">\(p_1 = p &lt; 1-p = p_0\)</span> (odds are always less frequent than evens).
We know we want approximately <span class="math">\(N\cdot p\)</span> odd numbers mapped in the first
<span class="math">\(N\)</span> mapped natural numbers.  Since we have to have a non-fractional
number of odds, let's pick <span class="math">\(\lceil N \cdot p \rceil\)</span> odds in the first
<span class="math">\(N\)</span> mapped natural numbers.  From this, we get this relationship for
any given <span class="math">\(N\)</span> and <span class="math">\(N+1\)</span> (try to validate it against Figure 1):</p>
<div class="math">
\begin{equation*}
\lceil (N+1)\cdot p \rceil - \lceil N\cdot p \rceil
= \left\{
    \begin{array}{ll}
        1 &amp;&amp; \text{ if } N \text{ has an odd mapped} \\
        0 &amp;&amp; \text{otherwise} \\
    \end{array}
\right. \tag{9}
\end{equation*}
</div>
<p>Another way to think about it is: if we're at <span class="math">\(N\)</span> and we've filled our <span class="math">\(\lceil N \cdot p\rceil\)</span>
odd number "quota" then we don't need to see another odd at <span class="math">\(N+1\)</span> (the <span class="math">\(0\)</span> case).
Conversely, if going to <span class="math">\(N+1\)</span> makes it so we're behind our odd number
"quota" then we should make sure that we map an odd at <span class="math">\(N\)</span> (the <span class="math">\(1\)</span> case).</p>
<p>Now here's the tricky part: what coding function <span class="math">\(x_{i+1} = C(x_i, b_{i+1})\)</span>
satisfies Equation 9 (where <span class="math">\(x_i\)</span> is our mapped natural number)?
It turns out this one does:</p>
<div class="math">
\begin{equation*}
C(x_i, b_{i+1})
= \left\{
    \begin{array}{ll}
        \lceil \frac{x_i+1}{1-p} \rceil - 1 &amp;&amp; \text{if } b_{i+1} = 0 \\
        \lfloor \frac{x_i}{p} \rfloor &amp;&amp; \text{otherwise} \\
    \end{array}
\right. \tag{10}
\end{equation*}
</div>
<p>I couldn't quite figure out a sensible derivation of why this particular
function works but it's probably non-trivial.  The main problem is
that we're working with natural numbers, so dealing with floor and ceil
operators is tricky.  Additionally, Equation 9 kind of looks like a
some kind of <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_difference_equation">difference equation</a>,
which are generally very difficult to solve.  However, I did manage to
prove that Equation 10 is consistent with Equation 9.  See Appendix A for the
proof.</p>
<p>Using Equation 10, we can now code any binary message using the same method we used
in the previous section with Equation 3: iteratively applying Equation 10 one
bit at a time.  The matching decoding function is essentially the reverse calculation:</p>
<div class="math">
\begin{align*}
(x_i, b_{i+1}) &amp;= D(x_{i+1}) \\
b_{i+1} &amp;= \lceil (x_{i+1}+1)\cdot p \rceil - \lceil x_{i+1}\cdot p \rceil  \\
x_i &amp;= \left\{
    \begin{array}{ll}
        x_{i+1} - \lceil x_{i+1} \cdot p \rceil &amp;&amp; \text{if } b_{i+1} = 0 \\
        \lceil x_{i+1} \cdot p \rceil &amp;&amp; \text{otherwise} \\
    \end{array}
\right. \tag{11}
\end{align*}
</div>
<p>The decoding of a bit is calculated exactly as we have designed it in Equation 9,
and depending on which bit was decoded, we perform the reverse calculation of
Equation 10.  For the <span class="math">\(b_{i+1} = 0\)</span> case, it may not look like the reverse
calculation but the math should work out (haven't proven it, but my
implementation works).  In the end, the equations to encode/decode are straight
forward but the logic of arriving at them is far from it.</p>
<div class="admonition admonition-example-3-encoding-a-binary-string-to-from-a-natural-number-using-uabs">
<p class="admonition-title">Example 3: Encoding a Binary String to/from a Natural Number using uABS</p>
<p>Using the same binary string as Example 2,
<span class="math">\(b_1 b_2 b_3 b_4 b_5 = 10011\)</span>, let's encode it using uABS but
with <span class="math">\(p=\frac{7}{10}\)</span> (recall we assume that <span class="math">\(p=p_1 &lt; p_0=1-p\)</span>).
Using Equation 10 and starting with <span class="math">\(x_0=1\)</span>, we get:</p>
<div class="math">
\begin{align*}
x_1 &amp;= C(x_0, b_1) = \lfloor \frac{x_0}{p} \rfloor = \lfloor 1\cdot \frac{10}{3} \rfloor = 3 \\
x_2 &amp;= C(x_1, b_2) = \lceil \frac{x_i+1}{1-p} \rceil - 1 = \lceil (3+1)\frac{10}{7} \rceil - 1 = 5 \\
x_3 &amp;= C(x_2, b_3) = \lceil \frac{x_i+1}{1-p} \rceil - 1 = \lceil (5+1)\frac{10}{7} \rceil - 1 = 8 \\
x_4 &amp;= C(x_3, b_4) = \lfloor \frac{x_0}{p} \rfloor = \lfloor 8\cdot \frac{10}{3} \rfloor = 26 \\
x_5 &amp;= C(x_4, b_5) = \lfloor \frac{x_0}{p} \rfloor = \lfloor 26\cdot \frac{10}{3} \rfloor = 86 \\
\tag{12}
\end{align*}
</div>
<p>Decoding can be applied in a similar way with Equation 11, which recovers
our original message of "10011" (but in reverse order):</p>
<div class="math">
\begin{align*}
b_5 &amp;= \lceil (x_5+1)\cdot p \rceil - \lceil x_5\cdot p \rceil
     = \lceil (86+1)\cdot \frac{3}{10} \rceil - \lceil 86\cdot \frac{3}{10} \rceil
     = 1 \\
x_4 &amp;= \lceil x_5 \cdot p \rceil
     = \lceil 86\cdot \frac{3}{10} \rceil
     = 26 \\
b_4 &amp;= \lceil (x_4+1)\cdot p \rceil - \lceil x_4\cdot p \rceil
     = \lceil (26+1)\cdot \frac{3}{10} \rceil - \lceil 26\cdot \frac{3}{10} \rceil
     = 1 \\
x_3 &amp;= \lceil x_4 \cdot p \rceil
     = \lceil 26\cdot \frac{3}{10} \rceil
     = 8 \\
b_3 &amp;= \lceil (x_3+1)\cdot p \rceil - \lceil x_3\cdot p \rceil
     = \lceil (8+1)\cdot \frac{3}{10} \rceil - \lceil 8\cdot \frac{3}{10} \rceil
     = 0 \\
x_2 &amp;= x_3 - \lceil x_3 \cdot p \rceil
     = 8 - \lceil 8\cdot \frac{3}{10} \rceil
     = 5 \\
b_2 &amp;= \lceil (x_2+1)\cdot p \rceil - \lceil x_2\cdot p \rceil
     = \lceil (5+1)\cdot \frac{3}{10} \rceil - \lceil 5\cdot \frac{3}{10} \rceil
     = 0 \\
x_1 &amp;= x_2 - \lceil x_2 \cdot p \rceil
     = 5 - \lceil 5\cdot \frac{3}{10} \rceil
     = 3 \\
b_1 &amp;= \lceil (x_1+1)\cdot p \rceil - \lceil x_1\cdot p \rceil
     = \lceil (3+1)\cdot \frac{3}{10} \rceil - \lceil 3\cdot \frac{3}{10} \rceil
     = 1 \\
x_0 &amp;= \lceil x_1 \cdot p \rceil
     = \lceil 3\cdot \frac{3}{10} \rceil
     = 1 \\
\tag{13}
\end{align*}
</div>
<p>Another popular way to visualize this is using a tabular method in Figure 2.
In the top row, we have the same visualization of evens/odds as Figure 1 for <span class="math">\(p=\frac{3}{10}\)</span>,
which is essentially <span class="math">\(C(x_i, b_{i+1})\)</span>.
In the second and third row, it shows which numbers are mapped to
evens/odds and counts the number of "slots" of evens/odds we have see up to
that point.
So for <span class="math">\(C(x_i, b_{i+1})=3\)</span>, it's mapped to the first odd "slot", and
for <span class="math">\(C(x_i, b_{i+1})=26\)</span>, it's mapped to the eighth odd "slot".  The
same thing happens on the even side.</p>
<div class="figure align-center">
<img alt="Tabular Visualization of uABS Encoding" src="../../images/ans_ex3.png" style="height: 150px;"><p class="caption"><strong>Figure 2: Tabular Visualization of uABS Encoding</strong></p>
</div>
<p>This turns out to be precisely what Equation 10 is doing: for any given
<span class="math">\(x_i\)</span> it's trying to find the next even/odd "slot" to put
<span class="math">\(x_{i+1}\)</span> in.
The yellow lines trace out what an encoding for "10011" would look like.
Our current number <span class="math">\(x_i\)</span> along with the incoming bit <span class="math">\(b_{i+1}\)</span>
defines which "slot" we should go in (the diagonal arrows), and Equation 10
calculates the next natural number associated with it (the "up" arrows).
Decoding would follow a similar process but in reverse.</p>
</div>
<p></p>
<h5> Range Variant (rANS) </h5>
<p>We saw that uABS works on a binary alphabet, but we can also apply the same concept
to an alphabet of any size (with some modifications). The first thing to notice
is that that the argument from Equation 8 works (more or less) with <em>any</em>
alphabet, not just binary ones (just replace the bit <span class="math">\(b_{i+1}\)</span> with symbol <span class="math">\(s_{i+1}\)</span>).
That is, adding an incremental symbol (instead of a bit) should only increase
the total entropy of the message by the entropy of that symbol.  Equation 8
would only need to reference symbol and the same logic would work.</p>
<p>Another problem are those pesky real numbers.  Theoretically, we can have
arbitrary real numbers for the probability distribution of our alphabet.  We
"magically" found a nice formula in Equation 10/11 that encodes/decodes any
arbitrary <span class="math">\(p\)</span>, but in the case of a larger alphabet, it's a bit tougher.
Instead, a restriction that we'll place is that we'll quantize the probability
distribution in <span class="math">\(2^n\)</span> chunks.  So <span class="math">\(p_s\approx \frac{f_s}{2^n}\)</span>,
where <span class="math">\(f_s\)</span> is a natural number.
This quantization of the probability distribution, simplifies things for us by
allowing us to have a simpler and more efficient coding/decoding function
(although it's not clear to me if it's possible to do it without quantization).</p>
<p>Instead of our previous idea of evens and odds, what we'll be doing is extending this idea
and "coloring" each number.  So for an alphabet of size 3, we might color
things red, green and blue.  Figure 3 shows a few examples with this alphabet
with <span class="math">\(n=3\)</span> quantization for a few different distributions (this is analogous
to Figure 1).</p>
<div class="figure align-center">
<img alt='Distribution of "blue", "green" and "red" symbols' src="../../images/ans_rans.png" style="height: 200px;"><p class="caption"><strong>Figure 3: Distribution of "blue", "green" and "red" symbols</strong></p>
</div>
<p>So how does it work?  It's not too far off from uABS, we use the following equations to encode/decode:</p>
<div class="math">
\begin{align*}
C(x_i, s_{i+1}) &amp;= \lfloor \frac{x_i}{f_s} \rfloor \cdot 2^n + CDF[s]  \tag{14} + (x_i \bmod f_s)  \\
s_{i+1} &amp;= \text{symbol}(x_{i+1} \bmod 2^n) \text{ such that } CDF[s] \leq x_{i+1} \bmod 2^n &lt; CDF[s+1] \tag{15} \\
x_i = D(x_{i+1}) &amp;= f_s \cdot \lfloor x_{i+1} / 2^n \rfloor - CDF[s] + (x_{i+1} \bmod 2^n) \tag{16}
\end{align*}
</div>
<p>Where <span class="math">\(CDF[s] := f_0 + f_1 + \ldots + f_{s-1}\)</span>, essentially the
cumulative distribution function for a given ordering of the symbols.  You'll notice
that since we've quantized the distribution in terms of powers of 2, we can replace
the multiplications, divisions and modulo with left shifting, right shifting
and logical masking, respectively, which makes this much more efficient computationally.</p>
<p>The intuition for Equation 14-16 isn't too far from from uABS: for a given
<span class="math">\(N\)</span>, we want to maintain the property that we roughly see
<span class="math">\(N\cdot  p_s = N \cdot \frac{f_s}{2^n}\)</span> of symbols <span class="math">\(s\)</span>. Looking
at Equation 14, we can see how it accomplishes this:</p>
<ul class="simple">
<li><p><span class="math">\(\lfloor \frac{x_i}{f_s} \rfloor \cdot 2^n\)</span>: finds the right <span class="math">\(2^n\)</span> range
(recall that we have a repeating pattern every <span class="math">\(2^n\)</span> natural numbers).
If <span class="math">\(f_s\)</span> is small, say <span class="math">\(f_s=1\)</span>, then it only appears once every
<span class="math">\(2^n\)</span> range.  If <span class="math">\(f_s\)</span> is large, then we would expect to see
<span class="math">\(f_s\)</span> numbers mapped to every <span class="math">\(2^n\)</span> range.</p></li>
<li><p><span class="math">\(CDF[s]\)</span> finds the offset within the <span class="math">\(2^n\)</span> range for the current
symbol <span class="math">\(s\)</span> -- all <span class="math">\(s\)</span> symbols will be grouped together within
this range starting here.</p></li>
<li><p><span class="math">\((x_i \bmod f_s)\)</span> finds the precise location within this sub-range
(which has precisely <span class="math">\(f_s\)</span> spaces allocated for it).</p></li>
</ul>
<p>The decoding is basically just the reverse operation of the encoding.</p>
<p>Since we maintain this repeating pattern, we implicitly are maintaining the
property that we'll see <span class="math">\(x_i \cdot p_s\)</span> "<span class="math">\(s\)</span>" symbols within the
first <span class="math">\(x_i\)</span> natural numbers.</p>
<div class="admonition admonition-example-4-encoding-a-ternary-string-to-from-a-natural-number-using-rans">
<p class="admonition-title">Example 4: Encoding a Ternary String to/from a Natural Number using rANS</p>
<p>Using the alphabet ['a', 'b', 'c'] with quantization <span class="math">\(n=3\)</span> and distribution
<span class="math">\([f_a, f_b, f_c]=[5, 2, 1]\)</span> (<span class="math">\(CDF[s] = [0, 5, 7, 8]\)</span>), let's encode the string "abc".
We need to start with <span class="math">\(x_0=8\)</span> or we won't be able to encode repeated
values of 'a' (similar to how we start uABS at 1). In fact, we just need to
start with the <span class="math">\(\max f_s\)</span> but to be safe, we'll use <span class="math">\(2^n\)</span>.
Using Equation 14:</p>
<div class="math">
\begin{align*}
x_1 &amp;= C(x_0, a)
     = \lfloor \frac{x_0}{f_a} \rfloor \cdot 2^3 + CDF[a] + (x_0 \bmod f_a)
     = \lfloor \frac{8}{5} \rfloor \cdot 8 + 0 + (8 \bmod 5)
     = 11 \\
x_2 &amp;= C(x_0, b)
     = \lfloor \frac{x_1}{f_b} \rfloor \cdot 2^3 + CDF[b] + (x_1 \bmod f_b)
     = \lfloor \frac{11}{2} \rfloor \cdot 8 + 5 + (11 \bmod 2)
     = 46  \\
x_3 &amp;= C(x_0, c)
     = \lfloor \frac{x_2}{f_c} \rfloor \cdot 2^3 + CDF[c] + (x_2 \bmod f_c)
     = \lfloor \frac{46}{1} \rfloor \cdot 8 + 7 + (46 \bmod 1)
     = 375 \\
 \tag{17}
\end{align*}
</div>
<p>Decoding, works similarly using Equation 15-16:</p>
<div class="math">
\begin{align*}
s_2 &amp;= \text{symbol}(x_3 \bmod 8) = \text{symbol}(375 \bmod 8) = c \\
x_2 &amp;= D(x_3)
     = f_c \cdot \lfloor x_3 / 8 \rfloor - CDF[c] + (x_3 \bmod 8)
     = 1 \cdot \lfloor 375 / 8 \rfloor - 7  + (375 \bmod 8)
     = 46 \\
s_1 &amp;= \text{symbol}(x_2 \bmod 8) = \text{symbol}(46 \bmod 8) = b \\
x_1 &amp;= D(x_2)
     = f_b \cdot \lfloor x_2 / 8 \rfloor - CDF[b] + (x_2 \bmod 8)
     = 2 \cdot \lfloor 46 / 8 \rfloor - 5  + (46 \bmod 8)
     = 11 \\
s_0 &amp;= \text{symbol}(x_1 \bmod 8) = \text{symbol}(11 \bmod 8) = a \\
x_0 &amp;= D(x_1)
     = f_a \cdot \lfloor x_1 / 8 \rfloor - CDF[a] + (x_1 \bmod 8)
     = 5 \cdot \lfloor 11 / 8 \rfloor - 0 + (11 \bmod 8)
     = 8 \\
\tag{18}
\end{align*}
</div>
<p>We can build the same table as Figure 2 except we'll have four rows:
for <span class="math">\(C(x_i, s_{i+1}), a, b, c\)</span>.  Building the table is left as
an exercise for the reader :)</p>
</div>
<div class="admonition admonition-note-about-the-starting-value-of-math-x-0">
<p class="admonition-title">Note about the starting value of <span class="math">\(x_0\)</span></p>
<p>In Example 4, we started on <span class="math">\(x_0=2^n\)</span>.  This is because if we didn't,
we could get into the situation where we couldn't distinguish certain
repetitions of strings such as: [a, aa, aaa], for example.  Using Example 4,
let's see what we'd get starting with <span class="math">\(x_0=1\)</span>:</p>
<div class="math">
\begin{align*}
x_1 &amp;= C(x_0, a)
     = \lfloor \frac{x_0}{f_a} \rfloor \cdot 2^3 + CDF[a] + (x_0 \bmod f_a)
     = \lfloor \frac{1}{5} \rfloor \cdot 8 + 0 + (1 \bmod 5)
     = 1 \\
x_2 &amp;= C(x_1, a)
     = \lfloor \frac{x_0}{f_a} \rfloor \cdot 2^3 + CDF[a] + (x_0 \bmod f_a)
     = \lfloor \frac{1}{5} \rfloor \cdot 8 + 0 + (1 \bmod 5)
     = 1 \\
x_3 &amp;= C(x_2, a)
     = \lfloor \frac{x_0}{f_a} \rfloor \cdot 2^3 + CDF[a] + (x_0 \bmod f_a)
     = \lfloor \frac{1}{5} \rfloor \cdot 8 + 0 + (1 \bmod 5)
     = 1 \\
 \tag{19}
\end{align*}
</div>
<p>As you can see we get nowhere fast.  The reason is that the first term
always rounds down, resulting in the exact same value.  Similarly, the
second term always resolves the same thing (since 'a' is the first symbol
in our ordering), and the third term as well.</p>
<p>I think (haven't really proven it) that the safest option is to have
<span class="math">\(\max f_s\)</span> as your starting value.  This will ensure that the first
term will always be &gt;= 0, resulting in a different number than you started
with.  To be safe, <span class="math">\(2^n &gt; \max f_s\)</span>, which is just a bit nicer.
In some sense, we're "wasting" the initial numbers here starting <span class="math">\(x_0\)</span>
larger but it's necessary in order to encode repeated strings and handle
these corner cases.</p>
<p>Another way you could go about it, is that do a fixed mapping for the first
<span class="math">\(2^n\)</span> numbers (a base case of sorts), and then from there you can
apply the formula.  I didn't try this but I think that this is also
possible.</p>
</div>
<p></p>
<h5> Renormalization </h5>
<p>The astute reader may have already been wondering how this can work in practice.
It works great when you only have a message of length five or so, but what about a
1 MB file?  If we use a 1-byte=256-length alphabet, we could potentially be
getting a number on the order of <span class="math">\(2^{1000000n}\)</span> over this 1M-length string.
Surely no integer type will be able to efficiently handle that!
It turns out there is a simple trick to ensure that <span class="math">\(x^i \in [2^M, 2^{2M} - 1]\)</span>.</p>
<p>The idea is that during encoding once <span class="math">\(x_i\)</span> gets too big, we simply write
out the lower <span class="math">\(M\)</span> bits to ensure it stays between <span class="math">\([2^M, 2^{2M} - 1]\)</span>
(e.g. <span class="math">\(M=16\)</span> bits).
Similarly, during decoding, if <span class="math">\(x_i\)</span> is too small, shift the current
number up and read in <span class="math">\(M\)</span> bits into the lower bits.  As long as you take
care to make sure each operation is symmetric, it should allow you to always
play with a number that fits within an integer type.</p>
<p><strong>Listing 1: Encoding and Decoding rANS Python Pseudocode with Renormalization</strong></p>
<div class="code"><table class="codetable">
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-1"><code data-line-number=" 1"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-1"></a><span class="n">MASK</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">M</span> <span class="o">-</span> <span class="mi">1</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-2"><code data-line-number=" 2"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-2"></a><span class="n">BOUND</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">M</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-3"><code data-line-number=" 3"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-3"></a>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-4"><code data-line-number=" 4"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-4"></a><span class="c1"># Encoding</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-5"><code data-line-number=" 5"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-5"></a><span class="n">s</span> <span class="o">=</span> <span class="n">readSymbol</span><span class="p">()</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-6"><code data-line-number=" 6"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-6"></a><span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">f</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">&lt;&lt;</span> <span class="n">n</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">%</span> <span class="n">f</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-7"><code data-line-number=" 7"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-7"></a><span class="k">if</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">&gt;</span> <span class="n">BOUND</span><span class="p">):</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-8"><code data-line-number=" 8"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-8"></a>    <span class="n">write16bits</span><span class="p">(</span><span class="n">x</span> <span class="o">&amp;</span> <span class="n">MASK</span><span class="p">)</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-9"><code data-line-number=" 9"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-9"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">M</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-10"><code data-line-number="10"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-10"></a><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">f</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">&lt;&lt;</span> <span class="n">n</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">%</span> <span class="n">f</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-11"><code data-line-number="11"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-11"></a>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-12"><code data-line-number="12"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-12"></a><span class="c1"># Decoding</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-13"><code data-line-number="13"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-13"></a><span class="n">s</span> <span class="o">=</span> <span class="n">symbol</span><span class="p">[</span><span class="n">x</span> <span class="o">&amp;</span> <span class="n">MASK</span><span class="p">]</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-14"><code data-line-number="14"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-14"></a><span class="n">writeSymbol</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-15"><code data-line-number="15"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-15"></a><span class="n">x</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">&amp;</span> <span class="n">MASK</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-16"><code data-line-number="16"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-16"></a><span class="k">if</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">**</span><span class="n">M</span><span class="p">):</span>
</code></td>
</tr>
<tr>
<td class="linenos linenodiv"><a href="#rest_code_f8d6887257414bb19a1bfe1da9f974a9-17"><code data-line-number="17"></code></a></td>
<td class="code"><code><a name="rest_code_f8d6887257414bb19a1bfe1da9f974a9-17"></a>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">&lt;&lt;</span> <span class="n">M</span> <span class="o">+</span> <span class="n">read16bits</span><span class="p">()</span>
</code></td>
</tr>
</table></div>
<p>Listing 1 shows the Python pseudo code for rANS encoding and decoding with
renormalization.  Notice that we use Equation 14-16 but with more efficient
bit-wise operations.</p>
<p></p>
<h5> Other variants </h5>
<p>As you can imagine, there are numerous variants of the above
algorithms/concepts, especially as it relates to efficient implementations.
One of the most practical is one called
<a class="reference external" href="https://en.wikipedia.org/wiki/Asymmetric_numeral_systems#Tabled_variant_(tANS)">tANS</a>
or the tabled variant.  In this variation, we build a finite state machine
(i.e. table) to pre-compute all the calculations we would have done in rANS.
This has a bit more upfront cost but will make the encoding/decoding much faster
without the need for multiplications.</p>
<p>Another extension of tANS is the ability to encrypt the message directly in the
tANS algorithm.  Since we're building a table, we don't really need to maintain
Equation 14-16 but rather can pick any repeating pattern.  So instead of the
typical rANS repeating pattern, we can scramble it based on some random number.
See [1] for more details.</p>
<p><br></p>
<h4> Implementation Details </h4>
<p>I implemented some toy versions of uABS and rANS in Python which you can find on my
<a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/ans">Github</a>.
Surprisingly, it was a bit trickier than I thought due to a few gotchas.
Here are some notes for implementing uABS:</p>
<ul class="simple">
<li><p>Python's integer type is theoretically unlimited but I used some <cite>numpy</cite>
functions, which <em>do</em> have a limited range (64-bit integer).  You can see
this when using uABS with large binary strings, particularly with close to
uniform distributions, where the code encodes/decodes string incorrectly.</p></li>
<li><p>The other "gotcha" is that with uABS, we are actually (sort of) dealing with
real numbers, which is a poor match for floating point data types.  Python's
floating point type definitely <em>has</em> limited precision, so the usual problems
of being not represent real numbers exactly become a problem.  Especially
when we need to apply ceil/floor where a <cite>0.000001</cite> difference is meaningful.
To hack around this, I simply just wrapped everything in Python's <cite>Decimal</cite>
type.</p></li>
<li><p>Finally, to ensure that the smaller <span class="math">\(p\)</span> was always mapped to the "1",
I had do some swapping of the characters and probabilities.</p></li>
</ul>
<p>For rANS, it was a bit easier, except for the renormalization, where I had to
play around a bit:</p>
<ul class="simple">
<li><p>I decided to simplify my life, I would just directly take the frequencies
(<span class="math">\(f_s\)</span>) along with the quantization bits (<span class="math">\(n\)</span>) instead of
introducing errors quantizing things myself.</p></li>
<li><p>As mentioned above, I kept having errors until I figured out that <span class="math">\(x_0\)</span>
needed to start at a large enough value.  With renormalization, I start
it at <span class="math">\(x_0=2^M-1\)</span> since we want <span class="math">\(x_i \in [2^M, 2^{2M}-1]\)</span>.
Turns out you need to have minus one there or else you get into a corner case
where the decoding logic stops decoding early (or at least my implementation did).</p></li>
<li><p>The other thing I had to "figure out" was the <cite>BOUND</cite> in Listing 1.  I initially
thought it was simply just <span class="math">\(2^{2M}-1\)</span> but I was wrong.  In [1], they
reference a <cite>bound[s]</cite> variable that is never defined, so I had to reverse
engineer it.  I'm almost positive there is a better way to do it than what
I have in Listing 1, but I think my way is the most straight forward.</p></li>
<li><p>In the decoding, there is a step where you have to lookup which symbol was
decoded.  I simply used <cite>numpy.argmax</cite>, which I presume does a linear search.
Apparently, this is one place where you can do something smarter but I wasn't
too interested in this part.</p></li>
<li><p>I didn't have to do any of the funny wrapping using <cite>Decimal</cite> that I did with
uABS because of the quantization to <span class="math">\(n\)</span> bits.  There is still a
division and call to <cite>floor()</cite> but since we're dealing with integers in the
division, the chances of causing issues is pretty small I think (at least I
haven't seen it yet).</p></li>
</ul>
<p>Finally, none of my toy implementation, nor the compression values are quite
realistic because you also need to include the encoding of the probability
distribution itself!  Something that you would surely include as metadata in a
file.  However, if we're compression a file that's relatively big, this constant
amount of data <em>should</em> be negligible.</p>
<p><br></p>
<h4> Experiments </h4>
<p>The setup for uABS and rANS experiments were roughly the same.  First, a
random strings of varying length was generated based on the alphabet,
distribution and quantization bits (for rANS).  Next, the compression algorithm
is run against the string and the original message size, ideal (Shannon limit) size,
and actual size were measured or calculated.  For each uABS experiment, each
setting was run with 100 different strings and averaged, while for rANS it was
run 50 times and averaged.</p>
<p>Figure 4 shows the results for uABS where "actual_ratio" stands for compression
ratio.  First off, more skewed distributions (lower <span class="math">\(p\)</span>) result in higher
compression.  This is sensible because we can exploit the fact that odd numbers
appear much more often.  Next, it's clear that as the message length increase,
we get a better compression ratio (closer to ideal).  This expected as the
asymptotic behavior of the code starts paying off.  Interesting, for more
skewed distributions (<span class="math">\(p=0.01\)</span>), it takes much longer message lengths for
us to get close to the theoretical limit.  We would probably need a message
length of <span class="math">\(N * 1 / p\)</span> to start approaching that limit.  Unfortunately, since
I didn't implement renormalization, I couldn't push the message length too much
further since the numbers got too big.</p>
<div class="figure align-center">
<img alt="Experimental Results" src="../../images/ans_uabs.png"><p class="caption"><strong>Figure 4: Experimental Results for uABS (dashed lines are the ideal compression ratio)</strong></p>
</div>
<p>Figure 5 show the first set of results for rANS.  Here we used an 256 character
alphabet (8-bits = 1byte) with 15 quantization bits and 24 renormalization
bits.   Figure 5 shows various distributions for varying message lengths.
Uniform is self explanatory, <cite>power_X</cite> are normalized power distributions with
exponent <span class="math">\(X\)</span>.  We see the same pattern of more skewed distributions
having higher compression and reaching close to theoretical limit with longer
message sizes.</p>
<div class="figure align-center">
<img alt="Experimental Results" src="../../images/rans_msg_len.png"><p class="caption"><strong>Figure 5: Experimental Results for rANS varying message length and distribution (dashed lines are the ideal compression ratio)</strong></p>
</div>
<p>Figure 6 shows an ablation study for rANS on quantization bits.  I held constant
<cite>power_50</cite> distribution with message length 1000 and varied quantization bits
and renormalization bits.  <cite>renormalization_bits = add_renorm_bits + quantization_bits</cite>.
We can see that more precise quantization yields better compression, as
expected.  It can get closer to the actual <cite>power_50</cite> distribution instead of
being a coarse approximation. Varying the renormalization bits relative to quantization
doesn't seem to have much effect in terms of compression ratio (I suspect there's more to
it here but I didn't want to spend too much time investigating it).</p>
<div class="figure align-center">
<img alt="Experimental Results" src="../../images/rans_quantization.png"><p class="caption"><strong>Figure 6: Experimental Results for rANS varying quantization bits and renormalization bits</strong></p>
</div>
<p><br></p>
<h4> Conclusion </h4>
<p>Well this post was definitely another tangent that I went off on.  In fact, the
post I actually wanted to write was ML related but I got side tracked trying to
understand ANS.  It just was so interesting that I thought I should learn it
more in depth and write a post on it.  I keep trying to make more time for
writing on this blog but I always seem to have more and more things keeping me
busy professionally and personally (which is a good thing!).  Anyways, look out
for a future post where I will make reference to ANS.  Thanks for reading!</p>
<p><br></p>
<h4> References </h4>
<ul class="simple">
<li><p>[1] "Lecture I: data compression ... data encoding", Jaroslaw Duda, Nokia Krakow, <a class="reference external" href="http://ww2.ii.uj.edu.pl/~smieja/teaching/ti/3a.pdf">http://ww2.ii.uj.edu.pl/~smieja/teaching/ti/3a.pdf</a></p></li>
<li><p>[2] "Asymmetric numeral systems", Jarek Duda, <a class="reference external" href="https://arxiv.org/abs/0902.0271">https://arxiv.org/abs/0902.0271</a></p></li>
<li><p>[3] Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Asymmetric_numeral_systems">https://en.wikipedia.org/wiki/Asymmetric_numeral_systems</a></p></li>
</ul>
<p><br></p>
<h4> Appendix A: Proof of uABS Coding Function </h4>
<p>(<em>As an aside: I spent longer than I'd like to admit trying to figure out this proof.
It turns out that trying to prove things involving floor and ceil functions wasn't
so obvious for a computer engineer by training.
I tried looking up a bunch of identities and going in circles using modulo
notation without much success.  It was only after going back to the definition
of the floor/ceil operators, did I figure out the proof below.  There's
probably some lesson here about first principles but I'll let you take what you
want from this story.</em>)</p>
<p>Let's start out by assuming that <span class="math">\(p = \frac{a}{b}\)</span> can be represented as
a rational number for some relatively prime <span class="math">\(a, b \in \mathbb{Z}^{+}\)</span>.
Practically, we're working with non-infinite precision, so it's not too big of
a stretch.  To verify that Equation 10 is consistent with Equation 9, we'll use
substitution and show that the two equations are consistent.</p>
<p><strong>Case 1: N is odd</strong></p>
<p>Re-write Equation 9 odd case:</p>
<div class="math">
\begin{align*}
x_{i+1} = \lfloor \frac{x_i}{p} \rfloor = \lfloor \frac{bx_i}{a} \rfloor = \frac{bx_i}{a} - \frac{m}{a} &amp;&amp; \text{for some } 0 \leq m &lt; a, m \in \mathbb{Z} \\
\tag{A.1}
\end{align*}
</div>
<p>Substitute Equation A.1 into Equation 9 (where we're taking <span class="math">\(N=x_{i+1}\)</span>):</p>
<div class="math">
\begin{align*}
\lceil (N+1)\cdot p \rceil - \lceil N\cdot p \rceil
&amp;= \lceil (\frac{bx}{a} - \frac{m}{a} + 1)\frac{a}{b} \rceil - \lceil (\frac{bx}{a} - \frac{m}{a})\frac{a}{b}  \rceil \\
&amp;= \lceil x - \frac{m}{b} + \frac{a}{b} \rceil -  \lceil x - \frac{m}{b} \rceil \\
&amp;= x + \lceil - \frac{m}{b} + \frac{a}{b} \rceil -  x - \lceil - \frac{m}{b} \rceil &amp;&amp; \text{ since } x \in \mathbb{Z} \\
&amp;= \lceil \frac{a-m}{b} \rceil -  \lceil - \frac{m}{b} \rceil  &amp;&amp; \text{ since } 0 \leq m &lt; a &lt; b \\
&amp;= 1 - 0 = 1
\tag{A.2}
\end{align*}
</div>
<p><strong>Case 2: N is even</strong></p>
<p>Substitute Equation 9 and A.3 into Equation 9:</p>
<div class="math">
\begin{align*}
\lceil (N+1)\cdot p \rceil - \lceil N\cdot p \rceil
&amp;= \lceil (\lceil \frac{x+1}{1-p} \rceil - 1 + 1)\cdot p \rceil -
    \lceil (\lceil \frac{x+1}{1-p} \rceil - 1) \cdot p \rceil \\
&amp;= \lceil \lceil \frac{(x+1)b}{b-a} \rceil \cdot \frac{a}{b} \rceil
    - \lceil (\lceil \frac{(x+1)b}{b-a}\rceil - 1) \cdot \frac{a}{b} \rceil \\
&amp;= \lceil (m\cdot (b-a) - \frac{i}{b-a}) \cdot \frac{a}{b} \rceil
    - \lceil (m\cdot (b-a) - \frac{i}{b-a} - 1) \cdot \frac{a}{b} \rceil
    &amp;&amp; \text{for some } 0 \leq i &lt; b-a, i \in \mathbb{Z}; m \in \mathbb{Z} \\
&amp;= m\cdot (b-a) + \lceil - \frac{i}{b-a} \cdot \frac{a}{b} \rceil
    - m\cdot (b-a) - \lceil (- \frac{i}{b-a} - 1) \cdot \frac{a}{b} \rceil
    &amp;&amp; \text{ since } m\cdot(b-a) \in \mathbb{Z} \\
&amp;= \lceil - \frac{i}{b-a} \cdot \frac{a}{b} \rceil
   - \lceil (- \frac{i}{b-a} - 1) \cdot \frac{a}{b} \rceil \\
&amp;= 0 - \lceil (- \frac{i}{b-a} - 1) \cdot \frac{a}{b} \rceil
   &amp;&amp; \text{ since } 0 \leq \frac{i}{b-a} &lt; 1; 0 &lt; \frac{a}{b} &lt; 1 \\
\tag{A.4}
\end{align*}
</div>
<p>Now looking at the last line and looking at the expression in the ceil function,
we can see that:</p>
<div class="math">
\begin{align*}
(- \frac{i}{b-a} - 1) \cdot \frac{a}{b} &amp; &gt; -\frac{2a}{b}
    &amp;&amp; \text {since } \frac{i}{b-a} &lt; 1 \\
&amp;\geq -1
    &amp;&amp; \text {since } \frac{a}{b} \leq 0.5 \text{ using assumption } p=\frac{a}{b} &lt; \frac{b-a}{b} = 1-p \\
\tag{A.5}
\end{align*}
</div>
<p>So <span class="math">\((- \frac{i}{b-a} - 1)\cdot \frac{a}{b} &gt; -1\)</span> (and obviously <span class="math">\(&lt; 0\)</span>), therefore
Equation A.4 resolves to <span class="math">\(- \lceil (- \frac{i}{b-a} - 1) \cdot \frac{a}{b} \rceil = 0\)</span> as required.</p>
<p><br></p>
<h4> Notes </h4>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>I actually liked most of the communication theory courses, it was interesting to learn the basics of how wired/wireless signals are transmitted and how they could be modelled using math.</p>
</dd>
</dl>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/arithmetic-coding/" rel="tag">Arithmetic Coding</a></li>
            <li><a class="tag p-category" href="../../categories/asymmetric-numeral-systems/" rel="tag">asymmetric numeral systems</a></li>
            <li><a class="tag p-category" href="../../categories/compression/" rel="tag">compression</a></li>
            <li><a class="tag p-category" href="../../categories/entropy/" rel="tag">entropy</a></li>
            <li><a class="tag p-category" href="../../categories/huffman-coding/" rel="tag">Huffman coding</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../model-explanability-with-shapley-additive-explanations-shap/" rel="prev" title="Model Explainability with SHapley Additive exPlanations (SHAP)">Previous post</a>
            </li>
            <li class="next">
                <a href="../lossless-compression-with-latent-variable-models-using-bits-back-coding/" rel="next" title="Lossless Compression with Latent Variable Models using Bits-Back Coding">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
