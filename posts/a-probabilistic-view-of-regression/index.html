<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Another look at linear regression through the lens of probability.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>A Probabilistic View of Linear Regression | Bounded Rationality</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/a-probabilistic-view-of-regression/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="Another look at linear regression through the lens of probability.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../normal-approximations-to-the-posterior-distribution/" title="Normal Approximation to the Posterior Distribution" type="text/html">
<link rel="next" href="../beyond-collaborative-filtering/" title="Beyond Collaborative Filtering" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="A Probabilistic View of Linear Regression">
<meta property="og:url" content="http://bjlkeng.github.io/posts/a-probabilistic-view-of-regression/">
<meta property="og:description" content="Another look at linear regression through the lens of probability.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-05-14T20:43:05-04:00">
<meta property="article:tag" content="Bayesian">
<meta property="article:tag" content="logistic">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="Poisson">
<meta property="article:tag" content="probability">
<meta property="article:tag" content="regression">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding data, math, and programming to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">A Probabilistic View of Linear Regression</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2016-05-14T20:43:05-04:00" itemprop="datePublished" title="2016-05-14 20:43">2016-05-14 20:43</time></a></p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>One thing that I always disliked about introductory material to linear
regression is how randomness is explained.  The explanations always
seemed unintuitive because, as I have frequently seen it, they appear as an
after thought rather than the central focus of the model.
In this post, I'm going to try to
take another approach to building an ordinary linear regression model starting
from a probabilistic point of view (which is pretty much just a Bayesian view).
After the general idea is established, I'll modify the model a bit and end up
with a Poisson regression using the exact same principles showing how
generalized linear models aren't any more complicated.  Hopefully, this will
help explain the "randomness" in linear regression in a more intuitive way.</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Background </h4>
<p>The basic idea behind a regression is that you want to model
the relationship between an outcome variable <span class="math">\(y\)</span> (a.k.a dependent
variable, endogenous variable, response variable), and a vector of explanatory
variables <span class="math">\({\bf x} = (x_1, x_2, \ldots, x_n)\)</span> (a.k.a. independent variables,
exogenous variables, covariates, features, or input variables).  A
<a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>
relates <span class="math">\(y\)</span> to a linear predictor function of
<span class="math">\(\bf{x}\)</span> (how they relate is a bit further down).  For a given data point
<span class="math">\(i\)</span>, the linear function is of the form:</p>
<div class="math">
\begin{equation*}
f(i) = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} \tag{1}
\end{equation*}
</div>
<p>Notice that the function is linear in the parameters <span class="math">\({\bf \beta} =
(\beta_0, \beta_1, \ldots, \beta_n)\)</span>, not necessarily in terms of the explanatory variables.
It's possible to use a non-linear function of another explanatory variable as an explanatory variable itself, e.g. <span class="math">\(f(i) = \beta_0 + \beta_1 x_{i} + \beta_2 x^2_{i} + \beta_3 x^3_{i}\)</span>
is a linear predictor function.</p>
<p>There are usually two main reasons to use a regression model:</p>
<ul class="simple">
<li>Predicting a future value of <span class="math">\(y\)</span> given its corresponding explanatory
variables.  An example of this is predicting a student's test scores given
attributes about the students.</li>
<li>Quantifying the strength of the relationship of <span class="math">\(y\)</span> in terms of its
explanatory variables.  An example of this is determining how strongly the
unit sales of a product varies with its price (i.e. price elasticity).</li>
</ul>
<p>The simplest form of linear regression model equates the outcome variable with
the linear predictor function (ordinary linear regression), adding an error
term (<span class="math">\(\varepsilon\)</span>) to model the noise that appears when fitting the
model.  The error term is added because the <span class="math">\(y\)</span> variable almost never can be
exactly determined by <span class="math">\({\bf x}\)</span>, there is always some noise or
uncertainty in the relationship which we want to model.</p>
<div class="math">
\begin{equation*}
y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} + \varepsilon_i \tag{2}
\end{equation*}
</div>
<p>From this equation, most introductory courses will go into estimating the
<span class="math">\(\beta\)</span> parameters using an <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares</a> approach given a set
of <span class="math">\((y_i, {\bf x_i})\)</span> pairs, which then can be used for either prediction
or quantification of strength of the relationship.  Instead of going the
traditional route, let's start from the ground up by specifying the probability
distribution of <span class="math">\(y\)</span> and working our way back up.</p>
<p><br></p>
<h4> Modeling the Outcome as a Normal Distribution </h4>
<p>Instead of starting off with both <span class="math">\(y\)</span> and <span class="math">\(\bf{x}\)</span> variables,
we'll start by describing the probability distribution of <em>just</em> <span class="math">\(y\)</span>
and <em>then</em> introducing the relationship to the explanatory variables.</p>
<p></p>
<h5> A Constant Mean Model </h5>
<p>First, let's model <span class="math">\(y\)</span> as a standard normal distribution with a
zero (i.e. known) mean and unit variance. Note this does <em>not</em> depend any explanatory
variables (no <span class="math">\({\bf x}\)</span>'s anywhere to be seen):</p>
<div class="math">
\begin{equation*}
Y \sim N(0, 1) \tag{3}
\end{equation*}
</div>
<p>In this model for <span class="math">\(y\)</span>, we have nothing to estimate -- all the normal
parameter distribution parameters are already set (mean <span class="math">\(\mu=0\)</span>, variance
<span class="math">\(\sigma^2=1\)</span>).
In the language of linear regression, this model would be represented as
<span class="math">\(y=0 + \varepsilon\)</span> with no dependence on any <span class="math">\({\bf x}\)</span> values and
<span class="math">\(\varepsilon\)</span> being a standard normal distribution.  Please note that
even though <em>on average</em> we expect <span class="math">\(y=0\)</span>, we still a expect certain amount
of fluctuation or randomness about the <span class="math">\(0\)</span>.</p>
<p>Next, let's make it a little bit more interesting by assuming a fixed
<em>unknown</em> mean and variance <span class="math">\(\sigma^2\)</span> corresponding to
<span class="math">\(y=\mu + \varepsilon\)</span> regression model (here <span class="math">\(\varepsilon\)</span> is a
zero mean and <span class="math">\(\sigma^2\)</span> variance):</p>
<div class="math">
\begin{equation*}
Y \sim N(\mu, \sigma^2) \tag{4}
\end{equation*}
</div>
<p>We are still not modeling the relationship between <span class="math">\(y\)</span> and <span class="math">\({\bf
x}\)</span> (bear with me here, we'll get there soon).  In Equation 4, if we're given
a set of <span class="math">\((y_i, {\bf x_i})\)</span>, we can get an unbiased estimate for
<span class="math">\(\mu\)</span> by just using the mean of all the <span class="math">\(y_i\)</span>'s
(we can also estimate <span class="math">\(\sigma^2\)</span> but let's keep it simple for now).
A more round about (but more insightful) way to find this estimate is to
maximize the <a class="reference external" href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a>
function.</p>
<p></p>
<h5> Maximizing Likelihood </h5>
<p>Consider that we have <span class="math">\(n\)</span> points, each of which is drawn in an independent
and identically distributed (i.i.d.) way from the normal distribution in Equation 4.
For a given, <span class="math">\(\mu, \sigma^2\)</span>, the probability of those <span class="math">\(n\)</span> points
being drawn define the likelihood function, which are just the multiplication
of <span class="math">\(n\)</span> normal probability density functions (PDF) (because they are independent).</p>
<div class="math">
\begin{equation*}
\mathcal{L(\mu|y)} = \prod_{i=1}^{n} P_Y(y_i|\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}} \tag{5}
\end{equation*}
</div>
<p>Once we have a likelihood function, a good estimate of the parameters (i.e.
<span class="math">\(\mu, \sigma^2\)</span>) is to just find the combination of parameters that
maximizes this function for the given data points.  In this scenario,
the data points are fixed (we have observed <span class="math">\(n\)</span> of them with known values) and
we are trying to estimate the unknown values for <span class="math">\(\mu\)</span> (or <span class="math">\(\sigma^2\)</span>).
Here we derive the maximum likelihood estimate for <span class="math">\(\mu\)</span>:</p>
<div class="math">
\begin{align*}
\hat{\mu} = \arg\max_\mu  \mathcal{L(\mu|y)} &amp;= \arg\max_\mu \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}} \\
&amp;= \arg\max_\mu \log\big(\prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i-\mu)^2}{2\sigma^2}}\big)\\
&amp;= \arg\max_\mu \sum_{i=1}^{n} \log(\frac{1}{\sigma\sqrt{2\pi}}) + \log(e^{-\frac{(y_i-\mu)^2}{2\sigma^2}}) \\
&amp;= \arg\max_\mu \sum_{i=1}^{n}\log(e^{-\frac{(y_i-\mu)^2}{2\sigma^2}})  \\
&amp;= \arg\max_\mu \sum_{i=1}^{n} -\frac{(y_i-\mu)^2}{2\sigma^2} \\
&amp;= \arg\min_\mu \sum_{i=1}^{n} (y_i-\mu)^2
\tag{6}
\end{align*}
</div>
<p>We use a couple of tricks here.  It turns out maximizing the likelihood is the same as
maximizing the log-likelihood <a class="footnote-reference" href="#id8" id="id1">[1]</a> and it makes the manipulation much easier.
Also, we can remove any additive or multiplicative constants where appropriate
because they do not affect the maximum likelihood value.</p>
<p>To find the actual value of the optimum point, we can take the partial
derivative of Equation 6 with respect to <span class="math">\(\mu\)</span> and set it to zero:</p>
<div class="math">
\begin{align*}
\frac{\partial}{\partial \mu}\log\mathcal{L(\mu|y)} &amp;= 0 \\
\frac{\partial}{\partial \mu}\sum_{i=1}^{n} (y_i-\mu)^2  &amp;= 0 \\
\sum_{i=1}^{n} -2(y_i-\mu)  &amp;= 0 \\
    n\mu = \sum_{i=1}^{n} y_i \\
    \mu = \frac{1}{n}\sum_{i=1}^{n} y_i \tag{7}
\end{align*}
</div>
<p>Which is precisely the mean of the <span class="math">\(y\)</span> values as expected.  Even though
we knew the answer ahead of time, this work will be useful once we complicate
the situation by introducing the explanatory variables.</p>
<p>Finally, the expected value of <span class="math">\(y\)</span> is just the expected value of a normal
distribution, which is just equal its mean:</p>
<div class="math">
\begin{equation*}
E(y) = \mu \tag{8}
\end{equation*}
</div>
<p></p>
<h5> A Couple of Important Ideas </h5>
<p>So far we haven't done anything too interesting.  We've simply looked at how to
estimate a "regression" model <span class="math">\(y=\mu + \varepsilon\)</span>, which simply
relates the outcome variable <span class="math">\(y\)</span> to a constant <span class="math">\(\mu\)</span>.
Another way to write this in terms of Equation 2 would be <span class="math">\(y=\beta_0 + \varepsilon\)</span>,
where we just relabel <span class="math">\(\mu=\beta_0\)</span>.</p>
<p>Before we move on, there are two points that I want to stress that might be easier to
appreciate with this extremely simple "regression".  First, <span class="math">\(y\)</span> is a random variable.
Assuming our model represents the data correctly, when we plot a histogram it
should bell shaped and centered at <span class="math">\(\mu\)</span>.  This is important to
understand because a common misconception with regressions is that <span class="math">\(y\)</span> is
a deterministic function of the <span class="math">\({\bf x}\)</span> (or in this case constant) values.
This confusion probably comes about because the error term <span class="math">\(\varepsilon\)</span>
error term is tacked on at the end of Equation 2 reducing its importance.
In our constant modeling of <span class="math">\(y\)</span>, it would be silly to think of <span class="math">\(y\)</span>
to be exactly equal to <span class="math">\(\mu\)</span> -- it's not.  Rather, the values of <span class="math">\(y\)</span>
are normally distributed around <span class="math">\(\mu\)</span> with <span class="math">\(\mu\)</span> just being the
expected value.</p>
<p>Second, <span class="math">\(\mu = \frac{1}{n}\sum_{i=1}^{n} y_i\)</span> (from Equation 7) is a
<em>point estimate</em>.  We don't know its exact value, whatever we estimate will probably
not be equal to its "true" value (if such a thing exists).  Had we sampled our data
points slightly differently, we would get a slightly different estimate of
<span class="math">\(\mu\)</span>. <em>This all points to the fact that</em> <span class="math">\(\mu\)</span> <em>is a random variable</em>
<a class="footnote-reference" href="#id9" id="id2">[2]</a>.  I won't talk too much more about this point since it's a bit outside
scope for this post but perhaps I'll discuss it in the future.</p>
<p><br></p>
<h4> Modeling Explanatory Variables </h4>
<p>Now that we have an understanding that <span class="math">\(y\)</span> is a random variable, let's
add in some explanatory variables.  We can model the expected value of
<span class="math">\(y\)</span> as a linear function of <span class="math">\(p\)</span> explanatory variables <a class="footnote-reference" href="#id10" id="id3">[3]</a> similar to
Equation 2:</p>
<div class="math">
\begin{equation*}
E(y|{\bf x}) = \beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p} \tag{9}
\end{equation*}
</div>
<p>Combining this Equation 8, the mean of <span class="math">\(y\)</span> is now just this linear
function.  Thus, <span class="math">\(y\)</span> is a normal variable with mean as a linear function
of <span class="math">\({\bf x}\)</span> and a fixed standard deviation:</p>
<div class="math">
\begin{equation*}
y \sim N(\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p}, \sigma^2) \tag{10}
\end{equation*}
</div>
<p>This notation makes it clear that <span class="math">\(y\)</span> is still a random normal variable
with an expected value corresponding to the linear function of <span class="math">\({\bf x}\)</span>.
The problem now is trying to find estimates for the <span class="math">\(p\)</span> <span class="math">\(\beta_i\)</span>
parameters instead of just a single <span class="math">\(\mu\)</span> value.</p>
<p></p>
<h5> Maximizing Likelihood </h5>
<p>To get point estimates for the <span class="math">\(\beta_i\)</span> parameters, we can again use a
maximum likelihood estimate.  Thankfully, the work we did above did not go to
waste as the steps are the same up to Equation 6.  From there, we can substitute
the linear equation from Equation 9 in for <span class="math">\(\mu\)</span> and try to find the maximum
values for the vector of <span class="math">\({\bf \beta}\)</span> values:</p>
<div class="math">
\begin{align*}
{\bf \beta}
&amp;= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i-\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p})^2 \\
&amp;= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i-\hat{y_i})^2
\tag{11}
\end{align*}
</div>
<p>We use the notation <span class="math">\(\hat{y_i} = E(y|{\bf x}, {\bf \beta})\)</span> to denote the
predicted value (or expected value) of <span class="math">\(y\)</span> of our model.  Notice that the
estimate for the <span class="math">\({\bf \beta}\)</span> values in Equation 11 is precisely the
equation for ordinary least squares estimates.</p>
<p>I won't go into detail of how to solve Equation 11 but any of the standard
ideas will work such as a gradient descent or taking partial derivatives with
respect to all the parameters, set them to zero and solve the system of
equations.  There are a huge variety of ways to solve this equation that have
been studied quite extensively.</p>
<p></p>
<h5> Prediction </h5>
<p>Once we have the coefficients for our linear regression from Equation 11, we
can now predict new values.  Given a vector of explanatory variables <span class="math">\({\bf x}\)</span>,
predicting <span class="math">\(y\)</span> is a simple computation:</p>
<div class="math">
\begin{equation*}
\hat{y_i} = E(y_i|{\bf x_i}) = \beta_{i0} + \beta_1 x_{i1} + ... + \beta_p x_{ip} \tag{12}
\end{equation*}
</div>
<p>I included the expectation here to emphasize that we're generating a point
estimate for <span class="math">\(y\)</span>.  The expectation is the most likely value for <span class="math">\(y\)</span>
(according to our model) but our model is really predicting that <span class="math">\(y\)</span>
is most likely a band of values within a few <span class="math">\(\sigma\)</span> of this expectation.
To actually find this range, we would need to estimate <span class="math">\(\sigma\)</span> but it's
a bit outside the scope of this post.</p>
<p>Many times though, a point estimate is good enough and we can use it directly
as a new prediction point.  With classical statistics, you can also derive a
confidence interval or a prediction interval around this point estimate to gain
some insight into the uncertainty of it.  A full Bayesian approach is probably
better though since you'll explicitly state your assumptions (e.g. priors).</p>
<p><br></p>
<h4> Generalized Linear Models (GLM) </h4>
<p>Changing up some of the modeling decision we made above, we get a different
type of regression model that is not any more complicated.
<a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized linear models</a>
are a generalization of the ordinary linear regression model we just looked at above
except that it makes different choices.  Namely, the choice of probability
distribution and choice of how the mean of the outcome variable relate to the
explanatory variables (i.e. "link function").  The above methodology for
deriving ordinary linear regression can be equally applied to any of the
generalized linear models.  We'll take a look at a <a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_regression">Poisson Regression</a> as an example.</p>
<p></p>
<h5> Poisson Regression </h5>
<p>The first big difference between ordinary and Poisson regression is the distribution
of the outcome variable <span class="math">\(y\)</span>.  A Poisson regression uses a
<a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> (duh!)
instead of a normal distribution:</p>
<div class="math">
\begin{align*}
Y \sim Poisson(\lambda) \\
E(Y) = Var(Y) = \lambda \tag{13}
\end{align*}
</div>
<p>The Poisson distribution is a discrete probability distribution with a single
parameter <span class="math">\(\lambda\)</span>.  Since the Poisson regression is discrete,
so is our outcome variable.  Typically, a Poisson regression is used to
represent count data such as the number of letters of mail (or email) in a
day, or perhaps the number of customers walking into a store.</p>
<p>The second difference between ordinary and Poisson regressions is how we relate
the linear function of explanatory variables to the mean of the outcome
variable.  The Poisson regression assumes that the logarithm of the expected
value of the outcome is equal to the linear function of the explanatory
variables:</p>
<div class="math">
\begin{equation*}
\log E(Y) = \log \lambda = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} \tag{14}
\end{equation*}
</div>
<p>Now with these two equations, we can again derive the log-likelihood function
in order to derive an expression to estimate the <span class="math">\({\bf \beta}\)</span> parameters
(i.e. the maximum likelihood estimate).
Using the same scenario as Equation 6, namely <span class="math">\(n\)</span> <span class="math">\((y_i, {\bf x_i})\)</span>
i.i.d. points, we can derive a log likelihood function (refer to the Wikipedia
link for a reference of the probability mass function of a Poisson distribution):</p>
<div class="math">
\begin{align*}
\arg\max_{\bf \beta}  \mathcal{L(\beta|y_i)}
    &amp;= \prod_{i=1}^{n} \frac{\lambda^{y_i} e^{-\lambda}}{y_i!} \\
\arg\max_{\bf \beta}  \log \mathcal{L(\beta|y)}
    &amp;= \sum_{i=1}^{n} \big( y_i \log\lambda - \lambda - \log{y_i!} \big) \\
    &amp;= \sum_{i=1}^{n} \big( y_i \log\lambda - \lambda \big) \\
    &amp;= \sum_{i=1}^{n} \big( y_i (\beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip})
        - e^{(\beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip})} \big) \tag{15}
\end{align*}
</div>
<p>You can arrive at the last line by substituting Equation 14 in.  Unlike ordinary linear
regression, Equation 15 doesn't have a closed form for its solution.  However, it is a convex
function meaning that we can use a numerical technique such as gradient descent
to find the unique optimal values of <span class="math">\({\bf \beta}\)</span> that maximize the
likelihood function.</p>
<p></p>
<h5> Prediction of Poisson Regression </h5>
<p>Once we have a point estimate for <span class="math">\({\bf \beta}\)</span>, we can define the
distribution for our outcome variable:</p>
<div class="math">
\begin{equation*}
Y \sim Poisson(exp\{\beta_0 + \beta_1 x_{1} + ... + \beta_p x_{p}\}) \tag{16}
\end{equation*}
</div>
<p>and correspondingly our point prediction of <span class="math">\(\hat{y_i}\)</span> given its
explanatory variables:</p>
<div class="math">
\begin{equation*}
\hat{y_i} = E(y_i) = exp\{\beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip}\} \tag{17}
\end{equation*}
</div>
<p></p>
<h5> Other GLM Models </h5>
<p>There are a variety of choices for the distribution of <span class="math">\(Y\)</span> and the
link functions.  This
<a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">table</a>
from Wikipedia has a really good overview from which you can derive the other
common types of GLMs.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> is
actually a type of GLM with outcome variable modeled as a
<a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>
and link function as the <a class="reference external" href="https://en.wikipedia.org/wiki/Logit">logit</a>
function (inverse of the <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>, hence the name).
In the same way as we did for the ordinary and Poisson regression, you can
derive a maximum likelihood expression and numerically solve for the required
coefficients (there is no closed form solution similar to the Poisson regression).</p>
<p><br></p>
<h4> Conclusion </h4>
<p>Linear regression is such a fundamental tool in statistics that sometimes
it is not explained in enough detail (or as clearly as it should be).
Building up a regression model from the bottom up is much more
interesting than the traditional method of presenting the end result and
scarcely relating it back to its probabilistic roots.  In my opinion, there's a
lot of beauty in statistics but only because it has its roots in probability.
I hope this post helped you see some of the beauty of this fundamental topic too.</p>
<p><br></p>
<h4> References and Further Reading </h4>
<ul class="simple">
<li>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">Linear Regression</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model">Generalized linear models</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_regression">Poisson Regression</a>
</li>
</ul>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>Since logarithm is monotonically increasing, it achieves the same maximum as the logarithm of a function at the same point.  It's also much more convenient to work with because many probability distributions have an exponents or are multiplicative.  The logarithm brings down the exponents and changes the multiplications to additions.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td>This is true at least in a Bayesian interpretation.  In a frequentist interpretation, there is a fixed true value of <span class="math">\(\mu\)</span>, and what is random is the confidence interval we can find that "traps" it.  I've written a bit about it <a class="reference external" href="../hypothesis-testing">here</a>.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id3">[3]</a></td>
<td>We explicitly use the conditional notation here because the value of <span class="math">\(y\)</span> depends on <span class="math">\({\bf x}\)</span>.</td>
</tr></tbody>
</table>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/bayesian/" rel="tag">Bayesian</a></li>
            <li><a class="tag p-category" href="../../categories/logistic/" rel="tag">logistic</a></li>
            <li><a class="tag p-category" href="../../categories/poisson/" rel="tag">Poisson</a></li>
            <li><a class="tag p-category" href="../../categories/probability/" rel="tag">probability</a></li>
            <li><a class="tag p-category" href="../../categories/regression/" rel="tag">regression</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../normal-approximations-to-the-posterior-distribution/" rel="prev" title="Normal Approximation to the Posterior Distribution">Previous post</a>
            </li>
            <li class="next">
                <a href="../beyond-collaborative-filtering/" rel="next" title="Beyond Collaborative Filtering">Next post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
            </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2017         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
