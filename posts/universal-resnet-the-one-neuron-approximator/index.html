<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Some fun playing around with neural network universal approximators.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Universal ResNet: The One-Neuron Approximator | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/universal-resnet-the-one-neuron-approximator/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="Some fun playing around with neural network universal approximators.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../hyperbolic-geometry-and-poincare-embeddings/" title="Hyperbolic Geometry and Poincaré Embeddings" type="text/html">
<link rel="next" href="../label-refinery/" title="Label Refinery: A Softer Approach" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Universal ResNet: The One-Neuron Approximator</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2018-08-03T08:03:28-04:00" itemprop="datePublished" title="2018-08-03 08:03">2018-08-03 08:03</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p><em>"In theory, theory and practice are the same. In practice, they are not."</em></p>
<p>I read a very interesting paper titled <em>ResNet with one-neuron hidden layers is
a Universal Approximator</em> by Lin and Jegelka [1].
The paper describes a simplified Residual Network as a universal approximator,
giving some theoretical backing to the wildly successful ResNet architecture.
In this post, I'm going to talk about this paper and a few of the related
universal approximation theorems for neural networks.
Instead of going through all the theoretical stuff, I'm simply going introduce
some theorems and play around with some toy datasets to see if we can get close
to the theoretical limits.</p>
<p>(You might also want to checkout my previous post where I played around with
ResNets: <a class="reference external" href="../residual-networks/">Residual Networks</a>)</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Universal Approximation Theorems </h4>
<p></p>
<h5> The OG Universal Approximation Theorem </h5>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> (1989) is one of
the most well known (and often incorrectly used) theorems when it comes to
neural networks.  It's probably because neural networks don't have as much
theoretical depth as other techniques so people feel like they have to give
some weight to its spectacular empirical results.  I'll paraphrase the theorem
here (if you're really interested in the math click on the Wikipedia link):</p>
<blockquote>
<p>The <strong>universal approximation theorem</strong> states that a feed-forward
network with a single hidden layer containing a finite number of neurons
can approximate functions on compact sets on <span class="math">\(\mathbb{R}^n\)</span>, under
mild assumptions of the activation function (non-constant,
monotonically-increasing, continuous).</p>
</blockquote>
<p>The implications is <em>theoretically</em> we can approximate any function just by
arbitrarily increasing the width of a single hidden layer.</p>
<p>For me, this theorem doesn't strike me as all that insightful (or useful).
First, in practice we never arbitrarily make the width of a neural network
really wide, never mind having a single hidden layer.  Second, it seems
intuitive that this might be the case doesn't it?  If we think about approximating
a function with an arbitrary number of <a class="reference external" href="https://en.wikipedia.org/wiki/Piecewise_linear_function">piece-wise linear functions</a>, then we should be able to approximate
<em>most</em> well-behaved functions (that's basically how integration works).
Similarly, if we have a neuron for each "piece" of the function, we should
intuitively be able to approximate it to any degree (although I'm sure the
actual proof is much more complicated and elegant).</p>
<div class="figure align-center">
<img alt="Piecewise Linear Function" src="../../images/piecewise_linear_approximation.png" style="height: 300px;"><p class="caption">Figure 1: If we can approximate functions with an arbitrary number of
piece-wise linear functions, why can't we do it with an arbitrary number of
hidden units? (source: Wikipedia)</p>
</div>
<p>So practically, there isn't much that's interesting about this (basic) version
of the theorem.  And definitely, people should stop quoting it as if it somehow
"proved" something.</p>
<p></p>
<h5> Universal Approximation Theorem for Width-Bounded ReLU Networks </h5>
<p>A <em>much</em> more interesting result for neural networks is a universal approximation
theorem for width-bounded neural networks published recently (2017) from [2]:</p>
<blockquote>
<p>Let <span class="math">\(n\)</span> denotes the input dimension, we show that width-<span class="math">\((n + 4)\)</span>
ReLU networks can approximate any Lebesgue integrable function on
n-dimensional space with respect to L1 distance.</p>
<p>Additionally, except for a negligible set, most functions <em>cannot</em>
be approximated by a ReLU network of width at most <span class="math">\(n\)</span>.</p>
</blockquote>
<p>So basically if you have just 4 extra neurons in each layer and arbitrary
number of hidden layers, you can approximate any Lebesque integrable function
(which are most functions).  This result is very interesting because this
is starting to look like a feed-forward network that we might build.  Just take
the number of inputs, add 4, and then make a bunch of hidden layers of that
size.  Of course, training this is another story, which we'll see below.
The negative result is also very interesting.  We need some extra width or else
we'll never be able to approximate anything.
There are also a whole bunch of other results on neural networks that have been
proven.  See the references from [1] and [2] if you're interested.</p>
<p></p>
<h5> Universal Approximation Theorem for The One-Neuron ResNet </h5>
<p>ResNet is a neural network architecture that contains a "residual connection".
Basically, it provides a shortcut from layer <span class="math">\(i\)</span> merging it with addition
to layer <span class="math">\(i+k\)</span> for some constant <span class="math">\(k\)</span>.  In between, you can do all
kinds of interesting things such as have multiple layers, increase or
bottleneck the width, but the important thing is that the input width of layer
<span class="math">\(i\)</span> is the same as the output width of layer <span class="math">\(i+k\)</span>.  Figure 2 shows
a simplified ResNet architecture where the "in between" transformation is a
single neuron.</p>
<div class="figure align-center">
<img alt="Basic ResNet Block" src="../../images/basic_resnet.png" style="height: 200px;"><p class="caption">Figure 2: The basic residual block with one neuron per hidden layer (source: [1])</p>
</div>
<p>Note that through this ResNet block there are two paths: one that goes through
a bottleneck of a <em>single</em> neuron, and one that is the identity function.  The
outputs then get added together at the end.  Writing out the expression of the
function, we have our ResNet block <span class="math">\(\mathcal{R}_i(x)\)</span> and our final network
<span class="math">\(Y(x)\)</span>:</p>
<div class="math">
\begin{align*}
\text{ReLU}(x) &amp;= max(x, 0) \\
\text{Id}(x) &amp;= x \\
\mathcal{R}_i({\bf x}) &amp;= {\bf V_i}\text{ReLU}({\bf U_i}{\bf x} + b_i) + \text{Id}({\bf x}) \\
Y(x) &amp;= \mathcal{R}_n({\bf x}) \circ \ldots \circ \mathcal{R}_1({\bf x})
\tag{1}
\end{align*}
</div>
<p>where <span class="math">\(\bf U\)</span> is <span class="math">\(d \text{x} 1\)</span> weight matrix, <span class="math">\(\bf V\)</span> is
<span class="math">\(1\text{x}d\)</span> weight matrix, and <span class="math">\(b\)</span> is a learnable constant.</p>
<p>Given the above ResNet architecture, the universal approximation theorem from
[1] states:</p>
<blockquote>
<p>ResNet with one single neuron per hidden layer is enough to provide
universal approximation for any Lebesgue-integrable function as the depth
goes to infinity.</p>
</blockquote>
<p>This is a pretty surprising statement!  Remember, this architecture's only
non-linear element is a bottleneck with a <em>single</em> neuron!  However,
this theorem somehow gives credibility to the power of Residual Networks
because even with a single neuron it is powerful enough to represent any
function.</p>
<p><br></p>
<h4> Experiments </h4>
<p>In this section I play around with a few different network architectures
relating to the theorems above to see how they would perform.  You can see my
work in this notebook on
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/universal_resnet/Universal%20Resnet.ipynb">GitHub</a>.</p>
<p>I generated three toy datasets labelled "easy", "medium", "hard" with two input
variables <span class="math">\(x_1, x_2\)</span>, and a single binary label.  Each dataset used the
same 300 <span class="math">\((x_1, x_2)\)</span> input points but had different predicates of
increasing complexity.  Figure 3 shows the three datasets.</p>
<div class="figure align-center">
<img alt='"easy", "medium" and "hard" datasets' src="../../images/universal_resnet_expr1.png" style="height: 200px;"><p class="caption">Figure 3: Plot of "easy", "medium" and "hard" datasets.  The shaded region
indicates where the generating function should be a "1".  The "x"s represent
"0"s and the dots represent "1"s.  Notice the "hard" dataset has a bunch of
spurious "1"s that are randomly placed on the grid.</p>
</div>
<p>First, let's take a look at the different architectures that I ran.  All the
experiments use a softmax output layer with a binary crossentropy loss.</p>
<ul class="simple">
<li><p><strong>Resnet</strong>: Stacking the above ResNet block (figure 2) with a width of 2
(same as the input).</p></li>
<li><p><strong>Dense (W=2, W=6)</strong>: Fully connected dense layers with a
width of either 2 (size of inputs) or 6 (size of inputs + 4 from the
universal theorem above) with ReLU activations.</p></li>
<li><p><strong>Single Dense with Variable Width</strong>: A single hidden layer but changing
the width (OG universal theorem) with ReLU activations.</p></li>
</ul>
<p>I varied depths of the first two architectures varied from 5, 10, and 40.  The
width of the last architecture varies from 5, 10, 40, 100, and 300.
For each combination, I ran 5 experiments and report the mean and standard
deviation of accuracy on the training data (no testing set here since we're trying
to see if it can approximate samples from the underlying function).  Bolded
results show the best run for each dataset.  The overall results are shown in
Table 1 and Table 2, while Appendix A shows figures plotting the best run of
each configuration.</p>
<p><em>Table 1: ResNet vs. Dense (W=2, 6) with varying depths on Easy, Medium and Hard datasets.</em></p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 13%">
</colgroup>
<tbody>
<tr>
<td></td>
<td colspan="3"><p>Easy</p></td>
<td colspan="3"><p>Medium</p></td>
<td colspan="3"><p>Hard</p></td>
</tr>
<tr>
<td></td>
<td><p>D=5</p></td>
<td><p>D=10</p></td>
<td><p>D=40</p></td>
<td><p>D=5</p></td>
<td><p>D=10</p></td>
<td><p>D=40</p></td>
<td><p>D=5</p></td>
<td><p>D=10</p></td>
<td><p>D=40</p></td>
</tr>
<tr>
<td><p>ResNet</p></td>
<td><p>98 ± 1</p></td>
<td><p><strong>99 ± 0</strong></p></td>
<td><p>98 ± 2</p></td>
<td><p>85 ± 8</p></td>
<td><p>95 ± 3</p></td>
<td><p>98 ± 2</p></td>
<td><p>67 ± 3</p></td>
<td><p>73 ± 3</p></td>
<td><p><strong>88 ± 4</strong></p></td>
</tr>
<tr>
<td><p>Dense (W=2)</p></td>
<td><p>86 ± 0</p></td>
<td><p>86 ± 0</p></td>
<td><p>86 ± 0</p></td>
<td><p>77 ± 0</p></td>
<td><p>77 ± 0</p></td>
<td><p>77 ± 0</p></td>
<td><p>62 ± 2</p></td>
<td><p>62 ± 0</p></td>
<td><p>62 ± 0</p></td>
</tr>
<tr>
<td><p>Dense (W=6)</p></td>
<td><p>99 ± 0</p></td>
<td><p>99 ± 1</p></td>
<td><p>88 ± 6</p></td>
<td><p><strong>99 ± 1</strong></p></td>
<td><p>97 ± 6</p></td>
<td><p>77 ± 0</p></td>
<td><p>85 ± 5</p></td>
<td><p>75 ± 8</p></td>
<td><p>62 ± 0</p></td>
</tr>
</tbody>
</table>
<p>We can see in Table 1 that ResNet has pretty consistent performance.  As we
increase the depth, it's able to successfully translate the increased capacity
into accuracy gains.  Not only that, it shows the best best performance in the
easy and hard datasets, while being pretty close on the medium one.
The fact that we are able to train a network so deep shows the uncanny ability
of ResNet architectures to train deep networks (as compared to the dense
layers).  So the ResNet universal approximation theorem for ResNet seems to be
holding up somewhat.</p>
<p>For the dense networks with width=2, it is totally incapable of learning
anything useful.  The accuracies reported are actually just the underlying
ratio of positive to negative labels where it pretty much just learned the
constant function (see figures in Appendix A).  This confirms the negative
result of the width bounded universal approximation theorem.</p>
<p>At width=6, the dense network shows more mixed results.  It performs quite well
at depths 5 and 10, even producing the best result on the medium dataset at
depth=5.  However, it totally breaks down at depth 40.  This is most likely due
to the difficulty of fitting really deep networks.  An important lesson:
theoretical results don't always translate into practical ones.
(We probably could have helped this along by adding some normalization to fit
the deep networks.  I did add do one trial with batch normalization but the
results didn't change much.)</p>
<p><em>Table 2: Dense with a single hidden layer and varying width on Easy, Medium and Hard datasets.</em></p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<tbody>
<tr>
<td><p>Width</p></td>
<td><p>5</p></td>
<td><p>10</p></td>
<td><p>40</p></td>
<td><p>100</p></td>
<td><p>300</p></td>
</tr>
<tr>
<td><p>Easy</p></td>
<td><p>99 ± 1</p></td>
<td><p>100 ± 0</p></td>
<td><p>100 ± 0</p></td>
<td><p><strong>100 ± 0</strong></p></td>
<td><p>99 ± 0</p></td>
</tr>
<tr>
<td><p>Medium</p></td>
<td><p>79 ± 2</p></td>
<td><p>86 ± 8</p></td>
<td><p>97 ± 3</p></td>
<td><p><strong>99 ± 1</strong></p></td>
<td><p>97 ± 2</p></td>
</tr>
<tr>
<td><p>Hard</p></td>
<td><p>66 ± 2</p></td>
<td><p>68 ± 4</p></td>
<td><p>77 ± 3</p></td>
<td><p><strong>81 ± 1</strong></p></td>
<td><p>78 ± 1</p></td>
</tr>
</tbody>
</table>
<p>Taking a look at similar results with a single hidden layer architecture
in Table 2, we see that the original universal approximation theorem shows more
consistent results.  Each increase in width is better than the previous except
at W=300 where it performs pretty close to the width below it.  It performs
on-par with the above architectures except on the hard where it performs
slightly worse.  However, we can see the results across the board are much more
consistent (smaller standard deviation).  This indicates that the single hidden
layer is also the easier to fit (no vanishing gradients).  However, it's also
been shown to be very inefficient -- you might need a non-linear (exponential?)
number of units before you can approximate something.  It turns out that these
datasets aren't that hard so 100 or so hidden units will do.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>Definitely a shorter post than my previous few.  Although like many theoretical
results these ones aren't too practical, but I thought this result was so
interesting because of the strong empirical evidence in favour of ResNet
architectures.  Many results in deep learning don't have great theoretical
foundations, so it's so nice to see a result for one of big ideas in this area.
My next post will likely be a shorter one too with another cute idea that I've
read about recently.  Stay tuned!</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li><p>Previous posts: <a class="reference external" href="../residual-networks/">Residual Networks</a></p></li>
<li><p>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a></p></li>
<li><p>[1] <a class="reference external" href="https://arxiv.org/abs/1806.10909">ResNet with one-neuron hidden layers is a Universal Approximator</a>, Hongzhou Lin, Stefanie Jegelka</p></li>
<li><p>[2] <a class="reference external" href="https://arxiv.org/abs/1709.02540">The Expressive Power of Neural Networks: A View from the Width</a> Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, Liwei Wang</p></li>
</ul>
<p><br></p>
<h4> Appendix A: Figures for Experiments </h4>
<p>The figures below show the <em>best</em> run out of the five runs for each of the
given experiments.  So the accuracies are at the top of the range
and do not match the averages in Table 1 and 2.</p>
<div class="figure align-center">
<img alt="Plots of Predictions (ResNet)" src="../../images/universal_resnet_expr2.png" style="height: 600px;"><p class="caption">Figure 3: Plot of predictions from the ResNet architecture.</p>
</div>
<p><br></p>
<div class="figure align-center">
<img alt="Plots of Predictions (Dense)" src="../../images/universal_resnet_expr3.png" style="height: 600px;"><p class="caption">Figure 4: Plot of predictions from the Dense (W=2) architecture.</p>
</div>
<p><br></p>
<div class="figure align-center">
<img alt="Plots of Predictions (Dense)" src="../../images/universal_resnet_expr4.png" style="height: 600px;"><p class="caption">Figure 5: Plot of predictions from the Dense (W=6) architecture.</p>
</div>
<p><br></p>
<div class="figure align-center">
<img alt="Plots of Predictions (Dense)" src="../../images/universal_resnet_expr7.png" style="height: 1200px;"><p class="caption">Figure 6: Plot of predictions from the Single Dense architecture.</p>
</div>
<p><br></p>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/hidden-layers/" rel="tag">hidden layers</a></li>
            <li><a class="tag p-category" href="../../categories/neural-networks/" rel="tag">neural networks</a></li>
            <li><a class="tag p-category" href="../../categories/residual-networks/" rel="tag">residual networks</a></li>
            <li><a class="tag p-category" href="../../categories/resnet/" rel="tag">ResNet</a></li>
            <li><a class="tag p-category" href="../../categories/universal-approximator/" rel="tag">universal approximator</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../hyperbolic-geometry-and-poincare-embeddings/" rel="prev" title="Hyperbolic Geometry and Poincaré Embeddings">Previous post</a>
            </li>
            <li class="next">
                <a href="../label-refinery/" rel="next" title="Label Refinery: A Softer Approach">Next post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2020         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
