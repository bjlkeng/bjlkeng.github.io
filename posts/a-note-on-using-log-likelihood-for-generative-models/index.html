<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A short exploration on the theory behind using log-likelihood to train and measure generative models using image-like data.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>A Note on Using Log-Likelihood for Generative Models | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../pixelcnn/" title="PixelCNN" type="text/html">
<link rel="next" href="../model-explanability-with-shapley-additive-explanations-shap/" title="Model Explainability with SHapley Additive exPlanations (SHAP)" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="A Note on Using Log-Likelihood for Generative Models">
<meta property="og:url" content="http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/">
<meta property="og:description" content="A short exploration on the theory behind using log-likelihood to train and measure generative models using image-like data.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-08-27T07:50:09-04:00">
<meta property="article:tag" content="generative models">
<meta property="article:tag" content="log-likelihood">
<meta property="article:tag" content="mathjax">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">A Note on Using Log-Likelihood for Generative Models</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2019-08-27T07:50:09-04:00" itemprop="datePublished" title="2019-08-27 07:50">2019-08-27 07:50</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>One of the things that I find is usually missing from many ML papers is how
they relate to the fundamentals.  There's always a throwaway line where it
assumes something that is not at all obvious (see my post on
<a class="reference external" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/">Importance Sampling</a>).  I'm the kind of person who likes to
understand things to a satisfactory degree (it's literally in the subtitle of
the blog) so I couldn't help myself investigating a minor idea I read about in
a paper.</p>
<p>This post investigates how to use continuous density outputs (e.g. a logistic
or normal distribution) to model discrete image data (e.g. 8-bit RGB values).
It seems like it might be something obvious such as setting the loss as the
average log-likelihood of the continuous density and that's <em>almost</em> the
whole story.  But leaving it at that skips over so many (interesting) and
non-obvious things that you would never know if you didn't bother to look.  I'm
a curious fellow so come with me and let's take a look!</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Review of Some Fundamental Concepts </h4>
<p>Before we get into the meat of the topic, I want to spend some time on some
basic questions regarding mixing continuous and discrete data (inspired
from [2,3]):</p>
<blockquote>
<p><em>How you find the probability of seeing observations drawn from a
continuous distributions? e.g. What's the probability of drawing</em>
<span class="math">\(x=0\)</span> <em>from a standard normal distribution?</em></p>
</blockquote>
<p>Seems like a simple problem, right?  Well, there are lots of paradoxes lurking
here, so let's clarify them before moving on.</p>
<p></p>
<h5> Zero Probability Events </h5>
<p>For example, assume you have sampled some data from an
<a class="reference external" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a>
continuous distribution, what is the probability of that happening?
Let's reason it out.</p>
<p>To make things concrete, assume we have a <span class="math">\(U\)</span>
<a class="reference external" href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous">continuous uniform distribution</a>
with support <span class="math">\([0,1]\)</span>. Let's draw a sample <span class="math">\(x\)</span> from <span class="math">\(U\)</span>, what
is the probability that we drew <span class="math">\(x\)</span>?  Well we know a few things about
this distribution, in particular, it's probability density function and how to
compute probability from it:</p>
<div class="math">
\begin{align*}
f_U(y) &amp;= \frac{1}{b-a} = \left\{
    \begin{array}{ll}
        1 &amp;&amp; \text{for support } [a,b] \\
        0 &amp;&amp; \text{otherwise}
    \end{array}
\right. \tag{1} \\
P(u \leq y \leq v) &amp;= \int_u^v f_U(y) dy = y \big|_u^v = v - u &amp;&amp; \text{for } u, v \in [0,1] \tag{2}
\end{align*}
</div>
<p>Very fundamental indeed!  So we know how to compute probability (Equation 2)
over a distribution if we have a continuous interval, but that doesn't
quite get us to a single data point.
Let's take the limit of Equation 2 to see how the interval behaves as it
approaches a single data point:</p>
<div class="math">
\begin{align*}
P(x|U) &amp;= \lim_{\epsilon \to 0}{P(x - \epsilon \leq y \leq x + \epsilon)} \\
&amp;= \lim_{\epsilon \to 0} \int_{x-\epsilon}^{x+\epsilon} f_U(y) dy \\
&amp;= \lim_{\epsilon \to 0} y \big|_{x-\epsilon}^{x+\epsilon} \\
&amp;= \lim_{\epsilon \to 0} { (x + \epsilon) - (x - \epsilon) } \\
&amp;= 0 \\
\tag{3}
\end{align*}
</div>
<p>So the probability of <span class="math">\(x\)</span> occurring is <span class="math">\(0\)</span>?!</p>
<p>Yes!  Which is a bit
of a paradox since we did indeed draw it from a distribution, so it definitely
"happened".  This is also not particular to the uniform distribution, if you do
the same with any (reasonable) distribution, you'll find the same thing.  How
can this be possible?</p>
<p>Another way to think about this: what if the probability of observing <span class="math">\(x\)</span>
wasn't <span class="math">\(0\)</span> but rather some positive number <span class="math">\(\epsilon\)</span>?  That means
every number in the range <span class="math">\([0, 1]\)</span> would have some positive probability
of being observed.  However, all those numbers/events are mutually exclusive,
so the sum of them should add up to <span class="math">\(1\)</span> but this isn't possible because
you have infinitely many of them, so each one must be infinitely small.
Infinities are weird.</p>
<p></p>
<h5> Hypothesis Testing with Continuous Data and Log-Likelihoods </h5>
<p>So how can resolve this paradox? As usual, we're asking the wrong question!
When we have observed an event, what we're really asking is "<em>what is the
probability that this event is generated by this hypothesis?</em>".  In other
words, we're really doing hypothesis testing! (Here, we're talking about
<a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">statistical hypotheses</a>,
which is an assumption about a distribution with a particular set of parameters.)</p>
<p>So let's try this again but a bit more generically.  Let's say we have observed
a real-valued datum <span class="math">\(x\)</span> (this can easily be extended to multiple points
given IID data), and we have <span class="math">\(N\)</span> hypothesis, <span class="math">\(M_1, M_2, \ldots,
M_N\)</span>, each representing a fixed parameter distribution with PDFs <span class="math">\(f_i\)</span>
and CDFs <span class="math">\(F_i\)</span>.</p>
<p>(Note: we'll use the notation "<span class="math">\(M\)</span>" here because it also implies that
our hypotheses are trained models.  That is, a set of assumptions about the
data with a particular set of trained parameters.  You can start to see
how this is going to lead us to generative models...)</p>
<p>Let's formulate our question: "<em>what is the probability of hypothesis</em> <span class="math">\(M_1\)</span>
<em>given datum</em> <span class="math">\(x\)</span>" (using Bayes rule):</p>
<div class="math">
\begin{align*}
P(M_1 | x) &amp;= \frac{P(x | M_1)P(M_1)}{P(x)} \\
           &amp;= \frac{P(x | M_1)P(M_1)}{\sum_{i=1}^N P(x | M_i)P(M_i)} \\
           \tag{4}
\end{align*}
</div>
<p>Here we're using the standard expansion of the denominator for <span class="math">\(P(x)\)</span>.
As with Bayesian analysis, we need some prior for how likely each model occurs.
Let's just assume we have no intuition about which model is better, so they're
equally likely.  Since we're also dealing with continuous values, we'll use the
"<span class="math">\(\epsilon\)</span> trick" we used above:</p>
<div class="math">
\begin{align*}
P(M_1 | x)
&amp;= \frac{P(x | M_1)P(M_1)}{\sum_{i=1}^N P(x | M_i)P(M_i)} \\
&amp;= \lim_{\epsilon \to 0}
    \frac{[\int_{x-\epsilon}^{x+\epsilon} f_1(x) dx][\frac{1}{N}]}{
        \sum_{i=1}^N [\int_{x-\epsilon}^{x+\epsilon} f_i(x) dx][\frac{1}{N}]} \\
&amp;= \lim_{\epsilon \to 0}
    \frac{F_1(x+\epsilon) - F_1(x-\epsilon)}{\sum_{i=1}^N \big(F_i(x+\epsilon) - F_i(x-\epsilon)\big)} \\
&amp;= \frac{\lim_{\epsilon \to 0} \frac{F_1(x+\epsilon) - F_1(x-\epsilon)}{\epsilon}}{
    \sum_{i=1}^N
    \lim_{\epsilon \to 0}
    \big(\frac{F_i(x+\epsilon) - F_i(x-\epsilon)}{\epsilon}\big)}
    &amp;&amp; \text{divide top and bottom by } \epsilon \text{ and distribute limit}
    \\
&amp;= \frac{f_1(x)}{\sum_{i=1}^N f_i(x)} &amp;&amp; \text{definition of derivative} \\
\tag{5}
\end{align*}
</div>
<p>You might have to brush off your calculus a bit with the comments above, but I
think you should be able to follow along.  The last step is not the typical
definition of a derivative but it's should be equivalent. (Note: this derivative
probably only works for smooth functions.)</p>
<p>The interesting thing here is that we've totally resolved the problem of dealing
with continuous data!  We're dealing only with PDFs now and removed the zero
probability case when looking at <span class="math">\(P(x|M_i)\)</span> in isolation.</p>
<div class="admonition admonition-example-1-probability-an-observation-is-from-a-given-two-gaussian-hypotheses">
<p class="admonition-title">Example 1: Probability an Observation is from a Given Two
Gaussian Hypotheses</p>
<p>Suppose we have a datum <span class="math">\(x=0\)</span> that we know is drawn from
one of two Gaussian distributions:</p>
<ul class="simple">
<li><p><span class="math">\(M_1: \mathcal{N}(\mu=0, \sigma^2=1)\)</span></p></li>
<li><p><span class="math">\(M_2: \mathcal{N}(\mu=1, \sigma^2=1)\)</span></p></li>
</ul>
<p>What is the probability of <span class="math">\(x\)</span> being drawn from each distribution
(assuming our priors are equally likely)?
Equivalently, what is <span class="math">\(P(M_1 | x)\)</span> and <span class="math">\(P(M_2 | x)\)</span>
with <span class="math">\(P(M_1)= P(M_2)=0.5\)</span>?</p>
<p>Using Equation 5 it is simply just the PDFs of the two normalized by
our priors:</p>
<div class="math">
\begin{align*}
P(M_1|x) &amp;= \frac{f_{M_1}P(M_1)}{f_{M_1}P(M_1)+ f_{M_2}P(M_2)} \\
         &amp;= \frac{-e^{x^2/2}(0.5)}{e^{-x^2/2}(0.5) + e^{-(x-1)^2/2}(0.5)}
         &amp;\approx 0.6225 \\
P(M_2|x) &amp;= \frac{f_{M_2}P(M_2)}{f_{M_1}P(M_1)+ f_{M_2}P(M_2)} \\
         &amp;= \frac{-e^{(x-1)^2/2}(0.5)}{e^{-x^2/2}(0.5) + e^{-(x-1)^2/2}(0.5)}
         &amp;\approx 0.3775 \tag{6}
\end{align*}
</div>
<p>Therefore, <span class="math">\(x\)</span> is more likely to be drawn from distribution <span class="math">\(M_1\)</span>.</p>
</div>
<p>Before we finish off this section, we should notice one thing.  Given a fixed
set of hypotheses (or models) the denominator in Equation 5 doesn't change.
That is, <span class="math">\(P(X)\)</span> is constant.  Therefore, assuming all models are equally
likely, we can do a relative comparison of models just by their PDFs.
From Equation 5:</p>
<div class="math">
\begin{equation*}
P(M_i | x) = \frac{f_i(x)}{\sum_{i=1}^N f_i(x)} \propto f_i(x)
\tag{7}
\end{equation*}
</div>
<p>Further, given a IID data for <span class="math">\(x\)</span>, we can do a relative comparison
of (fitted) models by taking the logarithm:</p>
<div class="math">
\begin{align*}
P(M_i | x_1 \ldots x_n) &amp;= \frac{f_i(x_1)\ldots f_i(x_n)}{\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)} \\
\log{P(M_i | x_1 \ldots x_n)} &amp;= \log\big[\frac{f_i(x_1)\ldots f_i(x_n)}{\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)}\big] \\
&amp;= \log{f_i(x_1)} + \ldots + \log{f_i(x_n)} - \log{[\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)]} \\
&amp;\propto \log{f_i(x_1)} + \ldots + \log{f_i(x_n)} \\
&amp;= \sum_{j=1}^n \log{f_i(x_j)} \\
\tag{8}
\end{align*}
</div>
<p>Where the last line of Equation 8 should look very familiar: it's the standard
log-likelihood that we maximize in many ML and statistical models. Thus,
we can directly compare how well a model represents some data using the loss
from the log-likelihood as you would expect. (However, keep in mind it is
<em>not</em> a probability.)</p>
<p></p>
<h5> Cross Entropy and Expected Message Length </h5>
<p>I won't go too deep into the concept of <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy">(information) entropy</a>,
you can find a more detailed explanation in my previous post on
<a class="reference external" href="../maximum-entropy-distributions/">Maximum Entropy Distributions</a>.
Information Entropy is defined as follows over a discrete probability
distribution <span class="math">\(P\)</span>:</p>
<div class="math">
\begin{equation*}
H(p) = -\sum_{x \in \mathcal{X}} P(x) \log P(x) \tag{9}
\end{equation*}
</div>
<p>One of the interesting properties for discrete random variables is that the
entropy provides a lower bound for how much (on average) those symbols can be
compressed
(via the <a class="reference external" href="https://en.wikipedia.org/wiki/Shannon's_source_coding_theorem">Shannon Coding Theorem</a>).
This is squarely in the domain of information theory and communication
systems.</p>
<p>For example, a <em>very</em> simplified application might be you might be sending a
string of bytes to your friend over a Wifi channel, how should you encode each
byte (i.e. symbol) into bits so that it minimizes the length of the message?
Intuitively, you would want the most frequent byte values (symbols) to have
the shortest length, the next most frequent byte to have a slightly longer
length, etc. <a class="reference external" href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Coding</a>
is an example of such a scheme that enables lossless compression that is
optimal.  Optimality (that is, symbol-to-symbol optimality) is precisely the
entropy lower bound.</p>
<p>A related concept is that of
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a>:
given two probability discrete distributions <span class="math">\(P\)</span> and <span class="math">\(Q\)</span> defined
over the same set, we have:</p>
<div class="math">
\begin{align*}
H(p,q) &amp;= -\sum_{x \in \mathcal{X}} P(x) \log Q(x) \\
&amp;= H(P) + D_{KL}(P||Q) \\
\tag{10}
\end{align*}
</div>
<p>This looks almost identical to the definition of entropy in Equation 9 except
we replace <span class="math">\(P\)</span> with <span class="math">\(Q\)</span> inside the logarithm. As you would
expect, this also has an equivalent interpretation: cross entropy gives
the average number of bits (using base 2 logarithm) needed to code a symbol
drawn from <span class="math">\(P\)</span> using the optimal code for <span class="math">\(Q\)</span>.
The second line of Equation 10 also shows us another interpretation of cross
entropy: <span class="math">\(H(P,Q)\)</span> is equal to entropy of <span class="math">\(P\)</span> plus the KL divergence
between <span class="math">\(Q\)</span> from <span class="math">\(P\)</span>.</p>
<p>Cross entropy can also be defined for the continuous case too
for continuous densities <span class="math">\(p\)</span> and <span class="math">\(q\)</span>:</p>
<div class="math">
\begin{equation*}
H(p, q) = -\int_{\mathcal{X}} P(x)\log Q(x) dx \tag{11}
\end{equation*}
</div>
<p>Note: We should be careful distinguishing between information entropy defined in
Equation 9 on discrete variables and the continuous version called
<a class="reference external" href="https://en.wikipedia.org/wiki/Differential_entropy">differential entropy</a>.
Differential entropy has a similar form but doesn't have the same nice
intuitive meaning of encoding into bits.  It also doesn't have nice properties,
for example, differential entropy can be negative.  A more interpretable
quantity is the KL divergence, which is the "number of extra bits to encode
P using Q".  See this
<a class="reference external" href="https://stats.stackexchange.com/questions/256203/how-to-interpret-differential-entropy">Stack Exchange question</a>
for more details.</p>
<p><br></p>
<h4> Generative Models, Log-Likelihoods and Image Data </h4>
<p>Evaluating generative models is a tricky subject mostly because there is no
"one metric to rule them all".  Unlike classifiers or regression problems,
there is no singular concept of "accuracy" or "error".  Generally, this is
because we evaluate generative models in two broad ways (a) quality of the
samples, and (b) average log-likelihoods.
Both of these metrics do not necessarily track each other, in other words,
we can have high log-likelihoods but low quality samples and vice versa
(of course they can be high or low on both too).  A more thorough discussion
of this topic is in [1].</p>
<p>However in this post, I just want to focus on the average log-likelihood method
for now, in particular, interpretations of it in terms of probability for image
data.  The (usual) reason why this is of more concern is that it's <em>easy</em> to
measure.  For example, how can we measure the quality of a generated image?
There's no obvious ways to do it ([1] discusses a few approximations to it).
That's why focusing on likelihood methods is so attractive (and perhaps
misguided?) because it's easier to interpret and compare different models.</p>
<p>This section will go over two cases for image data in particular: models with
discrete outputs and models with continuous outputs.</p>
<p></p>
<h5> Discrete Models </h5>
<p>Image data is naturally discrete. For a typical a 8-bit pixel (or subpixel),
you have <span class="math">\(2^8 = 256\)</span> possible values representing its intensity.  This
naturally lends itself well to a generative model outputting a discrete value.
There are two primary ways (that I know of) to model these pixels.</p>
<p>The first is pretty simple: just have a 256-way
<a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>
for each pixel with a
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a>
loss.  This is the most straightforward and direct way to model each pixel.
This is the method used in the original PixelCNN paper [4].  The main issue
with this is that the resulting network is <em>huge</em> because you have
256 times the number of subpixels you have
(e.g. <span class="math">\(32 x 32 x 3 x 256 = 3072 * 256 = 786432\)</span>).
This can't fit on any reasonably sized GPU.  The other issue is that
qualitatively, pixel intensity <span class="math">\(x \in [0, 255]\)</span> should be close to
<span class="math">\(x+1\)</span>, but if we model it as a softmax, they are more or less independent
with respect to their loss function so your model doesn't capture this
intuitive property.  In any case, using this method should <em>theoretically</em>
generate a good model if you can practically fit it.</p>
<p>The other method described in PixelCNN++ [5] uses a different tactic.  They use
a two step process: first model the intensity as a continuous distribution then
"round" to the nearest pixel by integrating the continuous distribution in the
region around the pixel.  From my post on <a class="reference external" href="../pixelcnn/">PixelCNN</a>,
the rounding step works like so (see the post for more details):</p>
<div class="math">
\begin{equation*}
P(x|\mu,s) =
    \begin{cases}
        \sigma(\frac{x-\mu+0.5}{s}) &amp; \text{for } x = 0 \\
        \sigma(\frac{x-\mu+0.5}{s}) - \sigma(\frac{x-\mu-0.5}{s})
            &amp; \text{for } 0 &lt; x &lt; 255 \\
        1 - \sigma(\frac{x-\mu-0.5}{s}) &amp; \text{for } x = 255
    \end{cases}
\tag{12}
\end{equation*}
</div>
<p>Here <span class="math">\(\sigma\)</span> is the CDF of our continuous pixel intensity distribution
parameterized by <span class="math">\(\mu, s\)</span>.  To find the probability of a given pixel
<span class="math">\(P(x|\mu,s)\)</span>, we simply integrate the distribution across a pixel width
(i.e. take the difference of the CDFs).</p>
<p>This is actually a really elegant solution because we have the nice property
adjacent pixels being similar to each other (assuming a smooth distribution)
and we have a clear way to generate a probability.  It also is much more
parameter efficient (2 parameters vs. 256) but practically you'll need
a more complex distribution.  In the paper, they use a mixture of five logistic
distributions, so it's 10 parameters vs. 256, still a win.</p>
<p>Finally, training the model is as simply as minimizing the negative log-likelihood
of Equation 12 (for <span class="math">\(N\)</span> IID images):</p>
<div class="math">
\begin{align*}
\mathcal{L}({\bf \mu, s})
    &amp;= -\log P({\bf x_1, \ldots, x_N}) \\
    &amp;= -\log[P({\bf x_1}),\ldots, P({x_N})] \\
    &amp;= -\sum_{i=1}^N \log P({\bf x_i}|{\bf \mu, s}) \\
    \tag{13}
\end{align*}
</div>
<p>If we want the average log-likelihood, we can divide Equation 13 by <span class="math">\(N\)</span>
to get:</p>
<div class="math">
\begin{align*}
\frac{1}{N}\mathcal{L}({\bf \mu, s})
    &amp;=  -\sum_{i=1}^N \frac{1}{N} \log P({\bf x_i}|{\bf \mu, s}) \\
    &amp;=  -\sum_{i=1}^N p_{true}({\bf x_i}) \log P({\bf x_i}|{\bf \mu, s}) \\
    \tag{14}
\end{align*}
</div>
<p>where <span class="math">\(p_{true} = \frac{1}{N}\)</span> because we assume the sample dataset we
is uniformly sampled from the "true distribution" of images.
As you can see this directly gives us the cross-entropy from Equation 10.  This
means that we can directly interpret our average log-likelihood loss in terms
of cross entropy, which gives us the "average number of bits (using base 2
logarithm) needed to code a sample from <span class="math">\(p_{true}\)</span> using our model
<span class="math">\(P\)</span>".  Dividing this by the number of pixels, gives us the "bits per
pixel" metric that we see often in papers (e.g. [4], [5]).</p>
<p><br></p>
<h4> Continuous Models </h4>
<p>What do you do when your image data is discrete but your model outputs
a continuous distribution (e.g. normal, logistic etc.)?  When mixing discrete
image data with continuous distributions you get zero probability events
like we discussed above, which could lead to a infinite differential entropy
during training <a class="footnote-reference brackets" href="#id2" id="id1">1</a>.  One alternative is the "rounding" method above. Another
"trick" described in [1] (and previously in a few other papers), is to
add <em>uniform</em> noise to de-quantize the data.  Let's see how this works.</p>
<p>Suppose we have image data <span class="math">\({\bf x} \in \{0,\ldots,255\}^D\)</span> with
a discrete probability distribution <span class="math">\(P(X)\)</span>, uniform noise
<span class="math">\({\bf u} \in [0,1]^D\)</span>, and define noisy data
<span class="math">\({\bf y} = {\bf x} + {\bf u}\)</span>.  Let <span class="math">\(p\)</span> refer to the noisy data
density and <span class="math">\(q\)</span> refer to the continuous density output by our model.
Let's take a look at the negative cross entropy of these two distributions:</p>
<div class="math">
\begin{align*}
-H(p,q) &amp;= \int p({\bf y})\log q({\bf y})d{\bf y} \\
       &amp;= \int_{\bf y} \int_{\bf v} p({\bf y})\log q({\bf y})d{\bf v}d{\bf y}
       &amp;&amp; \text{add dummy variable } v \\
       &amp;= \int_{\bf x} \int_{\bf u} p({\bf x} + {\bf u})\log q({\bf x} + {\bf u})d{{\bf u}}d{\bf x}
       &amp;&amp; \text{change of variable: } y=x+u, v=u \\
       &amp;= \sum_{\bf x} \int_{\bf u\in [0,1]^D} p({\bf x} + {\bf u})\log q({\bf x} + {\bf u})d{{\bf u}} \\
       &amp;= \sum_{\bf x} P({\bf x}) \int_{\bf u\in [0,1]^D} \log q({\bf x} + {\bf u})d{{\bf u}}
       &amp;&amp; p({\bf x} + {\bf u}) := P({\bf x}) \\
       &amp;\leq \sum_{\bf x} P({\bf x}) \log \big[\int_{\bf u\in [0,1]^D} q({\bf x} + {\bf u}) d{{\bf u}\big]}
       &amp;&amp; \text{by Jensen's inequality} \\
       &amp;= \sum_{\bf x} P({\bf x}) \log Q({\bf x}) \\
       &amp;= -H(P,Q) \\
       \tag{15}
\end{align*}
</div>
<p>where we define
<span class="math">\(Q({\bf x}) = \int_{\bf u\in [0,1]^D} q({\bf x} + {\bf u}) d{\bf u}\)</span>.
A couple of points on the derivation:</p>
<ul class="simple">
<li><p>When doing the <a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcIII/ChangeOfVariables.aspx">change of variable</a> we
implicitly are using the determinant of the Jacobian but in this case it's
just 1.</p></li>
<li><p>The integral of <span class="math">\(\int_x dx = \sum_x\)</span> since <span class="math">\(x\)</span> is discrete.</p></li>
<li><p>We defined the noisy data density <span class="math">\(p({\bf x} + {\bf u})\)</span> as
<span class="math">\(P({\bf x})\)</span> since that's the most logical way to define the density
(the density should sum to 1 as expected).</p></li>
</ul>
<p>When training a generative model, we'll usually try to minimize
<span class="math">\(H(p,q) \geq H(P,Q)\)</span>, which sets an upper bound on the cross entropy
of our original discrete data distribution <span class="math">\(P({\bf x})\)</span> and our
"discretized" continuous density distribution <span class="math">\(Q({\bf x})\)</span>. In a similar
way to the discrete models above, we can then also be interpret the loss as
the average number of bits to encode the image (or "bits per pixel" if dividing
through by the total number of pixels).</p>
<p>On a final note, as before, we would typically assume IID data for our
<span class="math">\(N\)</span> training data points <span class="math">\({\bf x}\)</span> and that we would draw <span class="math">\(M\)</span>
uniform noise samples for each data point, which leads us to:</p>
<div class="math">
\begin{align*}
H(p,q) &amp;= \int p({\bf y})\log q({\bf y})d{\bf y} \\
       &amp;= \sum_{i=1}^N P({\bf x_i}) \int_{\bf u\in [0,1]^D} \log q({\bf x_i} + {\bf u})d{{\bf u}} \\
       &amp;= \sum_{i=1}^N \frac{1}{N} \int_{\bf u\in [0,1]^D} \log q({\bf x_i} + {\bf u})d{{\bf u}} \\
       &amp;\approx \sum_{i=1}^N \frac{1}{N} \big[\frac{1}{M}\sum_{j=1}^M \log q({\bf x_i} + {\bf u_j})\big] \\
       &amp;= \frac{1}{NM} \sum_{i=1}^N \sum_{j=1}^M \log q({\bf x_i} + {\bf u_j}) \\
       \tag{16}
\end{align*}
</div>
<p>So after all, our loss function is still just the average log-likelihood with
the addition that we're averaging over <span class="math">\(M\)</span> uniform noise samples per data
point.  Usually we just draw a single uniform noise sample per data point, per
epoch, which, given enough iterations, will also converge to the same value.
Note the caveat is that this "trick" is just estimating the upper bound
of "bits per pixel" because of our use of a continuous density output of our
model.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>A bit of a digression but, as usual, I came across it while investigating a
topic but realized I didn't fully understand a supporting idea.  I really like
some of these side posts because they explain in fundamental terms (i.e.
probability), ideas that are taken for granted in many papers.  What's nice is
that these explanations can help all those unwitting souls (like myself) who
want a deeper understanding of the throwaway lines you commonly see in ML papers.
Hope you liked it!</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li><p>[1] "A note on the evaluation of generative models", Lucas Theis, Aäron van den Oord, Matthias Bethge, <a class="reference external" href="http://arxiv.org/abs/1511.01844">http://arxiv.org/abs/1511.01844</a></p></li>
<li><dl class="simple">
<dt>Stack Exchange Questions:</dt>
<dd><ul>
<li><p>[2] <a class="reference external" href="https://math.stackexchange.com/questions/2818318/probability-that-a-sample-is-generated-from-a-distribution">Probability that a Continuous Event is Generated from a Distribution</a></p></li>
<li><p>[3] <a class="reference external" href="https://math.stackexchange.com/questions/920241/can-an-observed-event-in-fact-be-of-zero-probability">Zero Probability Event</a></p></li>
</ul></dd>
</dl></li>
<li><p>[4] "Pixel Recurrent Neural Networks," Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, <a class="reference external" href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a>.</p></li>
<li><p>[5] "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications," Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, <a class="reference external" href="http://arxiv.org/abs/1701.05517">http://arxiv.org/abs/1701.05517</a>.</p></li>
<li><p>Previous posts: <a class="reference external" href="../autoregressive-autoencoders/">Autoregressive Autoencoders</a>, <a class="reference external" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/">Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders</a>, <a class="reference external" href="../pixelcnn/">PixelCNN</a></p></li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>Imagine you have a single data point <span class="math">\(x=0\)</span> and you are trying to learn a normal distribution.  What's the best fit normal distribution?  Well definitely you want the mean to be 0 but what about the variance?  Of course, you want it to be infinitesimally small which maximizes the density, but this means in the limit the density tends to infinity causing the degenerate scenario described above.</p>
</dd>
</dl>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/generative-models/" rel="tag">generative models</a></li>
            <li><a class="tag p-category" href="../../categories/log-likelihood/" rel="tag">log-likelihood</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../pixelcnn/" rel="prev" title="PixelCNN">Previous post</a>
            </li>
            <li class="next">
                <a href="../model-explanability-with-shapley-additive-explanations-shap/" rel="next" title="Model Explainability with SHapley Additive exPlanations (SHAP)">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
