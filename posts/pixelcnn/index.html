<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A post of PixelCNN generative models.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PixelCNN | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/pixelcnn/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="A post of PixelCNN generative models.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/" title="Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="PixelCNN">
<meta property="og:url" content="http://bjlkeng.github.io/posts/pixelcnn/">
<meta property="og:description" content="A post of PixelCNN generative models.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-07-22T07:11:09-04:00">
<meta property="article:tag" content="autoregressive">
<meta property="article:tag" content="CIFAR10">
<meta property="article:tag" content="generative models">
<meta property="article:tag" content="mathjax">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">PixelCNN</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                    Brian Keng
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2019-07-22T07:11:09-04:00" itemprop="datePublished" title="2019-07-22 07:11">2019-07-22 07:11</time></a></p>
            
        <p class="sourceline"><a href="index.rst" id="sourcelink">Source</a></p>

                <meta name="description" itemprop="description" content="A post of PixelCNN generative models.">
</div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>It's been a long time coming but I'm finally getting this post out!  I read
this paper a couple of years ago and wanted to really understand it because it
was state of the art at the time (still pretty close even now).  As usual
though, once I started down the variational autoencoder line of posts, there
was always <em>yet</em> another VAE paper to look into so I never got around to
looking at this one.</p>
<p>This post is all about a proper probabilistic generative model called Pixel
Convolutional Neural Networks or PixelCNN.  It was originally proposed
as a side contribution of Pixel Recurrent Neural Networks in [1] and later
expanded upon in [2,3] (and I'm sure many other papers).  The real cool thing
about it is that it's (a) probabilistic, and (b) autoregressive.  It's still
counter-intuitive to me that you can generate images one pixel at at time, but
I'm jumping ahead of myself here.  We'll go over some background material, the
method, and my painstaking attempts at an implementation (and what I learned
from it).  Let's get started!</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Autoregressive Generative Models </h4>
<p>Before we begin, we should review autoregressive generative models.
I'll basically summarize what I wrote in one of my previous post:
<a class="reference external" href="../autoregressive-autoencoders">Autoregressive Autoencoders</a>.</p>
<p>An <a class="reference external" href="https://en.wikipedia.org/wiki/Autoregressive_model">Autoregressive model</a>
is usually used in the context of time-series modelling
(of <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_process">random processes</a>)
where <span class="math">\(y_n\)</span> depends on <span class="math">\(y_{n-1}\)</span> or some earlier value.
In particular, literature usually assume a linear dependence and name these
"AR" models of the "ARIMA" notoriety.  Here "auto" refers to self, and
"regressive" means regressed against.</p>
<p>In the context of deep generative models, we'll drop the condition of linear
dependence and formulate our image problem as a random process.  In particular,
we will:</p>
<ol class="loweralpha simple">
<li>Use a deep generative models (obviously non-linear), and</li>
<li>Assume the pixels of an image are random variables with a specific ordering
(top to bottom, left to right), which formulates it as a random process.</li>
</ol>
<p>With that in mind, let's review the
<a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">product rule</a>:</p>
<div class="math">
\begin{equation*}
p({\bf x}) = \prod_{i=1}^{D} p(x_i | {\bf x}_{&lt;i})  \tag{1}
\end{equation*}
</div>
<p>where <span class="math">\({\bf x}_{&lt;i} = [x_1, \ldots, x_{i-1}]\)</span>.  Basically, component
<span class="math">\(i\)</span> of <span class="math">\({\bf x}\)</span> only depends on the dimensions of <span class="math">\(j &lt; i\)</span>.
In your head, you can think of each <span class="math">\(x_i\)</span> as a pixel.  So each pixel is
going to have a probability distribution that is a function of all the
(sub-)pixels that came before it (for RGB images, each of "R", "G", "B" are
treated as separate sub-pixels).</p>
<p>The way to generate an image from an autoregressive generative model is as follows:</p>
<ol class="arabic simple">
<li>Naturally, the first (sub-)pixel in our sequence has nothing before it so it's
a pure unconditional distribution.  We simply sample from this
distribution to get a concrete realization for the first (sub-)pixel.</li>
<li>Each subsequent (sub-)pixel distribution is generated in sequence conditioned on
all (or a subset of) previously sampled (sub-)pixels.  We simply sample from
this conditional distribution to get the current pixel value.</li>
<li>Repeat until you have the entire image.</li>
</ol>
<p>According to my intuition, this is a really weird way to generate an image!
Think of it, if you want to generate a picture of a dog, you start at the top
left pixel, figure out what it is, then move on to the one beside it, and so
on, until somehow you have an image of a dog?
This goes against my human sensibilities of generating images where I
implicitly have a concept of a dog and recursively fill in broad
regions of the image getting to finer detail as I go along.
In any case, we can still get good negative log-likelihood (although
the quality of the images are another story), so that's all that counts (?).</p>
<p><br></p>
<h4> PixelCNN </h4>
<p>Now that we covered autoregressive generative models, PixelCNN is
not too difficult to understand.  We want to build a <em>single</em> CNN that takes as
input an image and outputs a <em>distribution</em> for each (sub-)pixel (theoretically,
you could have a different network for each pixel but that seems inefficient).
There are a couple subtleties when doing that:</p>
<ol class="loweralpha simple">
<li>Due to the autoregressive nature, output pixel <span class="math">\(i\)</span> should not see any
input pixels <span class="math">\(\geq i\)</span>, otherwise it wouldn't be autoregressive (you
could "see the future").</li>
<li>Selecting a distribution and its corresponding loss function in order to
model the output pixels.</li>
</ol>
<p>Let's take a look at each one separately.</p>
<p></p>
<h5> Masked Convolution </h5>
<p>The masked convolution is basically the same idea as the masked autoencoder
from my post on MADE:
<a class="reference external" href="../autoregressive-autoencoders">Autoregressive Autoencoders</a>.
As stated above, we impose (an arbitrary) ordering on the sub-pixels:
top to bottom, left to right, R to G to B.  Given this, we want to make sure
input pixels <span class="math">\(\geq i\)</span> are "hidden" from output pixel <span class="math">\(i\)</span>, and we can
accomplish this with <em>masks</em>.  This is shown on the left side of Figure 1 where
output pixel <span class="math">\(i\)</span> only "reads" from its predecessors (the center image is the
same thing for a larger multi-scale resolution).  This can easily be generated
using a "mask" implemented as an element-wise multiplication with a
convolution kernel.  We'll talk about the implementation of this later on.</p>
<div class="figure align-center">
<img alt="PixelCNN Mask" src="../../images/pixelcnn_mask.png" style="width: 600px;"><p class="caption">Figure 1: PixelCNN Mask (source: [1])</p>
</div>
<p>However, the mask doesn't just deal with the spatial location of full pixels,
it also has to take into account the RGB values, which leads us to two
different types of masks: A and B.  For the first convolution
layer on the original image, the same rule applies as above, never read ahead.
For full pixel <span class="math">\(i\)</span>'s mask when reading from pixel <span class="math">\(x_i\)</span>, the "B"
pixel should only be connected to "G" and "R"; the "G" pixel should only be
connected to "R"; and the "R" pixel shouldn't be connected at all to pixel
<span class="math">\(i\)</span>.  You can see the connectivity on the right side of Figure 1
(Note you still have full "read" access to all sub-pixels from predecessors,
the differences for these masks only affect the current pixel).</p>
<p>For layers other than the first one, things change.  Since any sub-pixel output
from a convolution layer has already been masked from it's corresponding
input sub-pixel, you are free to use it in another convolution.  That might
be a bit confusing but you can see this in Mask B in Figure 1.  For example,
the output of the "G" sub-pixel in Mask B, depends on the "G" sub-pixel output
from Mask A, which in turn depends only on the "R".  That means "G" from the
second layer only depends on "R" from the original input, which is what we
wanted.  If we didn't do this, the "G" output from Mask B would never be able
to "read" from the "R" sub-pixel in the original input.  If this is just
confusing, just take a minute to study the connectivity in the diagram and I
think it should be pretty clear.</p>
<p>Keep in mind Figure 1 really only shows the case where you have three color
channels.  In all convolution layers beyond the first one, we likely have many
different filters per layer.  Just imagine instead of "RGB", we have
"RRRRGGGGBBBB" as input to Mask B, and you can probably guess what the
connectivity should look like.  Again this only applies to the current
sub-pixels, we should have full connectivity to all spatial predecessors.</p>
<p></p>
<h5> Discretized Logistic Mixture Likelihood </h5>
<p>I covered this topic in my previous post in detail
(<a class="reference external" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders">Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders</a>) so I'll just do a summary here.</p>
<p>Modelling pixels is a funny problem.  On one hand, we can view them as 8-bit
discrete random variables (e.g. 256-way softmax), but on the other hand, we can
view them as a sort-of continuous distribution (e.g. real-valued output).
The former approach is the way that [1] tackles the problem: just stick a
256-softmax output on each sub-pixel.  There are two issues: (1) it uses up a
gigantic amount of resources, and (2) there is no relationship between adjacent
pixel values (R=100 should be similar to R=101).  Mostly due to the first reason,
I opted out to not implement it (couldn't fit a full image on my meager GTX 1070).</p>
<p>The other way to go about it is to imagine the pixel generation process as such described in [2]:</p>
<ol class="arabic">
<li>
<p class="first">For each sub-pixel, generate a continuous distribution <span class="math">\(\nu\)</span>
representing the intensity.  For example, <span class="math">\(\nu\)</span> could be a
<a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_distribution">logistic distribution</a>
parameterized by <span class="math">\(\mu, s\)</span>.</p>
</li>
<li>
<p class="first">Next, "round" each sub-pixel to a
discretized distribution over <span class="math">\([0, 255] \in \mathbb{Z}\)</span> by
integrating over the appropriate width along the real line (assuming a
logistic distribution):</p>
<div class="math">
\begin{equation*}
P(x|\mu,s) =
    \begin{cases}
        \sigma(\frac{x-\mu+0.5}{s}) &amp; \text{for } x = 0 \\
        \sigma(\frac{x-\mu+0.5}{s}) - \sigma(\frac{x-\mu-0.5}{s})
            &amp; \text{for } 0 &lt; x &lt; 255 \\
        1 - \sigma(\frac{x-\mu-0.5}{s}) &amp; \text{for } x = 255
    \end{cases}
\tag{2}
\end{equation*}
</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function (recall sigmoid is the CDF of
the logistic distribution).
Here we basically take the <span class="math">\(\pm 0.5\)</span> interval around each pixel value
to compute its discretized probability mass.  For the edges, we just
integrate to infinity.</p>
</li>
</ol>
<p>However, this will make each pixel uni-modal, which doesn't afford us much
flexibility.  To improve it, we use a mixture of logistics for <span class="math">\(\nu\)</span>:</p>
<div class="math">
\begin{equation*}
\nu \sim \sum_{i=1}^K \pi_i logistic(\mu_i, s_i) \tag{3}
\end{equation*}
</div>
<p>where <span class="math">\(\pi_i\)</span> is the categorical weight.  To bring it back to neural
networks, for each sub-pixel, we want our network to output three things:</p>
<ul class="simple">
<li>
<span class="math">\(K\)</span> <span class="math">\(\mu_i\)</span>'s representing the centers of our logistic distributions</li>
<li>
<span class="math">\(K\)</span> <span class="math">\(s_i\)</span>'s representing the scale of our logistic distributions</li>
<li>
<span class="math">\(K\)</span> <span class="math">\(\pi_i\)</span>'s representing the mixture weights (summing to 1)</li>
</ul>
<p>We don't need that many mixture components to get a good result ([2] uses
<span class="math">\(K=5\)</span>), which is a lot less than a 256-way softmax.  Figure 2 shows
a realization of a toy discretized mixture I did while testing.  You can see
clearly that we can model multi-modal distributions, and at the same time
have a lot of mass at the 0 and 255 pixels (which [2] claim is a good thing
because black and white occur a lot in images?).</p>
<div class="figure align-center">
<img alt="Distribution of Discretized Logistic Mixtures" src="../../images/pixelcnn_histogram.png" style="width: 400px;"><p class="caption">Figure 2: Distribution of Discretized Logistic Mixtures: Top to
bottom represent the "R", "G", "B" components of a single pixel</p>
</div>
<p>All of this can be implemented pretty easily by strapping a bunch of masked
convolution layers together and setting the last layer to have <span class="math">\(3*3K\)</span>
filters (one for each sub-pixel).  The really interesting stuff happens in the
loss function though which computes the negative log of Equation 2.  The nice thing
about this loss is that it's fully probabilistic from the start.
We'll get to implementing it in a later section but suffice it to say, it's not
easy dealing with overflow!</p>
<p></p>
<h5> Training and Generating Samples </h5>
<p>Training this network is actually pretty easy. All we have to do is make
the actual image available on the input and output of the network.
Note the input tensor of the network takes an image, while the output
of the network outputs a distribution for each sub-pixel.  You still use
the image on both the input and output but use a custom
loss function that computes the negative log-likelihood using the network
outputs.  Other than the complexity of the loss function, you just set it and go!</p>
<p>Generating images is something that is a bit more complicated but follows the same idea from my post on <a class="reference external" href="../autoregressive-autoencoders">Autoregressive Autoencoders</a>:</p>
<ol class="arabic simple" start="0">
<li>Set <span class="math">\(i=0\)</span> to represent current iteration and pixel (implicitly we
would translate it to the row/col/sub-pixel in the image).</li>
<li>Start off with a tensor for your image <span class="math">\(\bf x^0\)</span> with any initialization
(it doesn't matter).</li>
<li>Feed <span class="math">\(\bf x^i\)</span> into the PixelCNN network to generate distributional
outputs <span class="math">\(\bf y^{i+1}\)</span>.</li>
<li>Randomly sample sub-pixel <span class="math">\(u_{i+1}\)</span> from the mixture distribution
defined by <span class="math">\(\bf y^{i+1}\)</span> (we only need the subset of values for
sub-pixel <span class="math">\(i+1\)</span>).</li>
<li>Set <span class="math">\(\bf x^{i+1}\)</span> as <span class="math">\(\bf x^i\)</span> but replacing the current sub-pixel with
<span class="math">\(u_{i+1}\)</span>.</li>
<li>Repeat step 2-4 until entire image is generated.</li>
</ol>
<p>From this algorithm, you can see that we're generating one pixel at a time, a
very slow process!  For a 32x32x3 image, we basically need to do a forward
pass of the network 3072 times for a single image (we have some
parallelism because we can do several images in batch but of course we have the
"9 women making 9 babies in a month" problem).  This is the downside of
autoregressive models: training is done in parallel (for convolutional
autoregressive models) but generation is sequential (and slow).  As a data
point, my slow implementation took almost 37 mins to generate 16 images
(forward passes were parallelized on the GPU but sampling was sequential in a
loop on the CPU).</p>
<p><br></p>
<h4> Implementation Details </h4>
<p>So far the theory isn't too bad: some masked layers, extra outputs and some
sigmoid losses and we're done, right?  Well it's a bit trickier than that,
especially the loss function.  I'll explain the details (and headaches) that I went
through implementing it in Keras.  As usual, you can find all my code in this
<a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/notebooks/pixel_cnn/pixelcnn.ipynb">Github repo</a>.</p>
<p></p>
<h5> Masked Convolution Layer </h5>
<p>The masked convolution layer (which I named <tt class="docutils literal">PixelConv2D</tt>) was actually
pretty easy to implement in Keras because I just inherited from the <tt class="docutils literal">Conv2D</tt>
layer, build a binary mask and then did an element-wise product with the kernel.
There's just a bit of accounting that needs to go on in building the mask such
as ensuring that your input is a multiple of 3 and that the right bits are set.
This probably isn't the most efficient method of doing it because you literally
are multiplying by a binary matrix every time, but it probably is the easiest.</p>
<p></p>
<h5> PixelCNN Outputs </h5>
<p>The output of the network is a distribution, but how is that realized?
The last layer is composed is made up of three sets of <tt class="docutils literal">PixelConv2D</tt> layers
representing:</p>
<ul class="simple">
<li>Logistic mean values <span class="math">\(\mu\)</span>, filters = # of mixture component, no activation</li>
<li>Logistic log of inverse scale values <span class="math">\(s\)</span>, filters = # of mixture components,
"softplus" activation function</li>
<li>Pre-softmax mixture inputs, filters = # of mixture components, no activation</li>
</ul>
<p>The mean is pretty straight forward.  We put no restriction on it being in
our normalized pixel interval (i.e. <span class="math">\([-1, 1]\)</span>), which seems to work out
fine.</p>
<p>The network output corresponding to scale is set to be an inverse because we
never want to divide by 0.  As for modelling the output as the logarithm, I
suspect (but haven't observed) that it's just a better match for neural network
output ranges.  For example, your network needs to output <span class="math">\(6\)</span> instead of
<span class="math">\(e^6\)</span>, where the latter will have to have huge weights on the last layer.
The "softplus" seems to be the best fit here because it's very smooth (unlike
"ReLU"), and the non-negative logarithm values ensure <span class="math">\(s &lt; 1\)</span>.  Since
we're dealing with normalized pixels between <span class="math">\([-1, 1]\)</span>, we would never
want a shape parameter wider than half the interval (that would just put
almost all the mass on the end points).  In fact, I've observed log inverse
scale to be in the 3-5 range, meaning the shape parameter is as small as
<span class="math">\(\frac{1}{e^5}\)</span>.</p>
<p>Finally, the network's output corresponding to the mixture components are
the <em>inputs</em> to the softmax, essentially the "pre-softmax".  This is done
because in the loss function we compute the <span class="math">\(\log\)</span> of the softmax, which
is numerically more stable to compute if we have the raw pre-softmax inputs
rather than the direct softmax outputs.  It's a small change and just requires a
few extra processing steps when we're actually generating images to get the
mixture weights.</p>
<p></p>
<h5> Network Architecture </h5>
<p>I used the same architecture as the PixelCNN paper [1] except for the
outputs where I used logistic mixture outputs instead of a softmax (as
described above):</p>
<ul class="simple">
<li>5x5 Conv Layer, Mask A, ReLU</li>
<li>15 - 3x3 Resnet Blocks, Mask B, <span class="math">\(h=128\)</span> (shown in Figure 3)</li>
<li>2 - 1x1 Conv layers, Mask B, ReLU, <span class="math">\(filters=1024\)</span>
</li>
<li>Output layers (as described above)</li>
</ul>
<div class="figure align-center">
<img alt="PixelCNN Resnet Block" src="../../images/pixelcnn_resnet.png" style="width: 200px;"><p class="caption">Figure 3: PixelCNN Resnet Block (source [1])</p>
</div>
<p>The network widths above are for each colour channel, which are concatenated
after each operation and fed into the next <tt class="docutils literal">PixelConv2D</tt> which know how to
deal with them via masks.</p>
<p></p>
<h5> Loss Function </h5>
<p>The loss function was definitely the hardest part of the entire implementation.
There are so many subtleties, I don't know where to begin.  And this was <em>after</em>
I heavily referenced the PixelCNN++ code [4].  Let's start with
computing the log-likelihood shown in Equation 2.  There are actually 4 different
cases (actual condition in parenthesis when scaled to pixel range <span class="math">\([-1,1]\)</span>).</p>
<p>Note: I'm using <span class="math">\(0.5\)</span> in the equations below but substitute
<span class="math">\(\frac{1}{2(127.5)}\)</span> for the rescaled pixel range <span class="math">\([-1,1]\)</span>.</p>
<p><strong>Case 1 Black Pixel</strong>: <span class="math">\(x\leq 0\)</span> (<span class="math">\(x &lt; -0.999\)</span>)</p>
<p>Here we just need to do a bit of math to simplify the expression:</p>
<div class="math">
\begin{equation*}
\log\big( \sigma(\frac{x-\mu+0.5}{s}) \big)
= \frac{x-\mu+0.5}{s} - \text{softplus}(\frac{x-\mu+0.5}{s})
\tag{4}
\end{equation*}
</div>
<p>where softplus is defined as <span class="math">\(\log(e^x+1)\)</span>, see this
<a class="reference external" href="https://math.stackexchange.com/questions/2320905/obtaining-derivative-of-log-of-sigmoid-function">Math Stack Exchange Question</a> for more details.</p>
<p><strong>Case 2 White Pixel</strong>: <span class="math">\(x\geq 255\)</span> (<span class="math">\(x &gt; 0.999\)</span>)</p>
<p>Again, simply a simplification of Equation 2:</p>
<div class="math">
\begin{equation*}
\log\big(1 - \sigma(\frac{x-\mu-0.5}{s}) \big)
= -\text{softplus}(\frac{x-\mu-0.5}{s})
\tag{5}
\end{equation*}
</div>
<p>If you just expand out the sigmoid into exponentials, this should be a pretty
easy to derive.</p>
<p><strong>Case 3 Overflow Condition</strong>: <span class="math">\(\sigma(\frac{x-\mu+0.5}{s}) - \sigma(\frac{x-\mu-0.5}{s}) &lt; 10^{-5}\)</span></p>
<p>This is the part where we have to be careful.  Even if we don't have a black or
white pixel, we can still overflow.  Imagine the case where the centre of our logistic
distribution is way off from our pixel range e.g. <span class="math">\(\mu=1000, s=1\)</span>.  This means
that any pixel <span class="math">\(x\)</span> within the <span class="math">\([-1, 1]\)</span> range will be incredibly
close to <span class="math">\(0\)</span> (remember we don't have infinite precision) since it's so
far out in the tail of the distribution.  As such, the difference will also be
nearly zero and when we try to take the logarithm, we get <span class="math">\(-\infty\)</span> or NaNs.</p>
<p>Interestingly enough, the code from PixelCNN++ [4] says that this condition
doesn't occur in their code, but for me it definitely happens.  I took this
condition out and I started getting NaNs everywhere.  It's possible with
their architecture it doesn't happen but I suspect either their comment is
misleading or they just didn't do a lot of checks.</p>
<p>Anyways, to solve this problem, we actually approximate the integral (area
under the PDF) by taking the centered PDF of the logistic and multiply it by a
pixel width interval (<span class="math">\(\frac{1}{127.5}\)</span> is a pixel width interval in
our [-1, 1] range):</p>
<div class="math">
\begin{align*}
\log(\text{PDF} \cdot \frac{1}{127.5})
&amp;= \log\big(\frac{e^{-(x-m)/s}}{{s(1 + e^{-(x-m)/s})^2}}\big)-\log(127.5) \\
&amp;= -\frac{x-m}{s} - \log(s) - 2\log(1+e^{-(x-m)/s}) - \log(127.5) \\
&amp;= -\frac{x-m}{s} - \log(s) - 2\cdot\text{softplus}(-\frac{x-m}{s}) - \log(127.5)
\\ \tag{6}
\end{align*}
</div>
<p>If that weren't enough, I did some extra work to make it even more precise!
This was not in the original implementation [4], and actually in retrospect,
I'm not sure if it even helps but I'm going to explain it anyways since I spent
a bunch of time on it.</p>
<p>For Equation 6, it obviously is not equivalent to the actual expression
<span class="math">\(\log\big(\sigma(\frac{x+0.5-\mu}{s}) - \sigma(\frac{x-0.5-\mu}{s})\big)\)</span>.
So at the cross over point of <span class="math">\(10^{-5}\)</span>, there must be some sort of
discontinuity in the loss.  This is shown in Figure 4.</p>
<div class="figure align-center">
<img alt="PixelCNN Loss Discontinuity for Edge Case" src="../../images/pixelcnn_discontinuity1.png" style="height: 250px;"><p class="caption">Figure 4: PixelCNN Loss Discontinuity for Edge Case</p>
</div>
<p>For various values of <span class="math">\(\text{invs}=\log(\frac{1}{e^s})\)</span>, I plotted the
discontinuity as a function of the centered x values.  The dotted line
represents the cross over point.  When you are very far away from the center
(larger x values), the exception case kicks in, but when we get closer
to being in the right region, we get the normal case.
As we get a tighter distribution (larger invs), the point at which the
exception case kicks in is sooner.
The problem is the exception case has a <em>smaller</em> loss than the normal case,
which means it's possible that it might flip/flop between the two cases.</p>
<p>To adjust for it, I added a line of best fit as a function of invs.
Figure 5 shows this line of best fit (never mind the axis label where
I'm using inconsistent naming).</p>
<div class="figure align-center">
<img alt="PixelCNN Loss Adjustment for Edge Case" src="../../images/pixelcnn_discontinuity2.png" style="height: 400px;"><p class="caption">Figure 5: PixelCNN Loss Adjustment for Edge Case</p>
</div>
<p>As you can see, the line of best fit is pretty good up to
<span class="math">\(\text{invs}\approx 6\)</span>.  In most cases, I haven't observed such a tight
distribution on the outputs so it probably will serve us pretty well.
As I mentioned above, I'm actually kind of skeptical that it makes a difference
but here it is anyways.</p>
<p><strong>Case 4 Standard Scenario</strong>: Otherwise</p>
<p>We simply just directly compute the logarithm of the middle case of Equation 2 with
a protection on the difference:</p>
<div class="math">
\begin{equation*}
\log\big(\text{max}(\sigma(\frac{x-\mu+0.5}{s}) - \sigma(\frac{x-\mu-0.5}{s}), e^{-12})\big) \tag{7}
\end{equation*}
</div>
<p>I'm not sure if the <cite>max</cite> operation is needed here but the code in [4] says it
helps deal with problems computing the gradients in Tensorflow.  For example,
when you are close to 0, even though this case shouldn't occur because it's
protected using a <cite>tf.where</cite> statement for Case 3, the logarithm in this branch
may still be computed and generate a NaN, which could screw up the computation
(remember <cite>tf.where</cite> statements still have a gradient!).  Anyways, didn't test
it out much, just left it as is since it's pretty harmless.</p>
<p></p>
<h5> Implementation Strategy </h5>
<p>One thing about these types of models is that they're complex!  It's actually
very easy to have a small bug that takes a <em>long</em> time to debug.  After many
false starts trying to get it working in one shot, I decided to actually do
best practice and start small.  The way I did this was incrementally test out
new parts and then slowly put the pieces together.  These are the iterations
that I did (which correspond to the different notebooks you'll see on
<a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/notebooks/pixel_cnn/">Github</a>):</p>
<ul class="simple">
<li>
<cite>PixelConv2D</cite> layer: making sure that I got all the masking right.  I made
several toy networks ("unit tests") just to make sure the input was masked properly.</li>
<li>Next, I generated 2x2 RGB images from a known logistic
distribution for each sub-pixel.  Using this approach I could actually take a
look at the distributional parameter outputs, plot the distribution for each pixel
and then compare to actuals.  The network I used for this was essentially
just an output layer because I was testing how the loss function behaved.  I
spent <em>a lot</em> of time here trying to understand what was going on in the loss
function.  It was very helpful.</li>
<li>As a continuation, I generated the same RGB images but with a mixture of
logistics.  This was a natural extension of the previous iteration and
allowed me to test out the logic I had for mixtures.</li>
<li>As a further continuation, I put together the actual network I wanted to use
(with fewer Resnet layers).  I had a couple of bugs here too.</li>
<li>After I was more or less sure that things were working on the toy example,
I moved to the actual CIFAR10 images.  Here I started out with a single image
and tiny slices of it (e.g. 2x2, 4x4, 8x8, etc.) working my way up to the
entire image.  I wanted to see if I could overfit on a single example,
which would give me some indication that I was on the right track.</li>
<li>Naturally, I extended this to 2 images, then multiple images, finally the
entire dataset.</li>
</ul>
<p>One thing that I found incredibly useful is to take notes for each set of
experiments... duhhh!  I know it's obvious but it's easy to be lazy when you're
working on your own.  You'll see at the bottom of the notebook I put some notes
for each time I was working on it.  You'll see some of the frustration and
false starts that I went through.</p>
<p>Besides the obvious reasons why it's good to document progress, it was extra
helpful because I only get to work on this stuff so sporadically that it was
sometimes a month or two between sessions.  Try remembering what tweaks you
were doing to your loss function from two months ago!  Anyways, it was really
helpful because I could re-read my previous train of thought and then move on
to my next experiment.  Highly recommend it.</p>
<p>Another small thing that I did was that I prototyped everything in a notebook
first and then as I was more confident that it worked, I moved it into a Python
file.  This was helpful because each notebook didn't have it's own (perhaps out
of date) copy of a function.  It also made the notebooks a bit nicer to read.  I
would recommend doing the first prototype in a notebook though, you want the
agility of modifying things on the fly with the least friction.</p>
<p><br></p>
<h4> Experiments </h4>
<p>Based on the implementation above, I was able to train a PixelCNN network.
Some images generated are shown in Figure 6.  Not very impressive...
One thing that I think would make it better is to actually increase the convolution
size.  I'm just using a 5x5 window where with masking, only results in roughly
a 3x5 window.  I suspect going a bit bigger might help the image look better,
similarly with the internal 3x3 convolutions.  PixelCNN++ actually has varying
internal kernel sizes to capture different resolutions.</p>
<p>One thing that I struggled with for a long time was that I was generating crappy
images like the ones below.  I tried really hard to get images that looked like
the original ones. However, when I looked closer at the generated PixelCNN
images, they also had a mishmash of things.  They also weren't generating
anything intelligible either (although theirs did look much better than mine).
In any case, since we're not expecting photo-realistic images like GANs, I
decided to move on.</p>
<div class="figure align-center">
<img alt="PixelCNN Generated Images" src="../../images/pixelcnn_images.png" style="width: 400px;"><p class="caption">Figure 6: PixelCNN Generated Images</p>
</div>
<p>What I was relatively happy with are my loss performance results shown in Table 1.
Using the bits/pixel metric (see my discussion of this metric from my
<a class="reference external" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders">Importance Sampling</a>
post), I was able to achieve somewhat close results to the paper: 3.4 (mine) vs
3.14 ([1]).</p>
<p>It's not too surprising the results are different.  I tried to use the same
architecture as PixelCNN [1] <em>except</em> for how they model their output pixels.
For that I used the PixelCNN++ [2] paper.  PixelCNN has <em>more</em> flexibility in the
output layer being a 256-way softmax so you would expect it do a bit
better.  PixelCNN++ does do better overall but I think it's because of their
architectural changes with varying resolutions of conv layers compared with a
Resnet-like architecture of PixelCNN.</p>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">&lt;string&gt;</tt>, line 582)</p>
<p>Error in "csv-table" directive:
unknown option: "align".</p>
<pre class="literal-block">
.. csv-table:: Table 1: PixelCNN Loss on CIFAR10 (bits/pixel)
   :header: "Model", "Training Loss", "Validation Loss"
   :widths: 15, 10, 10
   :align: center

   "My Implementation", 3.40, 3.41
   "PixelCNN [1]", \-, 3.14
   "PixelCNN++ [2]", \-, 2.92

</pre>
</div>
<p>I trained the PixelCNN to reduce the learning rate on a loss plateau <a class="footnote-reference" href="#id2" id="id1">[1]</a> .
Interestingly, I couldn't get the architecture to overfit.  Somehow the
validation loss is incredibly close to the training loss.  This is in
contrast to the PixelCNN++ paper, which states that overfitting is a major
problem (which they address via dropout).  This kind of suggests that the
vanilla Resnet architecture probably isn't the most efficient for this task,
which the authors of [1] change up in their extension paper in [3].  Still, I'm
pretty happy with the results, which is my best CIFAR-10 loss to date.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>I'm so glad that I finally got to implement and write about this post.  It's
been a long time coming and was particularly hard this past year where I've been
so incredibly busy with work both at the company and university.  In the two
years since I wanted to write a post on this paper, there have been at least
another half dozen papers that have come out that I want to write posts on.
It's a bit of a never-ending cycle but that's what makes it fun!  Hopefully
you won't have to wait <em>another</em> two years for me to get to those papers!</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li>My code on Github: <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/notebooks/pixel_cnn/">Github repo</a>
</li>
<li>[1] "Pixel Recurrent Neural Networks," Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, <a class="reference external" href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a>.</li>
<li>[2] "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications," Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, <a class="reference external" href="http://arxiv.org/abs/1701.05517">http://arxiv.org/abs/1701.05517</a>.</li>
<li>[3] "Conditional Image Generation with PixelCNN Decoders," Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu, <a class="reference external" href="https://arxiv.org/abs/1606.05328">https://arxiv.org/abs/1606.05328</a>
</li>
<li>[4] PixelCNN++ code on Github: <a class="reference external" href="https://github.com/openai/pixel-cnn">https://github.com/openai/pixel-cnn</a>
</li>
<li>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Autoregressive_model">Autoregressive model</a>
</li>
<li>Previous posts: <a class="reference external" href="../autoregressive-autoencoders">Autoregressive Autoencoders</a>, <a class="reference external" href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders">Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders</a>
</li>
</ul>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>I needed to start relatively small in the learning rate at 0.002 and then gradually reduce the learning rate or else the loss plateaus.  One thing that I think happens is because we're modifying the scale parameter <span class="math">\(s\)</span>, you never want big jumps or else the loss changes dramatically.  Thus, as you get in the right area with <span class="math">\(\mu\)</span>, you want to learn slower to "fine-tune" <span class="math">\(s\)</span>.</td>
</tr></tbody>
</table>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/autoregressive/" rel="tag">autoregressive</a></li>
            <li><a class="tag p-category" href="../../categories/cifar10/" rel="tag">CIFAR10</a></li>
            <li><a class="tag p-category" href="../../categories/generative-models/" rel="tag">generative models</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/" rel="prev" title="Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders">Previous post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/all-nocdn.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
