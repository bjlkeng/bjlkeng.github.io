<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="An *explanation* of SHapley Additive exPlanations (SHAP)">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Model Explainability with SHapley Additive exPlanations (SHAP) | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/model-explanability-with-shapley-additive-explanations-shap/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../a-note-on-using-log-likelihood-for-generative-models/" title="A Note on Using Log-Likelihood for Generative Models" type="text/html">
<link rel="next" href="../lossless-compression-with-asymmetric-numeral-systems/" title="Lossless Compression with Asymmetric Numeral Systems" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Model Explainability with SHapley Additive exPlanations (SHAP)">
<meta property="og:url" content="http://bjlkeng.github.io/posts/model-explanability-with-shapley-additive-explanations-shap/">
<meta property="og:description" content="An *explanation* of SHapley Additive exPlanations (SHAP)">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-02-12T07:24:22-04:00">
<meta property="article:tag" content="explainability">
<meta property="article:tag" content="game theory">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="SHAP">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Model Explainability with SHapley Additive exPlanations (SHAP)</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2020-02-12T07:24:22-04:00" itemprop="datePublished" title="2020-02-12 07:24">2020-02-12 07:24</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>One of the big criticisms of modern machine learning is that it's essentially
a blackbox -- data in, prediction out, that's it.  And in some sense, how could
it be any other way?  When you have a highly non-linear model with high degrees
of interactions, how can you possibly hope to have a simple understanding of
what the model is doing?  Well, turns out there is an interesting (and
practical) line of research along these lines.</p>
<p>This post will dive into the ideas of a popular technique published in the last
few years call <em>SHapely Additive exPlanations</em> (or SHAP).  It builds upon
previous work in this area by providing a unified framework to think
about explanation models as well as a new technique with this framework that
uses Shapely values.  I'll go over the math, the intuition, and how it works.
No need for an implementation because there is already a nice little Python
package! Confused yet?  Keep reading and I'll <em>explain</em>.</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Shapely Values and Cooperative Games </h4>
<p></p>
<h5> Cooperative Games </h5>
<p>Imagine you're on a reality TV show where you're stuck on a desert island with
two other people. The longer all of you stay on the island, the more money
you will all make.  You need to work together in order to maximize your return.
Your payoff will change significantly depending on whom you're with.
In this scenario, of course, you want to pick the two best people to bring with
you, but how to choose?  For example, you probably want to pick someone who has
extensive survivability experience, they would obviously contribute a lot to a
better outcome.  On the other hand, you probably don't want to pick someone
like me who spends the majority of their free time indoors in front a computer,
I might just be dead weight.  In essence, we want to answer two questions:</p>
<ol class="arabic simple">
<li><p>How important is each person to the overall goal of surviving longer
(i.e. what's their fair share of the winnings)?</p></li>
<li><p>What outcome can we reasonably expect?</p></li>
</ol>
<p>This is essentially the problem of a cooperative game from game theory (this is
in contrast to the traditional non-cooperative games from game theory that most
people are familiar with).  Formally:</p>
<blockquote>
<p>A <strong>coalitional game</strong> is a where there is a set <span class="math">\(N\)</span> players and a
<em>value function</em> <span class="math">\(v\)</span> that maps each subset of players to a payoff.
Formally, <span class="math">\(v: 2^N \rightarrow \mathbb{R}\)</span> with <span class="math">\(v(\emptyset)=0\)</span>
(empty set is zero).</p>
</blockquote>
<p>The value function <span class="math">\(v(S)\)</span> describes the total expected payoff
for a subset of players <span class="math">\(S\)</span>.  For example, including a survivalist in
your desert island coalition will likely result in better outcome (payoff) than
including me.  This function answers the second question: what outcome can
we reasonably expect (given a subset of players)?</p>
<p>So far we've just setup the problem though.  Now we want to answer the
question: what should be everyone's "fair" distribution of the total payoff?
In other words, the first question: how much does each person contribute to
the overall goal.  Clearly, the survivalist should get more more of the payout
than me, but how much more?  Is there only one "unique" solution?  The answer
lies in Shapely values.</p>
<p></p>
<h5> Shapely Values <a class="footnote-reference brackets" href="#id2" id="id1">1</a> </h5>
<p><strong>Shapely values</strong> (named in honour of Lloyd Shapely), denoted by
<span class="math">\(\varphi_i(v)\)</span>, are a solution to the above coalition game.
For player <span class="math">\(i\)</span> and value function <span class="math">\(v\)</span>, the Shapely value
is defined as:</p>
<div class="math">
\begin{align*}
\varphi_i(v) &amp;= \frac{1}{N} \sum_{S \subseteq N \setminus \{i\}}
    {N-1 \choose |S|}^{-1} (v(S\cup \{i\}) - v(S)) \tag{1} \\
    &amp;= \frac{1}{\text{number of players}} \sum_{\text{coalitions excluding }i}
        \frac{\text{marginal contribution of i to coalition}}{\text{number of coalitions excluding i of this size}} \\
    &amp;= \sum_{S \subseteq N \setminus \{i\}}
    \frac{|S|!(N-|S|-1)!}{N!} (v(S\cup \{i\}) - v(S)) \tag{2}
\end{align*}
</div>
<p>This definition is quite intuitive: average over your marginal contribution in
every possible situation. Equation 1 shows this intuition the best where you
take the incremental benefit of including player <span class="math">\(i\)</span> and average over
every possible subset that could include player <span class="math">\(i\)</span>.
Equation 2 is a simplification that you might see more often, which is just
expanding the combination and simplifying.</p>
<p>Shapely values are also a very nice because they are the <em>only</em> solution with
these desirable properties:</p>
<ol class="arabic">
<li>
<p><strong>Efficiency</strong>: The sum of Shapely values of all agents is equal to the total for the grand coalition:</p>
<div class="math">
\begin{equation*}
\sum_{i\in N} \varphi_i(v) = v(N)
\end{equation*}
</div>
</li>
<li>
<p><strong>Symmetry</strong>: If <span class="math">\(i\)</span> and <span class="math">\(j\)</span> are two players who are equivalent in the sense that</p>
<div class="math">
\begin{equation*}
v(S \cup \{i\}) = v(S \cup \{j\})
\end{equation*}
</div>
<p>for every subset <span class="math">\(S\)</span> of <span class="math">\(N\)</span> which contain neither <span class="math">\(i\)</span>
nor <span class="math">\(j\)</span>, then <span class="math">\(\varphi_i(v) = \varphi_j(v)\)</span>.</p>
</li>
<li>
<p><strong>Linearity</strong>: Combining two coalition games <span class="math">\(v\)</span> and <span class="math">\(w\)</span> is linear
for every <span class="math">\(i\)</span> in <span class="math">\(N\)</span>:</p>
<div class="math">
\begin{align*}
\varphi_i(v+w) = \varphi_i(v) + \varphi_i(w) \\
\varphi_i(av) = a\varphi_i(v)
\end{align*}
</div>
</li>
<li><p><strong>Null Player</strong>: For a null player, defined as <span class="math">\(v(S\cup \{i\})=v(S)\)</span>
for all coalitions <span class="math">\(S\)</span> not containing <span class="math">\(i\)</span>, then
<span class="math">\(\varphi_i(v) = 0\)</span>.</p></li>
</ol>
<p>All of these properties seem like pretty obvious things you would want to have:</p>
<ul class="simple">
<li><p><strong>Efficiency</strong>: Of course, you want your distribution across players to
actually sum up to the total reward.</p></li>
<li><p><strong>Symmetry</strong>: If two people contribute the same to the game, you want
them to have the same payoff.</p></li>
<li><p><strong>Linearity</strong>: If a game is composed of two independent sub-games, you
want the total game to be the sum of the two games.</p></li>
<li><p><strong>Null Player</strong>: If a player contributes nothing, then their share should
be nothing.</p></li>
</ul>
<p>Let's take a look at a couple examples to get a feel for it.</p>
<div class="admonition admonition-example-1-glove-game">
<p class="admonition-title">Example 1: Glove Game</p>
<p>Imagine we have a game where players have left or right handed gloves.
The goal is to form pairs.  Imagine a three player game:
<span class="math">\(N=\{1,2,3\}\)</span> where Players 1 and 2 have left handed gloves and Player
3 has a right handed glove.  The value function is scoring for every subset
that has a L/R pair, and zero otherwise:</p>
<div class="math">
\begin{equation*}
v(S) = \begin{cases}
    1 &amp; \text{if } S \in \{\{1,3\}, \{2,3\}, \{1,2,3\}\} \\
    0 &amp; \text{otherwise}
\end{cases}
\end{equation*}
</div>
<p>Using Equation 1, we need to find the marginal contribution of players.
For Player 1, we have:</p>
<div class="math">
\begin{align*}
v(\{1\}) - v(\emptyset) &amp;= 0 - 0 &amp;= 0 \\
v(\{1,2\}) - v(\{2\}) &amp;= 0 - 0 &amp;= 0 \\
v(\{1,3\}) - v(\{3\}) &amp;= 1 - 0 &amp;= 1 \\
v(\{1,2,3\}) - v(\{2, 3\}) &amp;= 1 - 1 &amp;= 0
\end{align*}
</div>
<p>From Equation 1:</p>
<div class="math">
\begin{align*}
\varphi_1(v) &amp;= \frac{1}{N} \sum_{S \subseteq N \setminus \{i\}}
    {N-1 \choose |S|}^{-1} (v(S\cup \{i\}) - v(S)) \\
 &amp;= \frac{1}{3} \big[{2 \choose 0}^{-1} (v(\{1\}) - v(\emptyset))  +
     {2 \choose 1}^{-1} (v(\{1,2\}) - v(\{2\})) + \\
 &amp;\hspace{2.3em}  {2 \choose 1}^{-1} (v(\{1,3\}) - v(\{3\})) +
    {2 \choose 2}^{-1} (v(\{1,2,3\}) - v(\{,,3\}))
 \big]  \\
 &amp;= \frac{1}{3}[1(0) + \frac{1}{2}(0) + \frac{1}{2}(1) + (1)(0)] \\
 &amp;= \frac{1}{6} \\
 \tag{3}
\end{align*}
</div>
<p>Based on symmetry (Property 2), we can conclude Player 1 and 2 are
equivalent, so:</p>
<div class="math">
\begin{align*}
\varphi_2(v) = \varphi_1(v) = \frac{1}{6} \\
\tag{4}
\end{align*}
</div>
<p>Further, due efficiency (Property 1), we can find the remaining Shapely
value for Player 3:</p>
<div class="math">
\begin{align*}
\varphi_3(v) = v(N) - \varphi_1(v) - \varphi_3(v)
= 1 - \frac{1}{6} - \frac{1}{6}
= \frac{2}{3} \\
\tag{5}
\end{align*}
</div>
<p>As expected, since Player 3 has the only right handed glove, so their
split of the profits should be 4 times bigger than the other players.</p>
</div>
<div class="admonition admonition-example-2-business">
<p class="admonition-title">Example 2: Business</p>
<p>Consider an owner of a business, denoted by <span class="math">\(o\)</span>, who provides the
initial investment in the business.  If there is no initial investment,
then there is no business (zero return).  The business also has <span class="math">\(k\)</span>
workers <span class="math">\(w_1, \ldots, w_k\)</span>, each of whom contribute <span class="math">\(p\)</span> to the
overall profit.  Our set of players is:</p>
<div class="math">
\begin{equation*}
N = \{o, w_1, \ldots, w_k\} \tag{6}
\end{equation*}
</div>
<p>The value function for this game is:</p>
<div class="math">
\begin{align*}
v(S) = \begin{cases}
    mp &amp; \text{if } o \in S \\
    0 &amp; \text{otherwise }
\end{cases} \\
\tag{7}
\end{align*}
</div>
<p>where <span class="math">\(m\)</span> is the number of players in <span class="math">\(S \setminus o\)</span>.</p>
<p>We can setup Equation 1 but breaking down the sum into symmetrical parts
based on the size of <span class="math">\(S\)</span>:</p>
<div class="math">
\begin{align*}
\varphi_{o}(v) &amp;= \frac{1}{N} \sum_{S \subseteq N \setminus \{o\}}
    {N-1 \choose |S|}^{-1} (v(S\cup \{o\}) - v(S)) \\
&amp;= \frac{1}{N} \big[
    \sum_{m=1}^k
    \sum_{S \subseteq N \setminus \{o\}, |S|=m}
    {N-1 \choose |S|}^{-1} (v(S\cup \{o\}) - v(S))
    \big] \\
&amp;= \frac{1}{N} \big[
    \sum_{m=1}^k
    {N-1 \choose |S|}^{-1} {N-1 \choose |S|} mp
    \big] &amp;&amp; \text{each of the inner summations is symmetric}\\
&amp; &amp;&amp; \text{and } v(S)=0 \text{ since there is no owner}\\
&amp;= \frac{1}{N} \sum_{m=1}^k mp \\
&amp;= \frac{1}{k+1} \frac{k(k+1)p}{2} &amp;&amp; \text{N=k+1}\\
&amp;= \frac{kp}{2} \\
\tag{8}
\end{align*}
</div>
<p>By efficiency and symmetry, the rest of the k workers get the rest of the
<span class="math">\(\frac{kp}{2}\)</span> profits (total profits is <span class="math">\(kp\)</span>), and thus each
worker should get <span class="math">\(\frac{p}{2}\)</span> of the profits.  In other words, each
worker is "contributing" half of their share of the profits they make to
the business owner.</p>
<p>Is it fair though?  Shapely values seems to say so.  Whether this has any
implications for our capitalistic society where investors/business owners
get much more than half of the profits is left as an exercise for the
reader :p</p>
</div>
<p><br></p>
<h4> Feature Explainability as Shapely Values </h4>
<p></p>
<h5> Additive Feature Attribution Models </h5>
<p>The first thing we need to cover is what does it mean for a model
to be explainable?  Take a simple linear regression for example:</p>
<div class="math">
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n  \tag{9}
\end{equation*}
</div>
<p>This model probably follows our intuition of an explainable model:
each of the <span class="math">\(x_i\)</span> variables are <em>independent, additive</em>, and
we can clearly point to a coefficient (<span class="math">\(\beta_i\)</span>) saying how it
contributed to the overall result (<span class="math">\(y\)</span>).  But what about more complex
models?  Even if we stick with linear models, as soon as we introduce any
interactions, it gets messy:</p>
<div class="math">
\begin{equation*}
y = \beta_0 + \sum_{i=1}^n \beta_i x_i
    + \sum_{i=1}^{n-1} \sum_{j&gt;i}^{n} \beta_{i,j}x_i x_j
\tag{10}
\end{equation*}
</div>
<p>In this case, how much has <span class="math">\(x_i\)</span> contributed to the overall prediction?
I have no idea and the reason is that my intuition expects the variables to be
independent and additive, and if they're not, I'm not really sure how to "explain"
the model.</p>
<p>So far we've been trying to explain the model as a whole.  What if we did
something simpler?  What if instead of trying to explain the entire model,
we took on a simpler problem: explain a single data point.  Of course, we would
want some additional properties, namely, that we can interpret it like the
simple linear regression in Equation 9, that there is some guarantee of local
accuracy, and probably some guarantee that similar models would produce similar
explanations.  We'll get to all of that but first let's setup the problem and
go through some definitions.</p>
<p>What we are describing here are called <strong>local methods</strong> designed to explain a
single input <span class="math">\(\bf x\)</span> on a prediction model <span class="math">\(f({\bf x})\)</span>.
When looking to explain a single data point, we don't really care about the level (i.e.
value) of a feature, just how much its presence contributes to the overall prediction.
To that end, let's define a binary vector of <strong>simplified inputs</strong> <span class="math">\(\bf x'\)</span>
(denoted by <span class="math">\('\)</span>) that represents whether or not we want to include that
feature's contribution to the overall prediction (analogous to our cooperative
game above).  We also have a mapping function <span class="math">\(h_x({\bf x'}) = {\bf x}\)</span>
that translates this binary vector to the equivalent values
for the data point <span class="math">\(\bf x\)</span>.  Notice that this mapping function
<span class="math">\(h_x(\cdot)\)</span> is <em>specific</em> to data point <span class="math">\(x\)</span> -- you'll have one of
these functions for every data point.
It's a bit confusing to write out in words so let's take a look at an example
which should clarify the idea.</p>
<div class="admonition admonition-example-3-simplified-inputs">
<p class="admonition-title">Example 3: Simplified Inputs</p>
<p>Consider a simple linear interaction model with two real inputs
<span class="math">\(x_1, x_2\)</span>:</p>
<div class="math">
\begin{equation*}
y = 1 + 2 x_1 + 3 x_2 + 4 x_1 x_2  \tag{11}
\end{equation*}
</div>
<p>Let's look at two data points:</p>
<div class="math">
\begin{align*}
{\bf u} &amp;= (x_1, x_2) = (-1, -0.5), &amp;&amp;y = -0.5 \\
{\bf v} &amp;= (x_1, x_2) = (0.5, 1), &amp;&amp;y = 7 \\
\tag{12}
\end{align*}
</div>
<p>Let's suppose we wanted to look at the effect of <span class="math">\(x_1\)</span> for these two
data points.  We would look at the vector <span class="math">\({\bf z'} = [1, 0]\)</span>
(where <span class="math">\(z \in \{u, v\}\)</span>) and use their <span class="math">\(h\)</span> mapping to find the
original values:</p>
<div class="math">
\begin{align*}
h_{\bf u}({\bf z'}) = h_{\bf u}([1, 0]) = [-1, n/a] \\
h_{\bf v}({\bf z'}) = h_{\bf v}([1, 0]) = [0.5, n/a] \\
\tag{13}
\end{align*}
</div>
<p>where we represent missing values with "n/a" (we'll get to this later).  As
you can see, this formalism allows us to speak about whether we
should include a feature or not and their equivalent
values.</p>
</div>
<p>Now that we have these definitions, our end goal is to essentially build
a new <em>explanation model</em>, <span class="math">\(g({\bf z'})\)</span> that ensures that
<span class="math">\(g({\bf z'}) \approx f(h_{\bf x}({\bf z'}))\)</span> whenever
<span class="math">\({\bf z'} \approx {\bf x'}\)</span>.  In particular, we want this explanation
model to be simple like our linear regression in Equation 9.  Thus, let's define
this type of model:</p>
<blockquote>
<p><strong>Additive Feature Attribution Methods</strong> have an explanation model that
is a linear function of binary variables:</p>
<div class="math">
\begin{equation*}
g({\bf z'}) = \phi_0 + \sum_{i=1}^M \phi_i z_i' \tag{14}
\end{equation*}
</div>
<p>where <span class="math">\(z' \in \{0,1\}^M\)</span>, <span class="math">\(M\)</span> is the number of simplified input
features and <span class="math">\(\phi_i \in \mathbb{R}\)</span>.</p>
</blockquote>
<p>This essentially captures our intuition on how to explain (in this case) a single data
point: additive and independent.  Next, we'll look at some desirable properties
that we want to maintain in this mapping.</p>
<p></p>
<h5> Desirable Properties and Shapely Values </h5>
<p>The first property we would want from our point-wise explanation model <span class="math">\(g(\cdot)\)</span>
is some guarantee of local accuracy:</p>
<blockquote>
<p><strong>Property 1 (Local Accuracy)</strong></p>
<div class="math">
\begin{equation*}
f(x) = g({\bf x'}) = \phi_0 + \sum_{i=1}^M \phi_i x_i' \tag{15}
\end{equation*}
</div>
<p>The explanation model <span class="math">\(g\)</span> matches the original model when <span class="math">\(x =
h_x(x')\)</span>, where <span class="math">\(\phi_0 = h_x({\bf 0})\)</span> represents the model output
with all simplified inputs toggled off.</p>
</blockquote>
<p>All this property is saying is that if you pass in the original data point
with all features included (<span class="math">\(x\)</span>), your explanation model (<span class="math">\(g\)</span>) should
return the original value of your model (<span class="math">\(f\)</span>), seems reasonable.</p>
<blockquote>
<p><strong>Property 2 (Missingness)</strong></p>
<div class="math">
\begin{equation*}
x_i' = 0 \implies \phi_i = 0 \tag{16}
\end{equation*}
</div>
<p>Missing input features in the original data point (<span class="math">\(x_i\)</span>) have no
attributed impact.</p>
</blockquote>
<p>This property almost seems unnecessary because all it is saying is that if your
original data point doesn't include the variable ("missing") then it shouldn't
show any attributed impact in your explanation model.  The idea of a "missing"
input is still something we have to deal with because most models don't really
support missing feature columns.  We'll get to it in the next section.</p>
<blockquote>
<p><strong>Property 3 (Consistency)</strong>: Let <span class="math">\(f_x(z') = f(h_x(z'))\)</span> and <span class="math">\(z' \backslash i\)</span> denote
setting <span class="math">\(z_i'=0\)</span>.  For any two models <span class="math">\(f\)</span> and <span class="math">\(f'\)</span>, if</p>
<div class="math">
\begin{equation*}
f_x'(z')-f_x'(z' \backslash i) \geq f_x(z')-f_x(z' \backslash i) \tag{17}
\end{equation*}
</div>
<p>for all inputs <span class="math">\(z'\in {0,1}^M\)</span> then <span class="math">\(\phi_i(f', x) \geq \phi_i(f,x)\)</span>.</p>
</blockquote>
<p>This is a more important property that essentially says: if we have two
(point-wise) models (<span class="math">\(f, f'\)</span>) and feature <span class="math">\(i\)</span> consistently
contributes more to the output in <span class="math">\(f'\)</span> compared to <span class="math">\(f\)</span>, we would
want the coefficient of our explanation model for <span class="math">\(f'\)</span> to be bigger than
<span class="math">\(f\)</span> (i.e. <span class="math">\(\phi_i(f', x) \geq \phi_i(f,x)\)</span>).  It's a sensible
requirement that allows us to fairly compare different models using the same
explainability techniques.</p>
<p>These three properties lead us to this theorem:</p>
<blockquote>
<p><strong>Theorem 1</strong> The only possible explanation model <span class="math">\(g\)</span> following an
additive feature attribution method and satisfying Properties 1, 2, and 3
are the Shapely values from Equation 2:</p>
<div class="math">
\begin{equation*}
\phi_i(f,x) = \sum_{z'\subseteq x'}
    \frac{|z'|!(M-|z'|-1)!}{M!}[f_x(z')-f_x(z' \backslash i)] \tag{18}
\end{equation*}
</div>
</blockquote>
<p>This is a bit of a surprising result since it's unique.  Interestingly, some
previous methods (e.g. Lime) don't actually satisfy all of these conditions
such as local accuracy and/or consistency.  That's why (at least theoretically)
Shapely values are such a nice solution to this feature attribution problem.</p>
<p></p>
<h5> SHapely Additive exPlanations (SHAP) </h5>
<p>If it wasn't clear already, we're going to use Shapely values as our feature
attribution method, which is known as SHapely Additive exPlanations (SHAP).
From Theorem 1, we know that Shapely values provide the only unique solution to
Properties 1-3 for an additive feature attribution model.  The big question is
how do we calculate the Shapely values (and what do they intuitively mean)?</p>
<p>Recall that Shapely values rely on the value function, <span class="math">\(v(S)\)</span>, which determine
a mapping from a subset of features to an expected "payoff".  In the case of
Equation 18, our "payoff" is the model prediction <span class="math">\(f_x(z')\)</span> i.e.
the prediction of our model at point <span class="math">\(x\)</span> with subset of features
<span class="math">\(z'\)</span>.  Implicit in this definition is that we can evaluate our model with
just a subset of features (i.e. the value function), which most models do <em>not</em>
support.  So how does SHAP deal with it? Expectations!</p>
<p>To evaluate a model at point <span class="math">\(x\)</span> with a subset of features <span class="math">\(S\)</span>, it
starts out with the <strong>expectation</strong> of the function (recall, <span class="math">\(z'\)</span> is our binary
vector representing <span class="math">\(S\)</span> and <span class="math">\(h_x(\cdot)\)</span> is the mapping from the
binary vector to the actual features in data point <span class="math">\(x\)</span>):</p>
<div class="math">
\begin{align*}
f(h_x(z')) &amp;= E[f(z)|z_S] \\
           &amp;= E_{z_{\bar{S}}|z_S}[f(z)] \\
\tag{19}
\end{align*}
</div>
<p>There are two main issues with this.  First, most models don't have an explicit
probability density, nor do they know how to deal with missing values, so we
have to approximate it using our dataset.  Depending on the model, this may be
easy or hard to compute.  For example, if our data point is describing a
customer and we had one missing dimension, it might look like this:</p>
<div class="math">
\begin{align*}
{\bf x}&amp;=\{x_{sex}=M, x_{age=18}, x_{spending}=100, x_{recency}=2, x_{location}=City, \ldots\} \\
{\bf x_{\overline{sex}}}&amp;=\{x_{sex}=N/A, x_{age=18}, x_{spending}=100, x_{recency}=2, x_{location}=City, \ldots\} \\
\tag{20}
\end{align*}
</div>
<p>To compute the value of the function missing the feature <span class="math">\(x_{sex}\)</span>, we
could simply just average over all the data points that included it, holding
all the other variables constant.  This would approximate the expectation
<span class="math">\(E[f(z)|z_S]\)</span>. Of course, our estimate of this value could be wildly off
if you don't have enough data so we won't always do this explicitly.</p>
<p>The second issue is that if you look back at Equation 18 you realize that we
would have to compute this expectation for <em>every</em> subset of features, which is
exponential in the number of features!  This is definitely a big barrier to
any practical application of this technique.  Often times we will just sample
from the power set of features to approximate it (see Kernel SHAP), in other
cases we can calculate it precisely because of the type of model.  More on this
later.</p>
<p>We can also make two simplifications to deal with the missingness issue:
<strong>independence</strong> and <strong>linearity</strong>.</p>
<div class="math">
\begin{align*}
f(h_x(z')) &amp;= E[f(z)|z_S] \\
           &amp;= E_{z_{\bar{S}}|z_S}[f(z)] \tag{21} \\
           &amp;\approx E_{z_{\bar{S}}}[f(z)] &amp;&amp; \text{independence} \tag{22} \\
           &amp;\approx f(z_S, E[z_{\bar{S}}]) &amp;&amp; \text{linear} \tag{23}
\end{align*}
</div>
<p>Independence allows us to treat each dimension separately and not care about the
conditional aspect of trying to find a data point that "matches" <span class="math">\(x\)</span> (such as
in Equation 20).  We can simply sample values from the missing dimension(s)
independently to fill the "N/A"s and average over them to estimate the
expectation.  Of course, this will only really work out if we actually have
feature independence, otherwise we're likely to sample values from the missing dimensions that
create unlikely data points that don't really represent the underlying
distribution.</p>
<p>Linearity additionally allows us to simply compute the expectation
(<span class="math">\(E[z_{\bar{S}}]\)</span>) over each dimension of <span class="math">\(x\)</span> separately and plug
it into the model, removing the need to do sampling to estimate the expectation.</p>
<p>To summarize, we can calculate feature importance by:</p>
<ol class="arabic">
<li>
<p>Computing Shapely values as in Equation 18:</p>
<div class="math">
\begin{equation*}
\phi_i(f,x) = \sum_{z'\subseteq x'}
    \frac{|z'|!(M-|z'|-1)!}{M!}[f_x(z')-f_x(z' \backslash i)]
\end{equation*}
</div>
<p>which is often estimated by sample subsets from the power set of features
(Kernel SHAP).</p>
</li>
<li><p>To evaluate the model with "missing" values (<span class="math">\(f_x(z')\)</span>), we will use
our dataset to approximate the expectation <span class="math">\(f_x(z') = E[f(z)|z_S]\)</span>.
We will often assume independence and  linearity, which allows us to greatly
simplify the computation needed.</p></li>
</ol>
<p>Now this leaves us with two additional questions: how do we interpret these
feature importances?  And how can we compute these values efficiently for
different types of models.  We'll get to the first question in the next
subsection and the latter in the next section.</p>
<p></p>
<h5> Interpreting SHAP Feature Importances </h5>
<p>SHAP features get us close but not quite the simplicity of a linear model in
Equation 9.  The big difference is that we are analyzing things <em>on a per data point</em>
basis as opposed to Equation 9 where we are doing it globally over the entire dataset.
Also recall that SHAP is based on Shapely values, which are averages over situations
with and without the variable, leading us to <em>contrastive</em> comparisons with the
base case (no features/players).  Figure 1 from the SHAP paper shows a
visualization of this concept.</p>
<div class="figure align-center">
<img alt="SHAP Values" src="../../images/shap_values.png" style="width: 800px;"><p class="caption">Figure 1: SHAP Values [1]</p>
</div>
<p>Here are some notes on interpreting this diagram:</p>
<ul class="simple">
<li><p>Notice that <span class="math">\(\phi_0\)</span> is simply the expected value of the overall
function.  This is complete "missing"-ness.  If you are missing all features,
then the value you should use as a "starting point" or base case is just the
mean of the prediction over the entire dataset.  Isn't this more logical
(with respect to the dataset) than starting at 0?  This is <em>sort of</em> analgous
to the linear regression case except the intercept in linear regression is
the mean when all inputs are zero.  We have to have a different definition
here because missing a features is not the same thing as zeroing it out.</p></li>
<li><p>Looking at the calculated line segments in the diagram, you can see that each
value is calculated as the difference between an expectation relative to some
conditional value and the same expectation less one variable.  This is a
realization of Equation 18 where
<span class="math">\(f_x(z)-f_x(z \backslash i) = E[f(z) | z] - E[f(z) | z \backslash i]\)</span>.</p></li>
<li><p>The figure assumes independence and linearity because it associates
<span class="math">\(\phi_i\)</span> with one particular ordering of variables
(e.g. <span class="math">\(\phi_2 = E[f(z)|z_{1,2}]=x_{1,2} - E[F(z)|z_1=x_1]\)</span>).  If
we didn't have these assumptions, we would have to calculate <span class="math">\(\phi_i\)</span>
as in Equation 18 where <span class="math">\(\phi_i\)</span> would be averaged over all possible
subsets that include/exclude that variable.</p></li>
<li><p>The arrows corresponding to <span class="math">\(\phi_i\)</span> are sequenced additively to sum up
to the final prediction.  That is, SHAP generates an additive model where
each feature importance can be additively summed to generate the final
prediction (Property 1).  This is true regardless of whether you have
linearity and independence as shown in the diagram, or you have to sum
over all possible subsets as in Equation 18 (in the latter case the diagram
would look different).</p></li>
</ul>
<p>The SHAP values can be confusing because if you don't have the independence and
linearity assumptions, it's not very intuitive the calculate (it's not easy
visualizing averages over all possible subsets).  The important point here is
that they are <em>contrastive</em>, which means we can compare their relative value to
each other fairly <em>but</em> they don't have the same absolute interpretations as
linear regression.  This is most clearly seen by the fact that every
<span class="math">\(\phi_{i\neq0}\)</span> is relative to <span class="math">\(\phi_0\)</span>, the mean prediction of
your dataset.  Having a feature importance of <span class="math">\(\phi_i=10\)</span> does not mean
including the feature contributes <span class="math">\(10\)</span> to your prediction, it means
<em>relative</em> to the average prediction, it adds <span class="math">\(10\)</span>.  I suspect that there
is some manipulation you can do to approximately get that "absolute" type of
feature importance that we see in linear regression but I haven't fully figured
it out yet.</p>
<p>Let's take a look at the same visualization as above but from the visualization
that the SHAP Python package [3] provides (Figure 2):</p>
<div class="figure align-center">
<img alt="SHAP Values" src="../../images/shap_values2.png" style="width: 800px;"><p class="caption">Figure 2: SHAP Values from the SHAP Python package [3]</p>
</div>
<p>We can see the same idea as Figure 1, however we don't assume any particular
ordering, instead we just stack all the positive feature importances to the left,
and all negative to the right to arrive at our final model prediction of
<span class="math">\(24.41\)</span>.  Rotating Figure 2 by 90 degrees and stacking all possible
values side-by-side, we get Figure 3.</p>
<div class="figure align-center">
<img alt="SHAP Values" src="../../images/shap_values3.png" style="width: 800px;"><p class="caption">Figure 3: SHAP Values from the SHAP Python package for entire dataset [3]</p>
</div>
<p>Figure 3 shows a global view of all possible data points and their SHAP
contributions relative to the overall mean (<span class="math">\(22.34\)</span>).  The plot is
actually interactive (when created in a notebook) so you can scroll over each
data point and inspect the SHAP values.</p>
<div class="figure align-center">
<img alt="SHAP Values" src="../../images/shap_values4.png" style="width: 800px;"><p class="caption">Figure 4: Summary of SHAP values over all features [3]</p>
</div>
<p>Figure 4 shows another global summary of the distribution of SHAP values over
all features.  For each feature (horizontal rows), you can see the distribution
of feature importances.  From the diagram we can see that <code>LSTAT</code> and
<code>RM</code> have large effects on the prediction over the entire dataset (high
SHAP value shown on bottom axis).  High <code>LSTAT</code> values affect the
prediction negatively (red values on the left hand side), while high <code>RM</code>
values affect the prediction positively (red values on the right hand side),
similarly in the opposite direction for both variables.</p>
<p></p>
<h5> SHAP Summary </h5>
<p>As we can see the SHAP values are very useful and have some clear advantages but
also some limitations:</p>
<ul class="simple">
<li><p><em>Fairly distributed, Contrastive Explanations:</em> Each feature is treated the
same without any need for heuristics or special insight by the user.  However,
as mentioned above the explanations are contrastive (relative to the mean),
so not exactly the same as our simple linear regression model.</p></li>
<li><p><em>Solid Theoretical Foundation</em>: As we can see from above, almost all of the
preamble was defining the theoretical foundation, culminating in Theorem 1.
This is nice since it also frames certain other techniques (e.g. LIME) in
a new light.</p></li>
<li><p><em>Global model interpretations</em>: Unlike other methods (e.g. LIME), SHAP can
provide you with global interpretations (as seen in the plots above) from the
individual Shapely values for each data point.  Moreover, due to the
theoretical foundations and the fact that Shapely values are fairly
distributed, we know that the global interpretation is consistent.</p></li>
<li><p><em>Fast Implementations</em>: Practically, SHAP would only be useful if it were
fast enough to use.  Thankfully, there is a fast implementations if you are
using a tree-based model, which we'll discuss in the next section.  However,
the model agnostic versions utilize the independence assumption and can
be slow if you want to use it globally on the entire dataset (but work well
enough on a single data point).</p></li>
</ul>
<p>If you want a more in-depth treatment, [4] is an amazing reference summarizing
SHAP and many other techniques.</p>
<p><br></p>
<h4> Computing SHAP </h4>
<p>Now that we've covered all the theoretical aspects, let's talk about how it
works practically.  Practically, you don't need to do much: there is a great
Python package ([3]) by the authors of the SHAP paper that takes care of
everything.  But there are definitely nuances that you should probably know
when using the different APIs.  Let's take a look at the common methods and see
how they differ.</p>
<p></p>
<h5> Linear Models </h5>
<p>For linear models, we can directly compute the SHAP values which are related to the
model coefficients.</p>
<blockquote>
<p><strong>Corollary 1 (Linear SHAP)</strong>: Given a model <span class="math">\(f(x) = \sum_{j=1}^M w_jx_j + b\)</span> then
<span class="math">\(\phi_0(f,x)=b\)</span> and <span class="math">\(\phi_i(f,x) = w_j(x_j-E[x_j])\)</span>.</p>
</blockquote>
<p>As you can see there is a direct mapping from linear coefficients to SHAP values.  The
reason why it's not a direct mapping is that SHAP is <em>contrastive</em>, that is its feature
importance is compared to the mean.  That's why we have that extra term in
the SHAP value <span class="math">\(\phi_i\)</span>.</p>
<p></p>
<h5> Kernel SHAP (Model Agnostic) </h5>
<p>Many times we don't have the luxury of just using a linear model and have to use
something more complex.  In this case, we can compute things using a model agnostic
technique called Kernel SHAP, which is a combination of LIME ([5]) and Shapely values.</p>
<p>The basic idea here is that <em>for each data point</em> under analysis, we will:</p>
<ol class="arabic simple">
<li><p>Sample different <em>coalitions</em> of including the feature/not including the feature
i.e. <span class="math">\(z_k' \in \{0,1\}^M\)</span>, where <span class="math">\(M\)</span> is the number of features.</p></li>
<li><p>For each sample, get the prediction of <span class="math">\(z_k'\)</span> by applying our mapping
function on our model (i.e. <span class="math">\(f(h_x(z_k'))\)</span>), using the assumption
that the missing values are replaced with <em>randomly</em> sampled values for that
dimension (the independence assumption).  It's possible to additionally
assume linearity too, where we would replace the value with the mean of that
dimension or equivalent.  For example, in an image you might replace a pixel
with the mean of the surrounding pixels (see [4] for more details).</p></li>
<li><p>Compute a weight for each data point <span class="math">\(z_k'\)</span> using the SHAP kernel:
<span class="math">\(\pi_{x'}(z')=\frac{(M-1)}{(M choose |z'|)|z'|(M-|z'|)}\)</span>.</p></li>
<li><p>Fit a weighted linear model (see [1] for details)</p></li>
<li><p>Return the coefficients of the linear model as the Shapely values (<span class="math">\(\phi_k\)</span>).</p></li>
</ol>
<p>The intuition here is that we can learn more about a feature if we study it in
isolation.  That's why the SHAP kernel will weight having a single feature,
or correspondingly M-1 features, more heavily (the combination in the
denominator is largest when <span class="math">\(M choose |z'|\)</span> is small).  When
<span class="math">\(|z'|=1\)</span> or <span class="math">\(|z'|=M\)</span> the expression has a divide by zero, but you
can just drop these terms in general (that's how I interpret what the paper
says anyways).</p>
<p>The good part is that this technique works with <em>any</em> model.  However, it's relatively slow
since we have to sample a bunch of values for each data point in our training
set and fit a linear regression.  We also have to use the independence
assumption, which can be violated when our actual model does not have feature
independence.  This might lead to violations in the local accuracy or
consistency guarantees.</p>
<p></p>
<h5> TreeSHAP </h5>
<p>TreeSHAP [2] is a decision tree-specific algorithm to compute SHAP.  Due to the
nature of decision trees, it doesn't need to use the independence (or linear)
assumptions.  Furthermore, due to some clever optimization, it can actually be
computed in <span class="math">\(O(TLD^2)\)</span> time where <span class="math">\(T\)</span> is the number of trees,
<span class="math">\(L\)</span> is the number of leaves and <span class="math">\(D\)</span> is the depth.  So as long as
you don't have gigantic trees, it scales very well.</p>
<p>The crux of the algorithm is computing precisely <span class="math">\(E[f(x)|x_s]\)</span> (Equation 21),
which can be done recursively and shown below in Figure 5.  Vectors <span class="math">\(a\)</span> and <span class="math">\(b\)</span>
represent the left and right node indexes for each internal node, <span class="math">\(t\)</span> the
thresholds for each node, and <span class="math">\(d\)</span> is the vector of features used for
splitting.  <span class="math">\(r\)</span> represents the cover for each vector i.e. how many data
samples are in that sub-tree.</p>
<div class="figure align-center">
<img alt="Naive TreeSHAP Algorithm" src="../../images/shap_treeshap.png" style="width: 800px;"><p class="caption">Figure 5: Naive TreeSHAP Algorithm [2]</p>
</div>
<p>This naive algorithm is pretty straight-forward to understand.  It recursively
traverses the tree and either follows a branch if you are conditioning on a
variable (<span class="math">\(x_s\)</span>), otherwise computes a weighted average of the two
branches based on the number of data samples (the expectation).  It works
because decision trees are explicitly sequential in how they compute their
values, so "skipping" over a missing variable is easy: just jump over that
level of the tree by doing a weighted average over it.</p>
<p>One thing you'll notice is that computing SHAP values using Figure 5's
algorithm is very expensive, on the order of <span class="math">\(O(TLM2^M)\)</span>.
Exponential in the number of features!  The exponential part comes from the
fact that we still need to compute all subsets of <span class="math">\(M\)</span> features, which
means running the algorithm <span class="math">\(2^M\)</span> times.  It turns out we can do better.</p>
<p>A more complex algorithm which is shown in [2] tries to keep track of all
possible subsets as it traverses down the tree.  The algorithm is quite complex
and, honestly, I don't quite understand (or care to understand) all the details
at this point so I'll leave it as an exercise to the reader.  The more
interesting thing about this algorithm is that it runs in <span class="math">\(O(TLD^2)\)</span> time
and <span class="math">\(O(D^2+M)\)</span> memory.  Since we're only doing one traversal of the tree,
we get rid of the exponential number of calls but require more computation to
account for all the different subsets.  The extension to simple ensembles
of trees is pretty straight forward.  Since common algorithms like random
forests or gradient boosted trees are additive, we can simply compute the SHAP
value for each tree independently and add them up.</p>
<p></p>
<h5> Other SHAP Methods </h5>
<p>The papers by the original authors in [1, 2] show a few other variations to
deal with other model like neural networks (Deep SHAP), SHAP over the max
function, and quantifying local interaction effects.  Definitely worth a look
if you have some of those specific cases.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>With all the focus on deep learning in the recent years, it's refreshing to see
really impactful research in other fields, especially the burgeoning field of
explainable models.  It's especially important in this day and age of blackbox
models.  I also like the fact that there was some proper algorithmic work with
the TreeSHAP paper.  The work of speeding up a naive algorithm from exponential
to low-order polynomial reminds me of my grad school days (not that I ever had
a result like that, just the focus on algorithmic work).  Machine learning is
definitely a very wide field and the reason why it's so interesting is that you
have to constantly pull from so many different disciplines to understand it (to
a satisfactory degree).  See you next time!</p>
<p><br></p>
<h4> References </h4>
<ul class="simple">
<li><p>[1] "A Unified Approach to Interpreting Model Predictions", Scott M. Lundberg, Su-In Lee, <a class="reference external" href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions</a></p></li>
<li><p>[2] "Explainable AI for Trees: From Local Explanations to Global Understanding", Lundberg, et. al, <a class="reference external" href="https://arxiv.org/abs/1905.04610">https://arxiv.org/abs/1905.04610</a></p></li>
<li><p>[3] SHAP Python Package, <a class="reference external" href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></p></li>
<li><p>[4] "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable", Christoph Molnar, <a class="reference external" href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></p></li>
<li><p>[5] “Why should i trust you?: Explaining the predictions of any classifier”, Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.</p></li>
<li><p>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Shapley_value">https://en.wikipedia.org/wiki/Shapley_value</a></p></li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>This treatment of Shapely values is primarily a re-hash of the corresponding Wikipedia article.  It's actually written quite well, so you should go check it out!</p>
</dd>
</dl>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/explainability/" rel="tag">explainability</a></li>
            <li><a class="tag p-category" href="../../categories/game-theory/" rel="tag">game theory</a></li>
            <li><a class="tag p-category" href="../../categories/shap/" rel="tag">SHAP</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../a-note-on-using-log-likelihood-for-generative-models/" rel="prev" title="A Note on Using Log-Likelihood for Generative Models">Previous post</a>
            </li>
            <li class="next">
                <a href="../lossless-compression-with-asymmetric-numeral-systems/" rel="next" title="Lossless Compression with Asymmetric Numeral Systems">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2020         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
