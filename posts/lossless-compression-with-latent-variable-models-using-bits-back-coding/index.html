<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A post on using bits-back coding with latent variable models to do lossless compression.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Lossless Compression with Latent Variable Models using Bits-Back Coding | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../lossless-compression-with-asymmetric-numeral-systems/" title="Lossless Compression with Asymmetric Numeral Systems" type="text/html">
<link rel="next" href="../hamiltonian-monte-carlo/" title="Hamiltonian Monte Carlo" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Lossless Compression with Latent Variable Models using Bits-Back Codin">
<meta property="og:url" content="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/">
<meta property="og:description" content="A post on using bits-back coding with latent variable models to do lossless compression.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2021-07-06T11:00:00-05:00">
<meta property="article:tag" content="asymmetric numeral systems">
<meta property="article:tag" content="Bits-Back">
<meta property="article:tag" content="compression">
<meta property="article:tag" content="lossless">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="MNIST">
<meta property="article:tag" content="variational autoencoder">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Lossless Compression with Latent Variable Models using Bits-Back Coding</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2021-07-06T11:00:00-05:00" itemprop="datePublished" title="2021-07-06 11:00">2021-07-06 11:00</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>A lot of modern machine learning is related to this idea of "compression", or
maybe to use a fancier term "representations".  Taking a huge dimensional space
(e.g. images of 256 x 256 x 3 pixels = 196608 dimensions) and somehow compressing it into
a 1000 or so dimensional representation seems like pretty good compression to
me!  Unfortunately, it's not a lossless compression (or representation).
Somehow though, it seems intuitive that there must be a way to use what is learned in
these powerful lossy representations to help us better perform <em>lossless</em>
compression, right?  Of course there is! (It would be too anti-climatic of a
setup otherwise.)</p>
<p>This post is going to introduce a method to perform lossless compression that
leverages the learned "compression" of a machine learning latent variable
model using the Bits-Back coding algorithm.  Depending on how you first think
about it, this <em>seems</em> like it should either be (a) really easy or (b) not possible at
all.  The reality is kind of in between with an elegant theoretical algorithm
that is brought down by the realities of discretization and imperfect learning
by the model.  In today's post, I'll skim over some preliminaries (mostly
referring you to previous posts), go over the main Bits-Back coding algorithm
in detail, and discuss some of the implementation details and experiments that
I did while trying to write a toy version of the algorithm.</p>
<!-- TEASER_END -->
<div class="card card-body bg-light">
<h2>Table of Contents</h2>
<div class="contents local topic" id="contents">
<ul class="auto-toc simple">
<li><p><a class="reference internal" href="#background" id="id3"><span class="sectnum">1</span> Background</a></p></li>
<li>
<p><a class="reference internal" href="#lossless-compression-with-probabilistic-models" id="id4"><span class="sectnum">2</span> Lossless Compression with Probabilistic Models</a></p>
<ul class="auto-toc">
<li><p><a class="reference internal" href="#generative-autoregressive-models" id="id5"><span class="sectnum">2.1</span> Generative Autoregressive Models</a></p></li>
<li><p><a class="reference internal" href="#latent-variable-models" id="id6"><span class="sectnum">2.2</span> Latent Variable Models</a></p></li>
</ul>
</li>
<li>
<p><a class="reference internal" href="#bits-back-coding" id="id7"><span class="sectnum">3</span> Bits-Back Coding</a></p>
<ul class="auto-toc">
<li><p><a class="reference internal" href="#theoretical-limit-of-bits-back-coding" id="id8"><span class="sectnum">3.1</span> Theoretical Limit of Bits-Back Coding</a></p></li>
<li><p><a class="reference internal" href="#issues-affecting-the-efficiency-of-bits-back-coding" id="id9"><span class="sectnum">3.2</span> Issues Affecting The Efficiency of Bits-Back Coding</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#implementation-details" id="id10"><span class="sectnum">4</span> Implementation Details</a></p></li>
<li><p><a class="reference internal" href="#experiments" id="id11"><span class="sectnum">5</span> Experiments</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id12"><span class="sectnum">6</span> Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id13"><span class="sectnum">7</span> References</a></p></li>
</ul>
</div>
</div>
<p></p>
<p><em>2023-10-22: Updated the code and experimental results based on a reader's</em>
<a class="reference external" href="https://github.com/bjlkeng/sandbox/issues/10">comment</a> <em>(thanks bfs18!)
where they found a subtle bug in my code.  Fixed the bug and also fixed some
corner cases scenarios in my bitsback implementation.  Now I get something
closer to the paper.</em></p>
<div class="section" id="background">
<h2><a class="toc-backref" href="#id3"><span class="sectnum">1</span> Background</a></h2>
<p>A <strong>latent (or hidden) variable model</strong> is a statistical model where
you <em>don't</em> observe some of the variables (or in many cases, you create additional
intermediate unobserved variables to make the problem more tractable).  This
creates a relationship graph (usually a DAG) between observed (either input or
output, if applicable) and latent variables.  Often times the modeller will
have an explicit intuition about the meaning of these latent variables; other
times (especially for deep learning models), the modeller doesn't explicitly
give these latent variables meaning, and instead they are learned to represent
whatever the optimization procedure deems necessary.  See the background
section of my <a class="reference external" href="../the-expectation-maximization-algorithm/">Expectation Maximization Algorithm</a> post for more details.</p>
<p>A <strong>variational autoencoder</strong> is a special type of latent variable model that
contains two parts:</p>
<ol class="arabic simple">
<li><p>A generative model (aka "decoder") that defines a mapping from some
latent variables (usually independent standard Gaussians) to your data
distribution (e.g. images).</p></li>
<li><p>An approximate posterior network (aka "encoder") that maps from your
data distribution (e.g. images) to your latent variable space.</p></li>
</ol>
<p>There's also a bunch of math, a reparameterization trick, and some deep nets
strung together to make it all work out relatively nicely.  See my post of
<a class="reference external" href="../variational-autoencoders/">Variational Autoencoders</a> for more
details.</p>
<p>A special type of lossless compression algorithm called an <em>entropy encoder</em> exploits
the statistical properties of your input data to compress data efficiently. A
relatively new algorithm to perform this lossless compression is called
<strong>Asymmetrical Numeral Systems</strong> (ANS), which essentially map any input data string
to a (really large) natural number in a smart way such that frequent (or
predictable) characters/strings get mapped to smaller numbers, while infrequent
ones get mapped to larger ones.  One key feature of this algorithm is that
the compressed data stream is generated in last-in-first-out order (i.e. in
stack order).  This means when decompressing, you want to read the compressed data
from back-to-front (this will be useful for our discussion below).
My post on <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">Lossless Compression with Asymmetric Numeral Systems</a> gives a
much better intuition on the whole process and discusses a few variations and
implementation details.</p>
</div>
<div class="section" id="lossless-compression-with-probabilistic-models">
<h2><a class="toc-backref" href="#id4"><span class="sectnum">2</span> Lossless Compression with Probabilistic Models</a></h2>
<p>Suppose we wanted to perform lossless compression on datum
<span class="math">\({\bf x}=[x_1,\ldots,x_n]\)</span> composed of a vector (or tensor) of symbols
(i.e. an image <span class="math">\(\bf x\)</span> made up of <span class="math">\(n\)</span> pixels), and we are given:</p>
<ol class="arabic simple">
<li><p>A model that can tell us the probability of each symbol,
<span class="math">\(P_x(x_i|\ldots)=p_i\)</span> (that may or may not have some conditioning on it).</p></li>
<li><p>A entropy encoder (and corresponding decoder) that can return a compressed
stream of bits given input symbols and their probability distributions, call
it: <span class="math">\(B=\text{CODE}({\bf x}, P_x)\)</span>.</p></li>
</ol>
<p>Note: Here we are distinguishing between data points (denoted by a bold <span class="math">\(\bf x\)</span>),
and the different components of that data point (not bolded <span class="math">\(x_i\)</span>).
Usually we will want to encode <em>multiple</em> data points (e.g. images), each of which contains
multiple components (e.g. pixels).</p>
<p>Compression is pretty straight forward by just calling <span class="math">\(\text{CODE}\)</span>
with the input datum and its respective probability distribution to get
your compressed stream of bits <span class="math">\(B\)</span>. To decode, simply call the
corresponding <span class="math">\({\bf x}=\text{DECODE}(B, P_x)\)</span> function to recover your desired
input data.  Notice that our compressed bit stream needs to be paired with
<span class="math">\(P_x\)</span> or else we won't be able to decode it.</p>
<p>"Model" here can be really simple, we could just use a histogram of each
symbol in our input data stream, and this could serve as our probability
distribution.  You would expect that this simple approach would not do very well
for complex data, for example, trying to compress various images.  A histogram
for pixel values (i.e. symbols) across all images would be very dispersed
because images are so varied in their pixel values.  This implies any given
pixel would have small probabilities, and thus have poor compression (recall
the higher the probability of a symbol the better entropy compression methods
work).</p>
<p>So the intuition here is that we want a model that can <em>accurately</em> predict
(via a probability distribution) our datum and its corresponding symbols to allow the
entropy encoder to efficiently compress.  For example, we want our model to be
able to <em>accurately</em> generate a tight distribution around each of an image's
pixel values.  This would allow our entropy encoder to achieve a high
compression rate.  But there's a small problem here: to generate a tight
distribution around each symbol within a datum, we would need a different
distribution for each datum, otherwise any distribution we generate will be too
dispersed (e.g. not accurate) because it would have to apply to any generic datum.
Following this logic, we need a probability distribution to encode and decode
our datum but that distribution is conditional on the actual datum.  This means
when we're trying to decode the datum, we need to have the datum (encoding is
not a problem because we have the datum available)!  This circular dependency
can be resolved by using specific types of models and encoding things in a
particular order, which is the topic of the next two subsections.</p>
<div class="section" id="generative-autoregressive-models">
<h3><a class="toc-backref" href="#id5"><span class="sectnum">2.1</span> Generative Autoregressive Models</a></h3>
<p>A generative autoregressive models simply uses the probability chain rule to model the data:</p>
<div class="math">
\begin{equation*}
P({\bf x}) = \prod_{i=1}^{D} P_x(x_i | {\bf x}_{&lt;i})  \tag{1}
\end{equation*}
</div>
<p>Each indexed component of <span class="math">\(\bf x\)</span> (i.e. pixel) is dependent only on the
previous ones, using whatever indexing makes sense.  See my post on <a class="reference external" href="../pixelcnn/">PixelCNN</a>
for more details.</p>
<p>When using this type of model with an ANS entropy encoder, we have to be a bit
careful because it decompresses symbols in the last-in-first-out (stack) order.
Let's take a closer look in Figure 1.</p>
<div class="figure align-center">
<img alt="Entropy Encoding and Decoding with a Generative Autoregressive Model" src="../../images/bbans_autoregressive.png" style="width: 700px;"><p class="caption">Figure 1: Entropy Encoding and Decoding with a Generative Autoregressive Model</p>
</div>
<p>From the left side of Figure 1, notice that we have to
reverse the order of the input data to the ANS encoder (the autoregressive
model receives the input in the usual parallel order though).  This is needed
because we need to decode the data in the ascending order for the
autoregressive model to work (see decoding below).  Next, notice that our ANS
encoder requires both the (reversed) input data and the appropriate
distributions for each symbol (i.e.  each <span class="math">\(x_j\)</span> component).  Finally, the
compressed data is output, which (hopefully) is shorter than the original
input.</p>
<p>Decoding is shown on the right hand side of Figure 1.  It's a bit more
complicated because we must <em>iteratively</em> generate the distributions for each
symbol.  Starting by reversing the compressed data, we decode <span class="math">\(x_1\)</span> since
our model can unconditionally generate its distribution.
This is the reason why we needed to reverse our input during encoding.  Then,
we generate <span class="math">\(x_2|x_1\)</span> and so on for each <span class="math">\(x_i|x_{1,\ldots,i-1}\)</span>
until we've recovered the original data.  Notice that this is quite inefficient
since we have to call the model <span class="math">\(n\)</span> times for each component of
<span class="math">\(\bf x\)</span>.</p>
<p>I haven't tried this but it seems like something pretty reasonable to do
(assuming you have a good model <em>and</em> I haven't made a serious logical error).
The only problem with generative autoregressive models is that they are slow
because you have to call them <span class="math">\(n\)</span> times.  Perhaps that's why no one is
interested in this?  In any case, the next method overcomes this problem.</p>
</div>
<div class="section" id="latent-variable-models">
<h3><a class="toc-backref" href="#id6"><span class="sectnum">2.2</span> Latent Variable Models</a></h3>
<p>Latent variable models have a set of unobserved variables <span class="math">\(\bf z\)</span> in
addition to the observed ones <span class="math">\(\bf x\)</span>, giving us a likelihood function
of <span class="math">\(P(\bf x|\bf z)\)</span>.  We'll usually have a prior distribution
<span class="math">\(P({\bf z})\)</span> for <span class="math">\(\bf z\)</span> (implicitly or explicitly), and depending
on the model, we may or may not have access to a posterior distribution (more
likely an estimate of it) as well: <span class="math">\(q(\bf z| \bf x)\)</span>.</p>
<p>The key idea with these models is that we need to encode the latent variables
as well (or else we won't be able to generate the required distributions for
<span class="math">\(\bf x\)</span>).  Let's take a look at Figure 2 to see how the encoding works.</p>
<div class="figure align-center">
<img alt="Entropy Encoding with a Latent Variable Model" src="../../images/bbans_latent_encode.png" style="width: 600px;"><p class="caption">Figure 2: Entropy Encoding with a Latent Variable Model</p>
</div>
<p>Starting from the input data, we need to first generate some value for our
latent variable <span class="math">\(\bf z\)</span> so that we can use it with our model <span class="math">\(P(\bf x|\bf z)\)</span>.
This can be obtained either by sampling the prior (or posterior if available),
or really any other method that would generate an accurate distribution for <span class="math">\(\bf x\)</span>.
Once we have <span class="math">\(\bf z\)</span>, we can run it through our model, get distributions
for each <span class="math">\(x_i\)</span> and encode the input data as usual.  The one big
difference is that we also have to encode our latent variables.  The latent
variables <em>should</em> be distributed according to our prior distribution (for most
sensible models), so we can use it with the ANS coder to compress <span class="math">\(\bf
z\)</span>.  Notice that we cannot use the posterior here because we won't have access
to <span class="math">\(\bf x\)</span> at decompression time, therefore, would not be able to
decompress <span class="math">\(\bf z\)</span>.</p>
<div class="figure align-center">
<img alt="Entropy Decoding with a Latent Variable Model" src="../../images/bbans_latent_decode.png" style="width: 600px;"><p class="caption">Figure 3: Entropy Decoding with a Latent Variable Model</p>
</div>
<p>Decoding is shown in Figure 3 and works basically as the reverse of encoding.
The major thing to notice is that we have to do operations in a
last-in-first-out order.  That is, first decode <span class="math">\(\bf z\)</span>, use it to
generate distributional outputs for the components of <span class="math">\(\bf x\)</span>, then
use those outputs to decode the compressed data to recover our original message.</p>
<p>This is all relatively straight forward if you took time to think about it.
There are some other issues as well around discretizing <span class="math">\(\bf z\)</span> if it's
continuous but we'll cover that below.  The more interesting question is can we
do better?  The answer is a resounding "Yes!", and that's what this post is all
about.  By using a very clever trick you can get some "bits back" to improve
your compression performance.  Read on to find out more!</p>
</div>
</div>
<div class="section" id="bits-back-coding">
<h2><a class="toc-backref" href="#id7"><span class="sectnum">3</span> Bits-Back Coding</a></h2>
<p>From the previous section, we know that we can encode and decode data using a
latent variable model with relative ease.  The big downside is that we're
"wasting" space by encoding the latent variables.  They're necessary to
generate the distributions for our data, but otherwise are not directly
encoding any signal.  It turns out we can use a clever trick to recover
some of this "waste".</p>
<p>Notice in Figure 2, we randomly sample from (an estimate of) the posterior distribution.
In some sense, we're introducing new information from the random sample here
that we must encode.  Instead, why don't we utilize some of the existing bits
we've encoded to get a "pseudo-random" sample <a class="footnote-reference brackets" href="#id2" id="id1">1</a> ?  Figure 4 shows the encoding
process in more detail.</p>
<div class="figure align-center">
<img alt="Bits-Back Encoding with a Latent Variable Model" src="../../images/bbans_bb_encode.png" style="width: 600px;"><p class="caption">Figure 4: Bits-Back Encoding with a Latent Variable Model</p>
</div>
<p>The key difference here is that we're decoding the existing bitstream (from
previous data that we've compressed) to generate a (pseudo-) random sample <span class="math">\(\bf z\)</span>
using the posterior distribution.  This replaces the random sampling we
did in Figure 2.  Since the existing bitstream was encoded using a different
distribution, the sample we decode should <em>sort of</em> random.  The nice part
about this trick is that we're still going to encode <span class="math">\(\bf z\)</span> as usual so
any bits we've popped off the bitstream to generate our pseudo-random sample,
we get "back" (that is, aren't require to be on the bitstream anymore).  This
<em>reduces</em> the effective average size of encoding each datum + latent variables.</p>
<div class="figure align-center">
<img alt="Bits-Back Decoding with a Latent Variable Model" src="../../images/bbans_bb_decode.png" style="width: 600px;"><p class="caption">Figure 5: Bits-Back Decoding with a Latent Variable Model</p>
</div>
<p>Figure 5 shows decoding with Bits-Back.  It is the same as latent variable
decoding with the exception that we have to "put back" the bits we took off
originally.  Since our ANS encoding and decoding are lossless, the bits we
put back should be exactly the bits we took off.  The number of bits we
removed/put back will be dependent on the posterior distribution and the bits that
were originally there.</p>
<div class="figure align-center">
<img alt="Visualization of Bitstream for Bits-Back Coding" src="../../images/bbans_bitstream_view.png" style="width: 600px;"><p class="caption">Figure 6: Visualization of Bitstream for Bits-Back Coding</p>
</div>
<p>To get a better sense of how it works, Figure 6 shows a visualization of
encoding and decoding two data points.  Colors represent the different
data: green for existing bitstream, blue for <span class="math">\(\bf x^1\)</span>, and orange for <span class="math">\(\bf x^2\)</span>
(superscript represents data point index).  The different shades represent either
observed data <span class="math">\(\bf x\)</span> or latent variable <span class="math">\(\bf z\)</span>.</p>
<p>From Figure 6, the first step in the process is to <em>reduce</em> the bitstream length
by (pseudo-)randomly sampling <span class="math">\(\bf z\)</span>.  This is followed by encoding
<span class="math">\(\bf x\)</span> and <span class="math">\(\bf z\)</span> as usual.  This process repeats for each
additional datum.  Even though we have to encode <span class="math">\(\bf z\)</span>, the effective
size of the encoding is shorter because of the initial "bits back" we get each
time.  Decoding is the reverse operation of encoding: decode <span class="math">\(\bf z\)</span> and
<span class="math">\(\bf x\)</span>, put back the removed bits by utilizing the posterior
distribution (which is conditional on the <span class="math">\(\bf x\)</span> we just decoded).
And this repeats until all data has been decoded.</p>
<div class="section" id="theoretical-limit-of-bits-back-coding">
<h3><a class="toc-backref" href="#id8"><span class="sectnum">3.1</span> Theoretical Limit of Bits-Back Coding</a></h3>
<p>Turning back to some more detailed mathematical analysis, let's see how good
Bits-Back is theoretically.  We'll start off with a few assumptions:</p>
<ol class="arabic simple">
<li><p>Our data <span class="math">\(\bf x\)</span> and latent variables <span class="math">\(\bf z\)</span> are sampled from
the true joint distribution <span class="math">\(P({\bf x, z})=P({\bf x|z})P({\bf z})\)</span>,
which we have access to.  Of course in the real world, we don't have the
true distribution, just an approximation.  But if our model is very good, it
will hopefully be very close to the true distribution.</p></li>
<li><p>We have access to an approximate posterior <span class="math">\(q({\bf z|x})\)</span>.</p></li>
<li><p>Assume we have an entropy coder so that we can optimally code any data point.</p></li>
<li><p>The pseudo-random sample we get from Bits-Back coding is drawn from the approximate posterior <span class="math">\(q({\bf z|x})\)</span>.</p></li>
</ol>
<p>As noted above, if we naively use the latent variable encoding from Figure 2,
given a sample <span class="math">\((x, z)\)</span>, our expected message length should be
<span class="math">\(-(\log P({\bf z}) + \log P({\bf x|z}))\)</span> bits long.  This uses the fact
(roughly speaking) that the theoretical limit of the average number of bits
needed to represent a symbol (in the context of its probability distribution)
is its <a class="reference external" href="https://en.wikipedia.org/wiki/Information_content">information</a>.</p>
<p>However using Bits-Back with an approximate posterior <span class="math">\(q({\bf z|x})\)</span>
for a given <em>fixed</em> data point <span class="math">\(\bf x\)</span>, we can calculate the expected
message length over all possible <span class="math">\(\bf z\)</span> drawn from <span class="math">\(q({\bf z|x})\)</span>.
The idea is that we're (pseudo-)randomly drawing <span class="math">\(\bf z\)</span> values, which
affect each part of the process (Bits-Back, encoding <span class="math">\(x\)</span>, and encoding
<span class="math">\(z\)</span>) so we must average (i.e. take the expectation) over it:</p>
<div class="math">
\begin{align*}
L(q) &amp;= E_{q({\bf z|x})}[-\log P({\bf z}) - \log P({\bf x|z}) + \log q({\bf z|x})] \\
     &amp;= \sum_z q({\bf z|x})[-\log P({\bf z}) - \log P({\bf x|z}) + \log q({\bf z|x})]  \\
     &amp;= -\sum_z q({\bf z|x})\log \frac{P({\bf x, z})}{q({\bf z|x})}  \\
     &amp;= -E_{q({\bf z|x})}\big[\log \frac{P({\bf x, z})}{q({\bf z|x})}\big]  \\
     \tag{2}
\end{align*}
</div>
<p>Equation 2 is also known as the evidence lower bound (ELBO) (see my previous
<a class="reference external" href="../semi-supervised-learning-with-variational-autoencoders/">post on VAE</a>
for more details).  The nice thing about the ELBO is that many ML models (including
the variational autoencoder) use it as its objective function.  So by optimizing our
model, we're simultaneously optimizing our message length.</p>
<p>From Equation 2, we can also see that it is optimized when <span class="math">\(q({\bf z|x})\)</span> equals
to the true posterior <span class="math">\(P({\bf z|x})\)</span>:</p>
<div class="math">
\begin{align*}
-E_{q({\bf z|x})}\big[\log \frac{P({\bf x, z})}{q({\bf z|x})}\big]
&amp;= -E_{q({\bf z|x})}\big[\log \frac{P({\bf z|x})P({\bf x})}{P({\bf z|x})}\big]  &amp;&amp; \text{since }
P({\bf x, z}) = P({\bf z|x})P({\bf x}) \text{ and } q({\bf z|x})=P({\bf z|x}) \\
&amp;= -E_{q({\bf z|x})}\big[\log P({\bf x^1})\big] \\
&amp;= -\log P({\bf x}) \\
\tag{3}
\end{align*}
</div>
<p>Which is the optimal code length for sending our data point <span class="math">\(x\)</span> across.
So <em>theoretically</em> if we're able to satisfy all the assumptions and have an
exact posterior then we'll have a really good compressor!  Of course, we'll never
be in this theoretic ideal situation, we'll discuss some of the issues that
reduce this efficiency in the next subsection.</p>
</div>
<div class="section" id="issues-affecting-the-efficiency-of-bits-back-coding">
<h3><a class="toc-backref" href="#id9"><span class="sectnum">3.2</span> Issues Affecting The Efficiency of Bits-Back Coding</a></h3>
<p><strong>Transmitting the Model</strong>: All the above discussion assumes that the sender
and receiver have access to the latent variable model but that needs to be sent
as well!  The assumption here is that the model would be so generally applicable
that the compression package would include it by default (or have a plugin) to
download the model.  For example, photos are so common that we conceivably have
a single latent variable model for photos (e.g. something along the lines of
an ImageNet model).  This would enable the compression encoder/decoder package to include
it and encode images or whatever data distribution it is trained on.  For the
experiments below, I don't include the model size but if I did, it would be
much worse.  I think the benefits are only realized if you can amortize the
cost of sending the model over a huge dataset (this is similar to the fact that
you also need to have the compression/decompression algorithm on each side).</p>
<p><strong>Discretization</strong>: Another thing that the above discussion glossed over is how
to encode/decode continuous variables.  ANS (and similar entropy coders) work
on discrete symbols -- not continuous values.  Many popular latent variable
models also have continuous latent variables (e.g. normally distributed), so there
needs to be a discretization step to be able to send them over the wire (we're
assuming the data is discretized already but the same principles would apply).</p>
<p>Discretization is needed for both (a) when we (pseudo-)randomly sample our
<span class="math">\(\bf z\)</span> value (get/put "bits back"), and (b) for when we compress/decompress
the discretized latent variables themselves.</p>
<p>Discretization of a sample from a distribution is <em>abstractly</em> a relatively
simple operation:</p>
<ol class="arabic simple">
<li><p>Select the number of discrete buckets you want to represent your
discretized distribution, call it <span class="math">\(m\)</span>.
This basically is binning the <a class="reference external" href="https://en.wikipedia.org/wiki/Support_(mathematics)">support</a>
of the distribution.  [1] proposed that each bucket has <em>equi-probable mass</em>
(specifically not equally-spaced), which is what I implemented.</p></li>
<li><p>For a given value of <span class="math">\(z\)</span>, find the corresponding bucket index
(<span class="math">\(i\)</span>) the point falls in, set the discretized value to some deterministic
value relative to the bucket interval (e.g.  mid-point of the bucket interval).
This discretized <span class="math">\(z\)</span> value is what you use whenever a latent variable is needed
(either for input to the VAE decoder or when compressing/decompressing <span class="math">\(z\)</span>).</p></li>
</ol>
<p>Using the above, when given a value continuous value for <span class="math">\(z\)</span>, you can always
map it to a fixed bin, which maps any value landing in that bin to a
determinstic value.</p>
<p>However there is a subtlety here, how are you going to select your bins edges?
You can't use the approximate posterior distribution because you don't have
access to it when decoding <span class="math">\(z\)</span> (Figure 5).  The natural choice here is
the prior from the VAE, which is what is available throughout the process
(assuming you have the model).  This is what the paper [1] also used.</p>
<p>The paper [1] goes on to show that this discretization process adds overhead
to the compression process (it would be weird if it didn't since you lose
information).  However, in many cases you don't need that much precision.  In
my experiments I got a respectable result with with just 5-bits of discretization for
MNIST (which is a heavily skewed toward black and white pixels).  The paper
argues in their case, they didn't need to go beyond 16-bits because most ML
operations are 32-bits anyways.</p>
<p><strong>Quantization</strong>: In addition to discretization, we also have to worry about quantization from
the <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">rANS</a>
algorithm.  Once we have the buckets above, we also need to compress/decompress
the <span class="math">\(z\)</span> values using rANS, which requires quantization of the probability mass.
This can be done as such:</p>
<ol class="arabic simple">
<li><p>Select the number of bits used to quantization in the rANS algorithm
<span class="math">\(n\)</span>, where each bucket has an associated integer <span class="math">\(f_i\)</span> representing the
quantized probability <span class="math">\(\frac{f_i}{2^n}\)</span> of that bucket (assuming equi-probable buckets),
and the sum of all buckets equals to <span class="math">\(2^n\)</span>.</p></li>
<li><p>Use rANS to encode the discretized <span class="math">\(x/z\)</span> value as the symbol <span class="math">\(s_i\)</span>
with quantized frequency <span class="math">\(f_i\)</span> assuming an alphabet of <span class="math">\(m\)</span>
symbol values (where symbol is synonymous with bucket in this context).</p></li>
</ol>
<p>I used a <span class="math">\(n=16\)</span> bit quantization in my experiments, which seemed to be
enough precision (although I didn't try other values).  Quantization
affects the fidelity of the probability distribution passed to rANS, which
affects the compression ratio.  Ideally, you'll want as small a value as you
can get away with.</p>
<p><strong>Clean Bits</strong>: The last issue to discuss is the how the Bits-Back operation
pseudo-randomly samples the <span class="math">\(\bf z\)</span> value.  Since we're sampling from the
bits from the top of the existing bitstream, which is essentially the previous
prior-encoded posterior sample, we would not expect it to be a true random
sample (which would require a uniform random stream of bits).  Only in the
base-case with the first sample, can we achieve this either by seeding the
bitstream with truly uniform random bits or by just directly directly sample
from the posterior.  It seems to me like there's not too much to do about this
inefficiency because the whole point of this method is to get "bits back" so
it's difficult to retrieve a truly random sample.  From [1], the effect of this
wasn't so clear but they have some theoretical discussion referencing some
prior work.</p>
</div>
</div>
<div class="section" id="implementation-details">
<h2><a class="toc-backref" href="#id10"><span class="sectnum">4</span> Implementation Details</a></h2>
<p>There were three main parts to my toy implementation: the ANS algorithm, a variational
autoencoder, and the Bits-Back algorithm.  Below are some details on each.  You
can find the code I used here on my <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/bitsback">Github</a>.</p>
<p><strong>ANS</strong>: The implementation I used was almost identical to the toy implementation I used
from my <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">previous post on ANS</a>
(which I wrote while travelling down the rabbit hole to understand the Bits-Back algorithm).
That post has some details on the toy implementation that I used for it.  These are
notes for the incremental changes I made:</p>
<ul class="simple">
<li><p>I had to fix some slow parts of my implementation or else my experiments
would have taken forever.  For example, I was calculating the CDF in a slow way using
native Python data structures.  Switching to Numpy <cite>cusum</cite> fixed some of
that.  Additionally, I had to make sure that all my arrays were in Numpy objects
and not slow native Python lists.</p></li>
<li><p>As part of the algorithm, you have to calculate very big numbers that could
exceed 64 bits (especially with an alphabet size of 256 and renormalization
factor of 32).  Python integers are great for this because they have arbitrary size. The
only thing I had to be careful of was converting between my Numpy operations and Python integers.
It was mostly just wrapping most expressions in the main calculation with <cite>int</cite> but took a bit
to get it all sorted out.</p></li>
<li><p>The code needed to be refactored a bit so it could be used in the Bits-Back
algorithm.  I refactored it to encode symbols incrementally instead of having
the entire message available in the original implementation.</p></li>
<li><p>I used 16 bits of quantization to model the distribution of the 256 pixel values and
32 bit renormalization.</p></li>
</ul>
<p><strong>Variational Autoencoder</strong>: I just used a vanilla VAE as the latent model since I was just
experimenting on MNIST.  In an effort to modernize, I started with the basic <a class="reference external" href="https://raw.githubusercontent.com/keras-team/keras-io/master/examples/generative/vae.py">VAE Keras 2.0 example</a>
and added a few modifications:</p>
<ul class="simple">
<li><p>I added some ResNet identity blocks to beef up the representation power.  Still not sure
it really made much of a difference.</p></li>
<li><p>Outputs of the decoder used my implementation of mixture of logistics to model the distributional
outputs per pixel with 3 components.  I wrote about it a bit in my <a class="reference external" href="http://localhost:8000/posts/pixelcnn/">PixelCNN</a> post.  I'm also wary about whether
or not this actually made it better.  The original paper [1] just used a Beta-Binomial distribution.</p></li>
<li><p>I used 50 latent dimensions to match [1].</p></li>
</ul>
<p><strong>Bits-Back Algorithm</strong>: The Bits-Back algorithm is conceptually pretty simple but requires you to be a
bit careful in a few areas.  Here are some of the notable points:</p>
<ul>
<li><p>Since the quantization was always using equi-probable bins from a standard normal
distribution, it made sense to cache the ranges for speeding it up.</p></li>
<li><p>When quantizing the bins, you have to be careful at the edge bins which
represent <span class="math">\(+\infty,-\infty\)</span> and the quantized value for that bin
obviously can't be infinity.  I just used the next closest bin edge instead.</p></li>
<li><p>Quantizing the continuous values of the latent variable distributions was a
pain.  For the case of quantizing a standard normal distribution (i.e., the
prior), it was easy because, by construction, each bin is equi-probable.  So
the distribution is just uniform across however many buckets we are using
(5-bits in my experiments).</p></li>
<li>
<p>However, if you're trying to quantize a non-standard <span class="math">\(\bf z\)</span> normal
distributions using equi-probable bins from a <em>standard</em> normal distribution
you have to be a bit more careful.  Two big things to consider here:</p>
<ol class="arabic">
<li><p>Ensure is that each bin has a <strong>non-zero</strong> probability or else when you're
using it later to decode and gets bits back, you might actually end up seeing a
zero probability symbol, which will break things.  Specifically, you want
each bucket to have at least probability <span class="math">\(\frac{1}{2^{n}}\)</span>.</p></li>
<li>
<p>You want the quantized distribution to best match the original non-quantized one
but with the constraint that we have some mass in each bin.  Here's the
algorithm I used:</p>
<ul class="simple">
<li><p>First, define the number of symbols in your alphabet <span class="math">\(m\)</span> (i.e.,
number of bins); I used <span class="math">\(m=5\)</span>.</p></li>
<li><p>Sample <span class="math">\(2^n - 2^m\)</span> for <span class="math">\((n=16, m=5)\)</span> equi-probable-spaced values per
variable from the original <span class="math">\(\bf z\)</span> distributions using the inverse
CDF function from SciPy.  This guarantees you get good representation
from the entire distribution.</p></li>
<li><p>From those sampled values, make a frequency histogram where the buckets
correspond to the <span class="math">\(m\)</span> equi-probable
buckets.  This is essentially the probability distribution in frequency form.</p></li>
<li><p>Add 1 to each of the histogram buckets to guarantee a non-zero
probability mass (see point 1 above).</p></li>
<li><p>This histogram serves as the probability distribution used by rANS to decode
the required values.  And because of the way rANS algorithm works, it uses a
frequency distribution so I could just pass it directly to the compressor.</p></li>
</ul>
<p>I'm not sure if there's a better way to do it but it seemed work well enough.</p>
</li>
</ol>
</li>
<li><p>Encoding/decoding the <span class="math">\(x\)</span> pixel values was much easier because they are
already discretized as 256 pixel values.  It's still a bit slow though since
I just loop through each pixel value and encode it sequentially.</p></li>
<li><p>The only tricky part for the pixel values was that I had to translate the probability
distribution (real numbers) over discrete pixels into a frequency distribution (integers).
That is for each pixel, we have discrete distribution over the 256 values but
each value has a real number that represents its probability.  So we need to convert it
to a frequency distribution in order to pass it to ANS.</p></li>
<li><p>The sum of the frequency distribution also needs to sum to <span class="math">\(2^{n}\)</span>.
Additionally, I wanted to ensure that no bin had zero probability (same issue
as above).  To pull this off, I just multiplied each bin's probability by
<span class="math">\(2^{n}\)</span>, added one to each bin, then calculate any
excess I have beyond <span class="math">\(2^{n}\)</span> and shave it off the
largest frequency bin.  This is obviously not an optimal way to do it but it <em>probably</em>
doesn't affect too much.</p></li>
<li><p>Again, I had to be careful not to implicitly convert some of the integer values to floats.
So in some places, I do some explicit casting of <cite>astype(np.uint64)</cite> so the values don't
get all mixed up when I send them into ANS.</p></li>
</ul>
</div>
<div class="section" id="experiments">
<h2><a class="toc-backref" href="#id11"><span class="sectnum">5</span> Experiments</a></h2>
<p>My compression results for MNIST (regular, non-binarized) are shown in Table 1.
My implementation can achieve 1.5264 bits/pixel (i.e. on average a pixel uses
1.53 bits to represent it) vs. other implementations of 1.4 or less.
That's not half bad (better than my original implementation which had a bug where I
got closer to 1.9 bits/pixel)!  I was at least able to beat gzip, which says something.
It still feels kind of far from [1] at 1.41, but it feels like I got the main theoretical
parts worked out.</p>
<table class="colwidths-given align-center">
<caption>Table 1: Compression Rates for MNIST (bits/pixel)</caption>
<colgroup>
<col style="width: 60%">
<col style="width: 40%">
</colgroup>
<thead><tr>
<th class="head"><p>Compressor</p></th>
<th class="head"><p>Compression Rates (bits/pixel)</p></th>
</tr></thead>
<tbody>
<tr>
<td><p>My Implementation (Bits-Back w/ ANS)</p></td>
<td><p>1.5264</p></td>
</tr>
<tr>
<td><p>Bits-Back w/ ANS [1]</p></td>
<td><p>1.41</p></td>
</tr>
<tr>
<td><p>Bits-Swap [2]</p></td>
<td><p>1.29</p></td>
</tr>
<tr>
<td><p>gzip</p></td>
<td><p>1.64</p></td>
</tr>
<tr>
<td><p>bz2</p></td>
<td><p>1.42</p></td>
</tr>
<tr>
<td><p>Uncompressed</p></td>
<td><p>8.00</p></td>
</tr>
</tbody>
</table>
<p>Interestingly [1] was using a simpler model (a single feed-forward layer for each of the encoder/decoder)
with a Beta-Binomial output distribution for each pixel.  This is obviously simpler than
my complex ResNet/multi-logistic method.  It's possible that I'm just not able to get a good fit with
my VAE model.  If you take a look in the notebook you'll see that the generated digits I can make
with the decoder look pretty bad.  So this is probably at least part of the reason why I was unable
to achieve good results.</p>
<p>The second reason is that I suspect my quantization isn't so great.  As mentioned above, I did so
funky rounding to ensure no zero-probability buckets, as well as an awkward way to discretize the
latent variables and data. I suspect there are some differences from [1]'s
implementation (which is open source by the way) but I didn't spend too much
time trying to diagnose the differences.</p>
<p>In any case, at least my implementation is able to <em>correctly</em> encode and decode and somewhat
approach the proper implementations.  As a toy implementation, I will make the bold assertion
that I coded it in a way that's a bit more clear than [1]'s implementation (and
has this handy write up) so maybe it's better for educational purposes?  I'll
let you be the judge of that.</p>
</div>
<div class="section" id="conclusion">
<h2><a class="toc-backref" href="#id12"><span class="sectnum">6</span> Conclusion</a></h2>
<p>So there you have it, a method for lossless compression using ML!  This mix of discrete
problems (e.g. compression) and ML is an incredibly interesting direction.  If
I get some time (and who knows when that will be), I'm definitely going to be
looking into some more of these topics.  But on this lossless compression topic, I'm
probably done for now.  There's another topic that I've been excited about recently
and have already started to go down that rabbit hole, so expect one (probably more)
posts on that subject.  Hope everyone is staying safe!</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id13"><span class="sectnum">7</span> References</a></h2>
<ul class="simple">
<li><p>Previous posts: <a class="reference external" href="../variational-autoencoders/">Variational Autoencoders</a>, <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">Lossless Compression with Asymmetric Numeral Systems</a>, <a class="reference external" href="../the-expectation-maximization-algorithm/">Expectation Maximization Algorithm</a></p></li>
<li><p>My implementation on Github: <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/bitsback">notebooks</a></p></li>
<li><p>[1] "Practical Lossless Compression with Latent Variables using Bits Back Coding", Townsend, Bird, Barber, <a class="reference external" href="https://arxiv.org/abs/1901.04866">ICLR 2019</a>.</p></li>
<li><p>[2] "Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables", Kingma, Abbeel, Ho, <a class="reference external" href="https://arxiv.org/abs/1905.06845">ICML 2019</a></p></li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>I'll be using this term "pseudo-random" to describe this faux random sampling process.  I know it's probably not accurate to even call it pseudo-random, but I'm using it colloquially to mean not truly random.</p>
</dd>
</dl>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/asymmetric-numeral-systems/" rel="tag">asymmetric numeral systems</a></li>
            <li><a class="tag p-category" href="../../categories/bits-back/" rel="tag">Bits-Back</a></li>
            <li><a class="tag p-category" href="../../categories/compression/" rel="tag">compression</a></li>
            <li><a class="tag p-category" href="../../categories/lossless/" rel="tag">lossless</a></li>
            <li><a class="tag p-category" href="../../categories/mnist/" rel="tag">MNIST</a></li>
            <li><a class="tag p-category" href="../../categories/variational-autoencoder/" rel="tag">variational autoencoder</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../lossless-compression-with-asymmetric-numeral-systems/" rel="prev" title="Lossless Compression with Asymmetric Numeral Systems">Previous post</a>
            </li>
            <li class="next">
                <a href="../hamiltonian-monte-carlo/" rel="next" title="Hamiltonian Monte Carlo">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            Hi, I'm <a href="http://www.briankeng.com/about">Brian Keng</a>.  This is
            <a href="../../">the place</a> where I write about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2025         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
