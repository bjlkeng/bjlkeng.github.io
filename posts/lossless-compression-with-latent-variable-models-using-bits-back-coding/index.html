<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A post on using bits-back coding with latent variable models to do lossless compression.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Lossless Compression with Latent Variable Models using Bits-Back Coding | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../lossless-compression-with-asymmetric-numeral-systems/" title="Lossless Compression with Asymmetric Numeral Systems" type="text/html">
<link rel="next" href="../hamiltonian-monte-carlo/" title="Hamiltonian Monte Carlo" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Lossless Compression with Latent Variable Models using Bits-Back Codin">
<meta property="og:url" content="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/">
<meta property="og:description" content="A post on using bits-back coding with latent variable models to do lossless compression.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2021-07-06T11:00:00-05:00">
<meta property="article:tag" content="asymmetric numeral systems">
<meta property="article:tag" content="Bits-Back">
<meta property="article:tag" content="compression">
<meta property="article:tag" content="lossless">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="MNIST">
<meta property="article:tag" content="variational autoencoder">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Lossless Compression with Latent Variable Models using Bits-Back Coding</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2021-07-06T11:00:00-05:00" itemprop="datePublished" title="2021-07-06 11:00">2021-07-06 11:00</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>A lot of modern machine learning is related to this idea of "compression", or
maybe to use a fancier term "representations".  Taking a huge dimensional space
(e.g. images of 256 x 256 x 3 pixels = 196608 dimensions) and somehow compressing it into
a 1000 or so dimensional representation seems like pretty good compression to
me!  Unfortunately, it's not a lossless compression (or representation).
Somehow though, it seems intuitive that there must be a way to use what is learned in
these powerful lossy representations to help us better perform <em>lossless</em>
compression, right?  Of course there is! (It would be too anti-climatic of a
setup otherwise.)</p>
<p>This post is going to introduce a method to perform lossless compression that
leverages the learned "compression" of a machine learning latent variable
model using the Bits-Back coding algorithm.  Depending on how you first think
about it, this <em>seems</em> like it should either be (a) really easy or (b) not possible at
all.  The reality is kind of in between with an elegant theoretical algorithm
that is brought down by the realities of discretization and imperfect learning
by the model.  In today's post, I'll skim over some preliminaries (mostly
referring you to previous posts), go over the main Bits-Back coding algorithm
in detail, and discuss some of the implementation details and experiments that
I did while trying to write a toy version of the algorithm.</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Background </h4>
<p>A <strong>latent (or hidden) variable model</strong> is a statistical model where
you <em>don't</em> observe some of the variables (or in many cases, you create additional
intermediate unobserved variables to make the problem more tractable).  This
creates a relationship graph (usually a DAG) between observed (either input or
output, if applicable) and latent variables.  Often times the modeller will
have an explicit intuition about the meaning of these latent variables; other
times (especially for deep learning models), the modeller doesn't explicitly
give these latent variables meaning, and instead they are learned to represent
whatever the optimization procedure deems necessary.  See the background
section of my <a class="reference external" href="../the-expectation-maximization-algorithm/">Expectation Maximization Algorithm</a> post for more details.</p>
<p>A <strong>variational autoencoder</strong> is a special type of latent variable model that
contains two parts:</p>
<ol class="arabic simple">
<li><p>A generative model (aka "decoder") that defines a mapping from some
latent variables (usually independent standard Gaussians) to your data
distribution (e.g. images).</p></li>
<li><p>An approximate posterior network (aka "encoder") that maps from your
data distribution (e.g. images) to your latent variable space.</p></li>
</ol>
<p>There's also a bunch of math, a reparameterization trick, and some deep nets
strung together to make it all work out relatively nicely.  See my post of
<a class="reference external" href="../variational-autoencoders/">Variational Autoencoders</a> for more
details.</p>
<p>A special type of lossless compression algorithm called an <em>entropy encoder</em> exploits
the statistical properties of your input data to compress data efficiently. A
relatively new algorithm to perform this lossless compression is called
<strong>Asymmetrical Numeral Systems</strong> (ANS), which essentially map any input data string
to a (really large) natural number in a smart way such that frequent (or
predictable) characters/strings get mapped to smaller numbers, while infrequent
ones get mapped to larger ones.  One key feature of this algorithm is that
the compressed data stream is generated in last-in-first-out order (i.e. in
stack order).  This means when decompressing, you want to read the compressed data
from back-to-front (this will be useful for our discussion below).
My post on <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">Lossless Compression with Asymmetric Numeral Systems</a> gives a
much better intuition on the whole process and discusses a few variations and
implementation details.</p>
<p><br></p>
<h4> Lossless Compression with Probabilistic Models </h4>
<p>Suppose we wanted to perform lossless compression on datum
<span class="math">\({\bf x}=[x_1,\ldots,x_n]\)</span> composed of a vector (or tensor) of symbols
(i.e. an image <span class="math">\(\bf x\)</span> made up of <span class="math">\(n\)</span> pixels), and we are given:</p>
<ol class="arabic simple">
<li><p>A model that can tell us the probability of each symbol,
<span class="math">\(P_x(x_i|\ldots)=p_i\)</span> (that may or may not have some conditioning on it).</p></li>
<li><p>A entropy encoder (and corresponding decoder) that can return a compressed
stream of bits given input symbols and their probability distributions, call
it: <span class="math">\(B=\text{CODE}({\bf x}, P_x)\)</span>.</p></li>
</ol>
<p>Note: Here we are distinguishing between data points (denoted by a bold <span class="math">\(\bf x\)</span>),
and the different components of that data point (not bolded <span class="math">\(x_i\)</span>).
Usually we will want to encode <em>multiple</em> data points (e.g. images), each of which contains
multiple components (e.g. pixels).</p>
<p>Compression is pretty straight forward by just calling <span class="math">\(\text{CODE}\)</span>
with the input datum and its respective probability distribution to get
your compressed stream of bits <span class="math">\(B\)</span>. To decode, simply call the
corresponding <span class="math">\({\bf x}=\text{DECODE}(B, P_x)\)</span> function to recover your desired
input data.  Notice that our compressed bit stream needs to be paired with
<span class="math">\(P_x\)</span> or else we won't be able to decode it.</p>
<p>"Model" here can be really simple, we could just use a histogram of each
symbol in our input data stream, and this could serve as our probability
distribution.  You would expect that this simple approach would not do very well
for complex data, for example, trying to compress various images.  A histogram
for pixel values (i.e. symbols) across all images would be very dispersed
because images are so varied in their pixel values.  This implies any given
pixel would have small probabilities, and thus have poor compression (recall
the higher the probability of a symbol the better entropy compression methods
work).</p>
<p>So the intuition here is that we want a model that can <em>accurately</em> predict
(via a probability distribution) our datum and its corresponding symbols to allow the
entropy encoder to efficiently compress.  For example, we want our model to be
able to <em>accurately</em> generate a tight distribution around each of an image's
pixel values.  This would allow our entropy encoder to achieve a high
compression rate.  But there's a small problem here: to generate a tight
distribution around each symbol within a datum, we would need a different
distribution for each datum, otherwise any distribution we generate will be too
dispersed (e.g. not accurate) because it would have to apply to any generic datum.
Following this logic, we need a probability distribution to encode and decode
our datum but that distribution is conditional on the actual datum.  This means
when we're trying to decode the datum, we need to have the datum (encoding is
not a problem because we have the datum available)!  This circular dependency
can be resolved by using specific types of models and encoding things in a
particular order, which is the topic of the next two subsections.</p>
<p></p>
<h5> Generative Autoregressive Models </h5>
<p>A generative autoregressive models simply uses the probability chain rule to model the data:</p>
<div class="math">
\begin{equation*}
P({\bf x}) = \prod_{i=1}^{D} P_x(x_i | {\bf x}_{&lt;i})  \tag{1}
\end{equation*}
</div>
<p>Each indexed component of <span class="math">\(\bf x\)</span> (i.e. pixel) is dependent only on the
previous ones, using whatever indexing makes sense.  See my post on <a class="reference external" href="../pixelcnn/">PixelCNN</a>
for more details.</p>
<p>When using this type of model with an ANS entropy encoder, we have to be a bit
careful because it decompresses symbols in the last-in-first-out (stack) order.
Let's take a closer look in Figure 1.</p>
<div class="figure align-center">
<img alt="Entropy Encoding and Decoding with a Generative Autoregressive Model" src="../../images/bbans_autoregressive.png" style="width: 700px;"><p class="caption">Figure 1: Entropy Encoding and Decoding with a Generative Autoregressive Model</p>
</div>
<p>From the left side of Figure 1, notice that we have to
reverse the order of the input data to the ANS encoder (the autoregressive
model receives the input in the usual parallel order though).  This is needed
because we need to decode the data in the ascending order for the
autoregressive model to work (see decoding below).  Next, notice that our ANS
encoder requires both the (reversed) input data and the appropriate
distributions for each symbol (i.e.  each <span class="math">\(x_j\)</span> component).  Finally, the
compressed data is output, which (hopefully) is shorter than the original
input.</p>
<p>Decoding is shown on the right hand side of Figure 1.  It's a bit more
complicated because we must <em>iteratively</em> generate the distributions for each
symbol.  Starting by reversing the compressed data, we decode <span class="math">\(x_1\)</span> since
our model can unconditionally generate its distribution.
This is the reason why we needed to reverse our input during encoding.  Then,
we generate <span class="math">\(x_2|x_1\)</span> and so on for each <span class="math">\(x_i|x_{1,\ldots,i-1}\)</span>
until we've recovered the original data.  Notice that this is quite inefficient
since we have to call the model <span class="math">\(n\)</span> times for each component of
<span class="math">\(\bf x\)</span>.</p>
<p>I haven't tried this but it seems like something pretty reasonable to do
(assuming you have a good model <em>and</em> I haven't made a serious logical error).
The only problem with generative autoregressive models is that they are slow
because you have to call them <span class="math">\(n\)</span> times.  Perhaps that's why no one is
interested in this?  In any case, the next method overcomes this problem.</p>
<p></p>
<h5> Latent Variable Models </h5>
<p>Latent variable models have a set of unobserved variables <span class="math">\(\bf z\)</span> in
addition to the observed ones <span class="math">\(\bf x\)</span>, giving us a likelihood function
of <span class="math">\(P(\bf x|\bf z)\)</span>.  We'll usually have a prior distribution
<span class="math">\(P({\bf z})\)</span> for <span class="math">\(\bf z\)</span> (implicitly or explicitly), and depending
on the model, we may or may not have access to a posterior distribution (more
likely an estimate of it) as well: <span class="math">\(q(\bf z| \bf x)\)</span>.</p>
<p>The key idea with these models is that we need to encode the latent variables
as well (or else we won't be able to generate the required distributions for
<span class="math">\(\bf x\)</span>).  Let's take a look at Figure 2 to see how the encoding works.</p>
<div class="figure align-center">
<img alt="Entropy Encoding with a Latent Variable Model" src="../../images/bbans_latent_encode.png" style="width: 600px;"><p class="caption">Figure 2: Entropy Encoding with a Latent Variable Model</p>
</div>
<p>Starting from the input data, we need to first generate some value for our
latent variable <span class="math">\(\bf z\)</span> so that we can use it with our model <span class="math">\(P(\bf x|\bf z)\)</span>.
This can be obtained either by sampling the prior (or posterior if available),
or really any other method that would generate an accurate distribution for <span class="math">\(\bf x\)</span>.
Once we have <span class="math">\(\bf z\)</span>, we can run it through our model, get distributions
for each <span class="math">\(x_i\)</span> and encode the input data as usual.  The one big
difference is that we also have to encode our latent variables.  The latent
variables <em>should</em> be distributed according to our prior distribution (for most
sensible models), so we can use it with the ANS coder to compress <span class="math">\(\bf
z\)</span>.  Notice that we cannot use the posterior here because we won't have access
to <span class="math">\(\bf x\)</span> at decompression time, therefore, would not be able to
decompress <span class="math">\(\bf z\)</span>.</p>
<div class="figure align-center">
<img alt="Entropy Decoding with a Latent Variable Model" src="../../images/bbans_latent_decode.png" style="width: 600px;"><p class="caption">Figure 3: Entropy Decoding with a Latent Variable Model</p>
</div>
<p>Decoding is shown in Figure 3 and works basically as the reverse of encoding.
The major thing to notice is that we have to do operations in a
last-in-first-out order.  That is, first decode <span class="math">\(\bf z\)</span>, use it to
generate distributional outputs for the components of <span class="math">\(\bf x\)</span>, then
use those outputs to decode the compressed data to recover our original message.</p>
<p>This is all relatively straight forward if you took time to think about it.
There are some other issues as well around discretizing <span class="math">\(\bf z\)</span> if it's
continuous but we'll cover that below.  The more interesting question is can we
do better?  The answer is a resounding "Yes!", and that's what this post is all
about.  By using a very clever trick you can get some "bits back" to improve
your compression performance.  Read on to find out more!</p>
<p><br></p>
<h4> Bits-Back Coding </h4>
<p>From the previous section, we know that we can encode and decode data using a
latent variable model with relative ease.  The big downside is that we're
"wasting" space by encoding the latent variables.  They're necessary to
generate the distributions for our data, but otherwise are not directly
encoding any signal.  It turns out we can use a clever trick to recover
some of this "waste".</p>
<p>Notice in Figure 2, we randomly sample from (an estimate of) the posterior distribution.
In some sense, we're introducing new information from the random sample here
that we must encode.  Instead, why don't we utilize some of the existing bits
we've encoded to get a "pseudo-random" sample <a class="footnote-reference brackets" href="#id2" id="id1">1</a> ?  Figure 4 shows the encoding
process in more detail.</p>
<div class="figure align-center">
<img alt="Bits-Back Encoding with a Latent Variable Model" src="../../images/bbans_bb_encode.png" style="width: 600px;"><p class="caption">Figure 4: Bits-Back Encoding with a Latent Variable Model</p>
</div>
<p>The key difference here is that we're decoding the existing bitstream (from
previous data that we've compressed) to generate a (pseudo-) random sample <span class="math">\(\bf z\)</span>
using the posterior distribution.  This replaces the random sampling we
did in Figure 2.  Since the existing bitstream was encoded using a different
distribution, the sample we decode should <em>sort of</em> random.  The nice part
about this trick is that we're still going to encode <span class="math">\(\bf z\)</span> as usual so
any bits we've popped off the bitstream to generate our pseudo-random sample,
we get "back" (that is, don't require to be on the bitstream anymore).  This
<em>reduces</em> the effective average size of encoding each datum + latent variables.</p>
<div class="figure align-center">
<img alt="Bits-Back Decoding with a Latent Variable Model" src="../../images/bbans_bb_decode.png" style="width: 600px;"><p class="caption">Figure 5: Bits-Back Decoding with a Latent Variable Model</p>
</div>
<p>Figure 5 shows decoding with Bits-Back.  It is the same as latent variable
decoding with the exception that we have to "put back" the bits we took off
originally.  Since our ANS encoding and decoding are lossless, the bits we
put back should be exactly the bits we took off.  The number of bits we
removed/put back will be dependent on the posterior distribution and the bits that
were originally there.</p>
<div class="figure align-center">
<img alt="Visualization of Bitstream for Bits-Back Coding" src="../../images/bbans_bitstream_view.png" style="width: 600px;"><p class="caption">Figure 6: Visualization of Bitstream for Bits-Back Coding</p>
</div>
<p>To get a better sense of how it works, Figure 6 shows a visualization of
encoding and decoding two data points.  Colors represent the different
data: green for existing bitstream, blue for <span class="math">\(\bf x^1\)</span>, and orange for <span class="math">\(\bf x^2\)</span>
(superscript represents data point index).  The different shades represent either
observed data <span class="math">\(\bf x\)</span> or latent variable <span class="math">\(\bf z\)</span>.</p>
<p>From Figure 6, the first step in the process is to <em>reduce</em> the bitstream length
by (pseudo-)randomly sampling <span class="math">\(\bf z\)</span>.  This is followed by encoding
<span class="math">\(\bf x\)</span> and <span class="math">\(\bf z\)</span> as usual.  This process repeats for each
additional datum.  Even though we have to encode <span class="math">\(\bf z\)</span>, the effective
size of the encoding is shorter because of the initial "bits back" we get each
time.  Decoding is the reverse operation of encoding: decode <span class="math">\(\bf z\)</span> and
<span class="math">\(\bf x\)</span>, put back the removed bits by utilizing the posterior
distribution (which is conditional on the <span class="math">\(\bf x\)</span> we just decoded).
And this repeats until all data has been decoded.</p>
<p></p>
<h5> Theoretical Limit of Bits-Back Coding </h5>
<p>Turning back to some more detailed mathematical analysis, let's see how good
Bits-Back is theoretically.  We'll start off with a few assumptions:</p>
<ol class="arabic simple">
<li><p>Our data <span class="math">\(\bf x\)</span> and latent variables <span class="math">\(\bf z\)</span> are sampled from
the true joint distribution <span class="math">\(P({\bf x, z})=P({\bf x|z})P({\bf z})\)</span>,
which we have access to.  Of course in the real world, we don't have the
true distribution, just an approximation.  But if our model is very good, it
will hopefully be very close to the true distribution.</p></li>
<li><p>We have access to an approximate posterior <span class="math">\(q({\bf z|x})\)</span>.</p></li>
<li><p>Assume we have an entropy coder so that we can optimally code any data point.</p></li>
<li><p>The pseudo-random sample we get from Bits-Back coding is drawn from the approximate posterior <span class="math">\(q({\bf z|x})\)</span>.</p></li>
</ol>
<p>As noted above, if we naively use the latent variable encoding from Figure 2,
given a sample <span class="math">\((x, z)\)</span>, our expected message length should be
<span class="math">\(-(\log P({\bf z}) + \log P({\bf x|z}))\)</span> bits long.  This uses the fact
(roughly speaking) that the theoretical limit of the average number of bits
needed to represent a symbol (in the context of its probability distribution)
is its <a class="reference external" href="https://en.wikipedia.org/wiki/Information_content">information</a>.</p>
<p>However using Bits-Back with an approximate posterior <span class="math">\(q({\bf z|x})\)</span>
for a given <em>fixed</em> data point <span class="math">\(\bf x\)</span>, we can calculate the expected
message length over all possible <span class="math">\(\bf z\)</span> drawn from <span class="math">\(q({\bf z|x})\)</span>.
The idea is that we're (pseudo-)randomly drawing <span class="math">\(\bf z\)</span> values, which
affect each part of the process (Bits-Back, encoding <span class="math">\(x\)</span>, and encoding
<span class="math">\(z\)</span>) so we must average (i.e. take the expectation) over it:</p>
<div class="math">
\begin{align*}
L(q) &amp;= E_{q({\bf z|x})}[-\log P({\bf z}) - \log P({\bf x|z}) + \log q({\bf z|x})] \\
     &amp;= \sum_z q({\bf z|x})[-\log P({\bf z}) - \log P({\bf x|z}) + \log q({\bf z|x})]  \\
     &amp;= -\sum_z q({\bf z|x})\log \frac{P({\bf x, z})}{q({\bf z|x})}  \\
     &amp;= -E_{q({\bf z|x})}\big[\log \frac{P({\bf x, z})}{q({\bf z|x})}\big]  \\
     \tag{2}
\end{align*}
</div>
<p>Equation 2 is also known as the evidence lower bound (ELBO) (see my previous
<a class="reference external" href="../semi-supervised-learning-with-variational-autoencoders/">post on VAE</a>
for more details).  The nice thing about the ELBO is that many ML models (including
the variational autoencoder) use it as its objective function.  So by optimizing our
model, we're simultaneously optimizing our message length.</p>
<p>From Equation 2, we can also see that it is optimized when <span class="math">\(q({\bf z|x})\)</span> equals
to the true posterior <span class="math">\(P({\bf z|x})\)</span>:</p>
<div class="math">
\begin{align*}
-E_{q({\bf z|x})}\big[\log \frac{P({\bf x, z})}{q({\bf z|x})}\big]
&amp;= -E_{q({\bf z|x})}\big[\log \frac{P({\bf z|x})P({\bf x})}{P({\bf z|x})}\big]  &amp;&amp; \text{since }
P({\bf x, z}) = P({\bf z|x})P({\bf x}) \text{ and } q({\bf z|x})=P({\bf z|x}) \\
&amp;= -E_{q({\bf z|x})}\big[\log P({\bf x^1})\big] \\
&amp;= -\log P({\bf x}) \\
\tag{3}
\end{align*}
</div>
<p>Which is the optimal code length for sending our data point <span class="math">\(x\)</span> across.
So <em>theoretically</em> if we're able to satisfy all the assumptions and have an
exact posterior then we'll have a really good compressor!  Of course, we'll never
be in this theoretic ideal situation, we'll discuss some of the issues that
reduce this efficiency in the next subsection.</p>
<p></p>
<h5> Issues Affecting The Efficiency of Bits-Back Coding </h5>
<p><strong>Transmitting the Model</strong>: All the above discussion assumes that the sender
and receiver have access to the latent variable model but that needs to be sent
as well!  The assumption here is that the model would be so generally applicable
that the compression package would include it by default (or have a plugin) to
download the model.  For example, photos are so common that we conceivably have
a single latent variable model for photos (e.g. something along the lines of
an ImageNet model).  This would enable the compression encoder/decoder package to include
it and encode images or whatever data distribution it is trained on.  For the
experiments below, I don't include the model size but if I did, it would be
much worse.  I think the benefits are only realized if you can amortize the
cost of sending the model over a huge dataset (this is similar to the fact that
you also need to have the compression/decompression algorithm on each side).</p>
<p><strong>Discretization</strong>: Another thing that the above discussion glossed over is how
to encode/decode continuous variables.  ANS (and similar entropy coders) work
on discrete symbols -- not continuous values.  Many popular latent variable
models also have continuous latent variables (e.g. normally distributed), so there
needs to be a discretization step to be able to send them over the wire (we're
assuming the data is discretized already but the same principles would apply).</p>
<p>Discretization is needed for both the approximate posterior encoding/decoding
where we (pseudo-)randomly sample our <span class="math">\(\bf z\)</span> value ("bits back"), and for
when we encode/decode the latent variables using the prior distribution.
Discretization of a sample from the distribution is a relatively simple
operation:</p>
<ol class="arabic simple" start="0">
<li><p>Select how many bits you want to use to represent your sample.  This will
create <span class="math">\(2^n\)</span> buckets for <span class="math">\(n\)</span> bits.</p></li>
<li><p>Partition the distribution's support into <span class="math">\(2^n\)</span> buckets.  [1] proposed
equi-probable mass buckets, which is what I implemented.</p></li>
<li><p>Find the corresponding bucket index (<span class="math">\(i\)</span>) the point falls in, set the
discretized value to some value relative to the bucket interval (e.g.
mid-point of the bucket interval).</p></li>
<li><p>You can use ANS to encode the discretized value as the symbol <span class="math">\(i\)</span> with
an alphabet of <span class="math">\(2^n\)</span> symbol values.  Note: If you use an equi-probable mass
buckets each symbol will have the same probability, so entropy encoding
shouldn't do much for a randomly sampled point.</p></li>
</ol>
<p>To decode, you do the reverse operation.  However, there is a
subtlety: you need the sender and receiver to have access to the distribution
in step 1.  The natural choice is the prior, which is what is available throughout
the process (assuming you have the model).  You wouldn't be able to use the
posterior because when you are trying to decode <span class="math">\(\bf z\)</span>, you would need access
to <span class="math">\(\bf x\)</span>, which you don't have available.  Additionally, you need
to have the same discretization step when you're sampling via "bits back" and
when sending <span class="math">\(\bf z\)</span> across the wire, or else you lose some precision in
the process.  So the prior is used throughout.</p>
<p>The paper [1] shows that the additional overhead is really just the cost of
discretization.  Since many continuous ML operations work fine with 32-bits
anyways, it shouldn't practically be a problem if you have enough precision.
As the paper suggests, I used a 16-bit discretization.</p>
<p><strong>Clean Bits</strong>: The last issue to discuss is the how the Bits-Back operation
pseudo-randomly samples the <span class="math">\(\bf z\)</span> value.  Since we're sampling from the
bits from the top of the existing bitstream, which is essentially the previous
prior-encoded posterior sample, we would not expect it to be a true random
sample (which would require a uniform random stream of bits).  Only in the
base-case with the first sample, can we achieve this either by seeding the
bitstream with truly uniform random bits or by just directly directly sample
from the posterior.  It seems to me like there's not too much to do about this
inefficiency because the whole point of this method is to get "bits back" so
it's difficult to retrieve a truly random sample.  From [1], the effect of this
wasn't so clear but they have some theoretical discussion referencing some
prior work.</p>
<p><br></p>
<h4> Implementation Details </h4>
<p>There were three main parts to my toy implementation: the ANS algorithm, a variational
autoencoder, and the Bits-Back algorithm.  Below are some details on each.  You
can find the code I used here on my <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/bitsback">Github</a>.</p>
<p><strong>ANS</strong>: The implementation I used was almost identical to the toy implementation I used
from my <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">previous post on ANS</a>
(which I wrote while travelling down the rabbit hole to understand the Bits-Back algorithm).
That post has some details on the toy implementation that I used for it.  These are
notes for the incremental changes I made:</p>
<ul class="simple">
<li><p>I had to fix some slow parts of my implementation or else my experiments
would have taken forever.  For example, I was calculating the CDF in a slow way using
native Python data structures.  Switching to Numpy <cite>cusum</cite> fixed some of
that.  Additionally, I had to make sure that all my arrays were in Numpy objects
and not slow native Python lists.</p></li>
<li><p>As part of the algorithm, you have to calculate very big numbers that could
exceed 64 bits (especially with an alphabet size of 256 and renormalization
factor of 32).  Python integers are great for this because they have arbitrary size. The
only thing I had to be careful of was converting between my Numpy operations and Python integers.
It was mostly just wrapping most expressions in the main calculation with <cite>int</cite> but took a bit
to get it all sorted out.</p></li>
<li><p>The code needed to be refactored a bit so it could be used in the Bits-Back
algorithm.  I refactored it to encode symbols incrementally instead of having
the entire message available in the original implementation.</p></li>
<li><p>I used 16 bits of quantization to model the distribution of the 256 pixel values and
32 bit renormalization.</p></li>
</ul>
<p><strong>Variational Autoencoder</strong>: I just used a vanilla VAE as the latent model since I was just
experimenting on MNIST.  In an effort to modernize, I started with the basic <a class="reference external" href="https://raw.githubusercontent.com/keras-team/keras-io/master/examples/generative/vae.py">VAE Keras 2.0 example</a>
and added a few modifications:</p>
<ul class="simple">
<li><p>I added some ResNet identity blocks to beef up the representation power.  Still not sure
it really made much of a difference.</p></li>
<li><p>Outputs of the decoder used my implementation of mixture of logistics to model the distributional
outputs per pixel with 3 components.  I wrote about it a bit in my <a class="reference external" href="http://localhost:8000/posts/pixelcnn/">PixelCNN</a> post.  I'm also wary about whether
or not this actually made it better.  The original paper [1] just used a Beta-Binomial distribution.</p></li>
<li><p>I used 50 latent dimensions to match [1].</p></li>
</ul>
<p><strong>Bits-Back Algorithm</strong>: The Bits-Back algorithm is conceptually pretty simple but requires you to be a
bit careful in a few areas.  Here are some of the notable points:</p>
<ul>
<li><p>Since the quantization was always using equi-probable bins from a standard normal
distribution, it made sense to cache the ranges for speeding it up.</p></li>
<li><p>Quantizing the continuous values of the latent variable distributions was
pain.  For the case of quantizing a standard normal distribution, it was easy
because, by construction, each bin is equi-probable.  So the distribution is just uniform
across however many buckets we are using (16-bits in my experiments to match the paper).</p></li>
<li>
<p>However, if you're trying to quantize a non-standard <span class="math">\(\bf z\)</span> normal
distributions using equi-probable bins from a <em>standard</em> normal distribution
(e.g. when we're trying to sample the pseudo-random <span class="math">\(\bf z\)</span> value),
you have to be a bit more careful:</p>
<ol class="arabic simple">
<li><p>I sampled <strong>2^n</strong> (n=14 in my case) equi-probable-spaced values per
variable from the original <span class="math">\(\bf z\)</span> distributions using the inverse
CDF function from SciPy.</p></li>
<li><p>From those sampled values, I made a frequency histogram where the
buckets correspond to a <em>standard</em> normal distribution equi-probable
buckets.  This is essentially the probability distribution in frequency form.</p></li>
<li><p>This histogram served as the probability distribution used by ANS to decode
the required values.  And because of the way ANS algorithm works, it uses a
frequency distribution so I could just pass it directly to the compressor.</p></li>
</ol>
<p>I'm not sure if there's a better way to do it but it seemed work well enough.
You can increase the number of samples in Step 1 to get a more accurate frequency
distribution but it slows down the algorithm as you might expect.</p>
</li>
<li><p>Encoding/decoding the <span class="math">\(x\)</span> pixel values was much easier because they are
already discretized as 256 pixel values.  It's still a bit slow though since
I just loop through each pixel value and encode it sequentially.</p></li>
<li><p>The only tricky part for the pixel values was that I had to translate the probability
distribution (real numbers) over discrete pixels into a frequency distribution (integers).
That is for each pixel, we have discrete distribution over the 256 values but
each value has a real number that represents its probability.  So we need to convert it
to a frequency distribution in order to pass it to ANS.</p></li>
<li><p>The sum of the frequency distribution also needs to sum to <span class="math">\(2^{\text{ANS quant bits}}\)</span>.
Additionally, I wanted to ensure that no bin had zero probability, or else if
you try to encode it, ANS gets super confused.  To pull this off, I just
multiplied each bin's probability by <span class="math">\(2^{\text{ANS quant bits}}\)</span>, added
one to each bin, then calculate any excess I have beyond <span class="math">\(2^{\text{ANS quant bits}}\)</span>
and shave it off the largest frequency bin.  This is obviously
not an optimal way to do it.  I do wonder if that's why I got results that were worse
than the paper, but I didn't spend too much time checking.</p></li>
<li><p>Again, I had to be careful not to implicitly convert some of the integer values to floats.
So in some places, I do some explicit casting of <cite>astype(np.uint64)</cite> so the values don't
get all mixed up when I send them into ANS.</p></li>
</ul>
<p><br></p>
<h4> Experiments </h4>
<p>My compression results for MNIST (regular, non-binarized) are shown in Table 1.
My implementation can achieve 1.94 bits/pixel (i.e. on average a pixel uses
1.94 bits to represent it) vs. other implementations of 1.4 or less.  Very
unimpressive if you ask me.  I wasn't really able to get close to the
implementation in [1].  Didn't really try too hard to make it work but I was
hoping that I would be able to at least beat the standard compressors (<cite>bz2</cite>
and <cite>gzip</cite>), unfortunately that didn't happen either.</p>
<table class="colwidths-given align-center">
<caption>Table 1: Compression Rates for MNIST (bits/pixel)</caption>
<colgroup>
<col style="width: 60%">
<col style="width: 40%">
</colgroup>
<thead><tr>
<th class="head"><p>Compressor</p></th>
<th class="head"><p>Compression Rates (bits/pixel)</p></th>
</tr></thead>
<tbody>
<tr>
<td><p>My Implementation (Bits-Back w/ ANS)</p></td>
<td><p>1.94</p></td>
</tr>
<tr>
<td><p>Bits-Back w/ ANS [1]</p></td>
<td><p>1.41</p></td>
</tr>
<tr>
<td><p>Bits-Swap [2]</p></td>
<td><p>1.29</p></td>
</tr>
<tr>
<td><p>gzip</p></td>
<td><p>1.64</p></td>
</tr>
<tr>
<td><p>bz2</p></td>
<td><p>1.42</p></td>
</tr>
<tr>
<td><p>Uncompressed</p></td>
<td><p>8.00</p></td>
</tr>
</tbody>
</table>
<p>Interestingly [1] was using a simpler model (a single feed-forward layer for each of the encoder/decoder)
with a Beta-Binomial output distribution for each pixel.  This is obviously simpler than
my complex ResNet/multi-logistic method.  It's possible that I'm just not able to get a good fit with
my VAE model.  If you take a look in the notebook you'll see that the generated digits I can make
with the decoder look pretty bad.  So this is probably at least part of the reason why I was unable
to achieve good results.</p>
<p>The second reason is that I suspect my quantization isn't so great.  As mentioned above, I did so
funky rounding to ensure no zero-probability buckets, as well as an awkward way to discretize the
latent variables and data.  I suspect there are some differences from [1]'s
implementation (which is open source by the way) but I didn't spend too much
time trying to figure out the differences.</p>
<p>In any case, at least my implementation is able to <em>correctly</em> encode and decode and somewhat
approach the proper implementations.  As a toy implementation, I will make the bold assertion
that I coded it in a way that's  a bit more clear than [1]'s implementation so
maybe it's better for educational purposes?  I'll let you be the judge of that.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>So there you have it, a method for lossless compression using ML!  This mix of discrete
problems (e.g. compression) and ML is an incredibly interesting direction.  If
I get some time (and who knows when that will be), I'm definitely going to be
looking into some more of these topics.  But on this lossless compression topic, I'm
probably done for now.  There's another topic that I've been excited about recently
and have already started to go down that rabbit hole, so expect one (probably more)
posts on that subject.  Hope everyone is staying safe!</p>
<p><br></p>
<h4> References </h4>
<ul class="simple">
<li><p>Previous posts: <a class="reference external" href="../variational-autoencoders/">Variational Autoencoders</a>, <a class="reference external" href="../lossless-compression-with-asymmetric-numeral-systems/">Lossless Compression with Asymmetric Numeral Systems</a>, <a class="reference external" href="../the-expectation-maximization-algorithm/">Expectation Maximization Algorithm</a></p></li>
<li><p>My implementation on Github: <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/bitsback">notebooks</a></p></li>
<li><p>[1] "Practical Lossless Compression with Latent Variables using Bits Back Coding", Townsend, Bird, Barber, <a class="reference external" href="https://arxiv.org/abs/1901.04866">ICLR 2019</a>.</p></li>
<li><p>[2] "Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables", Kingma, Abbeel, Ho, <a class="reference external" href="https://arxiv.org/abs/1905.06845">ICML 2019</a></p></li>
</ul>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>I'll be using this term "pseudo-random" to describe this faux random sampling process.  I know it's probably not accurate to even call it pseudo-random, but I'm using it colloquially to mean not truly random.</p>
</dd>
</dl>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/asymmetric-numeral-systems/" rel="tag">asymmetric numeral systems</a></li>
            <li><a class="tag p-category" href="../../categories/bits-back/" rel="tag">Bits-Back</a></li>
            <li><a class="tag p-category" href="../../categories/compression/" rel="tag">compression</a></li>
            <li><a class="tag p-category" href="../../categories/lossless/" rel="tag">lossless</a></li>
            <li><a class="tag p-category" href="../../categories/mnist/" rel="tag">MNIST</a></li>
            <li><a class="tag p-category" href="../../categories/variational-autoencoder/" rel="tag">variational autoencoder</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../lossless-compression-with-asymmetric-numeral-systems/" rel="prev" title="Lossless Compression with Asymmetric Numeral Systems">Previous post</a>
            </li>
            <li class="next">
                <a href="../hamiltonian-monte-carlo/" rel="next" title="Hamiltonian Monte Carlo">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2022         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
