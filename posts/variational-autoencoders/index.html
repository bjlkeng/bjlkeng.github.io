<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A brief introduction into variational autoencoders.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Variational Autoencoders | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/variational-autoencoders/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../variational-bayes-and-the-mean-field-approximation/" title="Variational Bayes and The Mean-Field Approximation" type="text/html">
<link rel="next" href="../a-variational-autoencoder-on-the-svnh-dataset/" title="A Variational Autoencoder on the SVHN dataset" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Variational Autoencoders">
<meta property="og:url" content="http://bjlkeng.github.io/posts/variational-autoencoders/">
<meta property="og:description" content="A brief introduction into variational autoencoders.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-05-30T08:19:36-04:00">
<meta property="article:tag" content="autoencoders">
<meta property="article:tag" content="generative models">
<meta property="article:tag" content="Kullback-Leibler">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="variational calculus">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Variational Autoencoders</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2017-05-30T08:19:36-04:00" itemprop="datePublished" title="2017-05-30 08:19">2017-05-30 08:19</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This post is going to talk about an incredibly interesting unsupervised
learning method in machine learning called variational autoencoders.  It's main
claim to fame is in building generative models of complex distributions like
handwritten digits, faces, and image segments among others.  The really cool
thing about this topic is that it has firm roots in probability but uses a
function approximator (i.e.  neural networks) to approximate an otherwise
intractable problem.  As usual, I'll try to start with some background and
motivation, include a healthy does of math, and along the way try to convey
some of the intuition of why it works.  I've also annotated a
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb">basic example</a>
so you can see how the math relates to an actual implementation.  I based much
of this post on Carl Doersch's <a class="reference external" href="https://arxiv.org/abs/1606.05908">tutorial</a>,
which has a great explanation on this whole topic, so make sure you check that
out too.</p>
<!-- TEASER_END -->
<p><br></p>
<h4> 1. Generative Models  </h4>
<p>The first thing to discuss is the idea of a
<a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">generative model</a>.
A generative model is a model which allows you to sample (i.e. randomly
generate data points) from a distribution similar to your observed (i.e. training)
data.  We can accomplish this by specifying a joint distribution over
all the dimensions of the data (including the "y" labels).
This allows us to generate any number of data points that has similar
characteristics to our observed data.  This is in contrast to a
<a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative model</a>,
which can only model dependence between the outcome variable (<span class="math">\(y\)</span>) and
features (<span class="math">\(X\)</span>).  For example, a binary classifier only outputs 0 or 1 "y"
labels and cannot generate a data point that looks like your "X" features.</p>
<p>Typically, as part of your model you'll want to specify latent variables that
represent some higher level concepts.  This allows for intuitive relationships
between the latent variables and the observed ones, while usually simplifying
the overall number of parameters (i.e. complexity) of the model.  We'll be
focusing on the application of generating new values that look like the
observed data but check out the
<a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">Wikipedia</a>
article for a better picture of some other applications.</p>
<div class="admonition admonition-example-1-generative-models">
<p class="admonition-title">Example 1: Generative Models</p>
<ol class="arabic">
<li><dl>
<dt>
<strong>Normal Distribution</strong> for modelling human heights:</dt>
<dd>
<p>Although generative models are usually only talked about in the context
of complex latent variable models, technically, a simple probability
distribution is a also generative model.  In this example, if we have
the height of different people (<span class="math">\(x\)</span>) as our 1-dimensional
observations, we can use a normal distribution as our generative model:</p>
<div class="math">
\begin{align*}
x &amp;\sim \mathcal{N}(\mu, \sigma^{2}) \\
\tag{1}
\end{align*}
</div>
<p>With this simple model, we can "generate" heights that look like our
observations by sampling the normal distribution (after fitting the
<span class="math">\(\mu\)</span> and <span class="math">\(\sigma^2\)</span> parameters).  Any data point we sample
from this distribution could plausibly be another observation we had
(assuming our model was a good fit for the data).</p>
</dd>
</dl></li>
<li><dl>
<dt>
<strong>Gaussian Mixture Model</strong> for modelling prices of different houses:</dt>
<dd>
<p>Let's suppose we have <span class="math">\(N\)</span> observations of housing prices in a
city.  We hypothesize that within a particular neighbourhood, house
prices tend to cluster around the neighbourhood mean.  Thus, we can
model this situation using a Gaussian mixture model as such:</p>
<div class="math">
\begin{align*}
p(x_i|\theta) &amp;=  \sum_{k=1}^K p(z_i=k) p(x_i| z_i=k, \mu_k, \sigma_k^2)  \\
x_i| z_i &amp;\sim \mathcal{N}(\mu_k, \sigma_k^2) \\
z_i &amp;\sim \text{Categorical}(\pi) \tag{2}
\end{align*}
</div>
<p>where <span class="math">\(z_i\)</span> is a categorical variable for a given neighbourhood,
<span class="math">\(x_i|z_i\)</span> is a normal distribution for the prices within a given
neighbourhood <span class="math">\(z_i=k\)</span>, and <span class="math">\(x_i\)</span> will be a Gaussian mixture of
each of the component neighbourhoods.</p>
<p>Using this model, we could then generate several different types of
observations.  If we wanted to generate a house of a particular
neighbourhood, we could sample the normal distribution from
<span class="math">\(x_i|z_i\)</span>.  If we wanted to sample the "average" house, we could
sample a price from each neighbourhood, and then compute their weighted
average in proportion to the distribution of the categorical variable
<span class="math">\(z_i\)</span>.</p>
<p>This more complex model is not as straightforward to fit.  A common
method is to use
<a class="reference external" href="../the-expectation-maximization-algorithm/">the expectation-maximization algorithm</a> or something similar such as variational inference.</p>
</dd>
</dl></li>
<li><dl>
<dt>
<strong>Handwritten digits</strong>:</dt>
<dd>
<p>A more modern application of generative models is for hand written
digits.  The <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database</a> is a large dataset of
handwritten digits typically used as a benchmark for machine learning
techniques.  Many recent techniques have shown good performance in
generating new samples of hand written digits.  Two of the most popular
approaches are variational autoencoders (the topic of this post) and
<a class="reference external" href="https://en.wikipedia.org/wiki/Generative_adversarial_networks">generative adversarial networks</a>.</p>
<p>In these types of approaches, the generative model is trained on the
tens of thousands of examples of 28x28 greyscale images, each
representing a single "0" to "9" digit.  Once trained, the model should
be able to reproduce random "0" to "9" 28x28 greyscale digits that, if
trained well, look like a hand written digit.</p>
</dd>
</dl></li>
</ol>
</div>
<p><br></p>
<h4> 2. An Implicit Generative Model (aka the "decoder") </h4>
<p>Let's continue to use this handwritten digit as our motivation
for generative models.  Generating a 28x28 greyscale image that looks like a digit
is non-trivial, especially if we are trying to model it directly.  The joint
distribution over 28x28 random variables is going to be complex, for example,
enforcing that "0"s have empty space near the middle but "1"s don't, is not
an easy thing to do.  Typically in these situations, we'll introduce latent variables
which encode higher level ideas.  In our example, one of the latent variables
might correspond to which digit we're using (0-9), another one may be the
stroke width we use, etc.  This model is simpler because there are
usually fewer parameters to estimate, reducing the number of data points
required for a good fit.  (See my post on
<a class="reference external" href="../the-expectation-maximization-algorithm/">the expectation-maximization algorithm</a>,
which has a brief description of latent variable models in the background section)</p>
<p>One of the downsides of any latent variable model is that you have to specify the
model! That is, you have to have some idea of what latent variables you want
to include, how these variables are related to each other and the observed variables,
and finally how to fit the model (which depends on the connectivity).
All of these issues introduce potential for a misspecification of the model.  For
example, maybe you forgot to include stroke width and now all your handwritten
digits are blurry because it averaged over all the types of stroke widths in your
training dataset.  Wouldn't it be nice if you didn't need to explicitly
specify the latent variables (and associated distributions), nor the
relationships between them, and on top of all of this had an easy way to fit
the model?
Enter variational autoencoders.</p>
<p></p>
<h5> 2.1 From a Standard Normal Distributions to a Complex Latent Variable Model </h5>
<p>There are a couple of big ideas here that allow us to create this implicit model
without explicitly specifying anything.
The first big idea here is that we're not going to explicitly define any
latent variables, that is, we won't say "this variable is for digit 0", "this one for digit 1",
...,  "this variable is for stroke width", etc.  Instead, we'll have our latent variables
as a simple uninterpretable standard <a class="reference external" href="https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic">isotropic</a>
multivariate normal distribution <span class="math">\(\mathcal{N}(0, I)\)</span> where <span class="math">\(I\)</span> is
the identify matrix.  You may be wondering how we can ever model anything
complex if we just use a normal distribution?  This leads us to the next big
idea.</p>
<p>The second big idea is that starting from any random variable <span class="math">\(Z\)</span>, there
exists a <em>deterministic</em> function <span class="math">\(Y=g(Z)\)</span> (under most conditions) such
that <span class="math">\(Y\)</span> can be any complex target distribution you want (see the box on "Inverse
Transform Sampling" below).  <em>The ingenious idea here is that we can learn</em>
<span class="math">\(g(\cdot)\)</span> <em>from the data</em>!  Thus, our variational autoencoder can
transform our boring, old normal distribution into any funky shaped
distribution we want!  As you may have already guessed, we use a neural network
as a function approximator to learn <span class="math">\(g(\cdot)\)</span>.</p>
<p>The last little bit in defining our latent variable model is translating our
latent variable into the final distribution of our observed data.  Here,
we'll also use something simple: we'll assume that the observed data
follows a isotropic normal distribution <span class="math">\(\mathcal{N}(g(z), \sigma^2 * I)\)</span>, with
mean following our learned latent random variable from the output of <span class="math">\(g\)</span>,
and identity covariance matrix scaled by a hyperparameter <span class="math">\(\sigma^2\)</span>.</p>
<p>The reason why we want to put a distribution on the output is that we want
to say that our output is <em>like</em> our observed data -- not exactly equal.
Remember, we're using a probabilistic interpretation here, so we need to write
a likelihood function and then maximize it, usually by taking its gradient.
If we didn't have an output distribution, we would implicitly be saying that
<span class="math">\(g(z)\)</span> was exactly equal i.e. a Dirac delta function, which would result
in a discontinuity.  This is important because we will eventually want to use
stochastic gradient descent to learn <span class="math">\(g(z)\)</span> and this implicitly requires
a smooth function.  We'll see how this probabilistic interpretation plays into
the loss/objective function below.</p>
<div class="admonition admonition-inverse-transform-sampling">
<p class="admonition-title">Inverse Transform Sampling</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a>
is a method for sampling from any distribution given its cumulative
distribution function (CDF), <span class="math">\(F(x)\)</span>.
For a given distribution with CDF <span class="math">\(F(x)\)</span>, it works as such:</p>
<ol class="arabic simple">
<li><p>Sample a value, <span class="math">\(u\)</span>, between <span class="math">\([0,1]\)</span> from a uniform
distribution.</p></li>
<li><p>Define the inverse of the CDF as <span class="math">\(F^{-1}(u)\)</span> (the domain is a
probability value between <span class="math">\([0,1]\)</span>).</p></li>
<li><p><span class="math">\(F^{-1}(u)\)</span> is a sample from your target distribution.</p></li>
</ol>
<p>Of course, this method has no claims on being efficient.  For example,
on continuous distributions, we would need to be able to find the inverse
of the CDF (or some close approximation), which is not at all trivial.
Typically, there are more efficient ways to perform sampling on any
particular distribution but this provides a theoretical way to
sample from <em>any</em> distribution.</p>
<p><strong>Proof</strong></p>
<p>The proof of correctness is actually pretty simple.  Let <span class="math">\(U\)</span>
be a uniform random variable on <span class="math">\([0,1]\)</span>, and <span class="math">\(F^{-1}\)</span>
as before, then we have:</p>
<div class="math">
\begin{align*}
&amp;P(F^{-1}(U) \leq x) \\
&amp;= P(U \leq F(x)) &amp;&amp; \text{apply } F \text{ to both sides} \\
&amp;= F(x)  &amp;&amp; \text{because } P(U\leq y) = y \text{ on } [0,1] \\
\tag{3}
\end{align*}
</div>
<p>Thus, we have shown that <span class="math">\(F^{-1}(U)\)</span> has the distribution
of our target random variable (since the CDF <span class="math">\(F(x)\)</span> is the same).</p>
<p>It's important to note what we did: we took an easy to sample random
variable <span class="math">\(U\)</span>, performed a <em>deterministic</em> transformation
<span class="math">\(F^{-1}(U)\)</span> and ended up with a random variable that was distributed
according to our target distribution.</p>
<p><strong>Example</strong></p>
<p>As a simple example, we can try to generate a exponential distribution
with CDF of <span class="math">\(F(x) = 1 - e^{-\lambda x}\)</span> for <span class="math">\(x \geq 0\)</span>.
The inverse is defined by <span class="math">\(x = F^{-1}(u) = -\frac{1}{\lambda}\log(1-y)\)</span>.
Thus, we can sample from an exponential distribution just by iteratively
evaluating this expression with a uniform randomly distributed number.</p>
<div class="figure align-center">
<img alt="Visualization of mapping between a uniform distribution and an exponential one (source: Wikipedia)" src="../../images/Inverse_transformation_method_for_exponential_distribution.jpg" style="height: 300px;"><p class="caption">Figure 1: The <span class="math">\(y\)</span> axis is our uniform random distribution and the <span class="math">\(x\)</span> axis is our exponentially distributed number.  You can see for each point on the <span class="math">\(y\)</span> axis, we can map it to a point on the <span class="math">\(x\)</span> axis.  Even though <span class="math">\(y\)</span> is distributed uniformly, their mapping is concentrated on values closer to <span class="math">\(0\)</span> on the <span class="math">\(x\)</span> axis, matching an exponential distribution (source: Wikipedia).</p>
</div>
<p><strong>Extensions</strong></p>
<p>Now instead of starting from a uniform distribution, what happens if we
want to sample from another distribution, say a normal distribution?
We just first apply the reverse of the inverse sampling transform
called the
<a class="reference external" href="https://en.wikipedia.org/wiki/Probability_integral_transform">Probability Integral Transform</a>.
So the steps would be:</p>
<ol class="arabic simple">
<li><p>Sample from a normal distribution.</p></li>
<li><p>Apply the probability integral transform using the CDF of a normal
distribution to get a uniformly distributed sample.</p></li>
<li><p>Apply inverse transform sampling with the inverse CDF of the target
distribution to get a sample from our target distribution.</p></li>
</ol>
<p>What about extending to multiple dimensions?  We can just break up the
joint distribution into its conditional components and sample each
sequentially to construct the overall sample:</p>
<div class="math">
\begin{equation*}
P(x_1,\ldots, x_n) = P(x_n|x_{n-1}, \ldots,x_1)\ldots P(x_2|x_1)P(x_1) \tag{4}
\end{equation*}
</div>
<p>In detail, first sample <span class="math">\(x_1\)</span> using the method above, then <span class="math">\(x_2|x_1\)</span>,
then <span class="math">\(x_3|x_2,x_1\)</span>, and so on.  Of course, this implicitly means you
would have the CDF of each of those distributions available, which
practically might not be possible.</p>
</div>
<div class="figure align-center">
<img alt="Variational Autoencoder Graphical Model" src="../../images/variational_autoencoder-decoder.png" style="height: 400px;"><p class="caption">Figure 2: A graphical model of a typical variational autoencoder (without a "encoder", just the "decoder"). We're using a modified plate notation: the circles represent variables/parameters, rectangular boxes with a number in the lower right corner to represent multiple instances of the contained variables, and the little diagram in the middle is a representation of a deterministic neural network (function approximator).</p>
</div>
<p><br></p>
<p>Writing the model out more formally, we have:</p>
<div class="math">
\begin{align*}
X_i &amp;\sim \mathcal{N}(\mu_i, \sigma^2 * I) &amp;&amp;&amp; \text{Observed variable} \\
\mu_i &amp;\sim g(Z_{1,\ldots,K}; \theta) &amp;&amp;&amp; \text{Implicit latent variable}\\
Z_k &amp;\sim \mathcal{N}(0, I)
\tag{5}
\end{align*}
</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math">\(X_{i=1,\ldots,N}\)</span> are our normally distributed observations</p></li>
<li><p><span class="math">\(\mu_{i=1,\ldots,N}\)</span> are the mean of our observed variables which potentially has a very complex distribution</p></li>
<li><p><span class="math">\(\sigma^2\)</span> is a hyperparameter</p></li>
<li><p><span class="math">\(I\)</span> is the identify matrix</p></li>
<li><p><span class="math">\(g(Z_{1,\ldots,K}; \theta)\)</span> is a deterministic function with parameters to be learned <span class="math">\(\theta\)</span> (e.g. weights of a neural network)</p></li>
<li><p><span class="math">\(Z_{i=1,\ldots,K}\)</span> are standard isotropic normally distributed random variables</p></li>
</ul>
<p>Note: we can put another distribution on <span class="math">\(X\)</span> like a Bernoulli for binary
data parameterized by <span class="math">\(p=g(z;\theta)\)</span>.  The important part is we're able to
maximize the likelihood over the <span class="math">\(\theta\)</span> parameters.  Implicitly, we
will want our output variable to be continuous in <span class="math">\(\theta\)</span> so we can
take its gradient.</p>
<p></p>
<h5> 2.2 A hard fit </h5>
<p>Given the model above, we have all we need to fit the model: observations from
<span class="math">\(X\)</span>, fully defined latent variables (<span class="math">\(Z\)</span>), and a
function approximator <span class="math">\(g(\cdot)\)</span>; the only thing we need to find is
<span class="math">\(\theta\)</span>.  This is a classic optimization problem where we could use an
MLE (or MAP) estimate.  Let's see how this works out.</p>
<p>First, we need to define the probability of seeing a single example <span class="math">\(x\)</span>:</p>
<div class="math">
\begin{align*}
P(X=x) &amp;= \int p(X=x,Z=z) dz \\
       &amp;= \int p(X=x|z;\theta)p(z) dz \\
&amp;= \int p_{\mathcal{N}}(x;g(z;\theta),\sigma^2*I)
        p_{\mathcal{N}}(z;0,I) dz \\
&amp;\approx \frac{1}{M} \sum_{m=1}^M p_{\mathcal{N}}(x;g(z_m;\theta),\sigma^2*I)
&amp;&amp;&amp; \text{where } z_m \sim \mathcal{N}(0,I) \\
\tag{6}
\end{align*}
</div>
<p>The probability of a single sample is just the joint probability of our given
model marginalizing (i.e. integrating) out <span class="math">\(Z\)</span>.  Since we don't have an
analytical form of the density, we approximate the integral by averaging over
<span class="math">\(M\)</span> samples from <span class="math">\(Z\sim \mathcal{N}(0, I)\)</span>.</p>
<p>Putting together the log-likelihood (defined by log'ing the density and summing over all
of our <span class="math">\(N\)</span> observations):</p>
<div class="math">
\begin{equation*}
\log P(X) \approx \sum_{i=1}^N
       \log(\frac{1}{M} \sum_{m=1}^M p_{\mathcal{N}}(x_i;g(z_m;\theta),\sigma^2*I)) \tag{7}
\end{equation*}
</div>
<p>Two problems here. First, the <span class="math">\(\log\)</span> can't be pushed inside the
summation, which actually isn't much a problem because we're not trying to
derive an analytical expression here; so long as we can use gradient descent
to learn <span class="math">\(\theta\)</span>, we're good.  In this case, we can easily take derivatives
since our density is normally distributed.</p>
<p>The other big problem, that is not easily seen through the notation, is that
<span class="math">\(z_m\)</span> is actually a <span class="math">\(K\)</span>-dimensional vector.  In order to approximate
the integral properly, we would have to sample over a <em>huge</em> number of
<span class="math">\(z\)</span> values for <em>each</em> <span class="math">\(x_i\)</span> sample!  This basically plays into the
<a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>
whereby each additional dimension of <span class="math">\(z\)</span> exponentially increases the
number of samples you need to properly approximate the volume of the space.
For small <span class="math">\(K\)</span> this might be feasible but any reasonable value, it will be
intractable.</p>
<p>Looking at it from another point of view, the reason why this is intractable is because
it's inefficient; for each <span class="math">\(x\)</span>, we have to average over a large number
(<span class="math">\(M\)</span>) samples.  But, for any given observation <span class="math">\(x\)</span>, most of the
<span class="math">\(z_m\)</span> will contribute very little to the likelihood.
Using the handwritten digit example, if we're trying to generate a "0", most of
the values of <span class="math">\(z_m\)</span> sampled from our prior will have a very small
probability of generating a "0", so we're wasting a lot of computation in
trying to average over all these samples <a class="footnote-reference brackets" href="#id4" id="id1">1</a>.</p>
<p>Wouldn't it be nice if we could just sample from <span class="math">\(p(z|X=x_i)\)</span> directly
and only pick <span class="math">\(z\)</span> values that contribute a significant amount to the
likelihood, thus getting rid the need for the large inefficient summation over
<span class="math">\(M\)</span> samples?  This is exactly what variational autoencoders proposes!</p>
<p>(Note: Just to be clear, each <span class="math">\(x_i\)</span> will likely have a <em>different</em>
<span class="math">\(p(z|X=x_i)\)</span>.  Imagine our hand written digit example, a "1" will
probably have a very different posterior shape than an "8".)</p>
<p></p>
<h5> 2.2 Summary </h5>
<p>To summarize, this is what we're trying to accomplish:</p>
<ul class="simple">
<li><p>Our generative model is an implicit latent variable model with latent
variables <span class="math">\(Z\)</span> as standard isotropic multivariate normal distribution.</p></li>
<li><p>The <span class="math">\(Z\)</span> variables are transformed into an arbitrarily complex
distribution by a deterministic function approximator (e.g. neural
network parameterized by <span class="math">\(\theta\)</span>) that can model our data.</p></li>
<li><p>We can fit our generative model via the likelihood by averaging over a huge
number of <span class="math">\(Z\)</span> samples; this becomes intractable for higher dimensions
(curse of dimensionality).</p></li>
<li><p>For any given sample <span class="math">\(x_i\)</span>, most of the <span class="math">\(z\)</span> samples will
contribute very little to the likelihood calculation, so we wish to sample
only the probable values of <span class="math">\(z_m\)</span> that contribute significantly to
the likelihood using the posterior distribution <span class="math">\(z|X=x_i\)</span>.</p></li>
</ul>
<p>From here, we can finally get to the "variational" part of variational
autoencoders.</p>
<p><br></p>
<h4> 3. Variational Bayes for the Posterior (aka the "encoder") </h4>
<p>From our novel idea of an implicit generative model, we come to a new problem:
how can we estimate the posterior distribution <span class="math">\(Z|X=x_i\)</span>? We have a couple of
problems, first, the posterior probably has no closed form analytic solution.
This is not terrible because this is a typical problem in Bayesian inference
which we solve via either <a class="reference external" href="../markov-chain-monte-carlo-mcmc-and-the-metropolis-hastings-algorithm/">Markov Chain Monte Carlo Methods</a> or
<a class="reference external" href="../variational-bayes-and-the-mean-field-approximation/">Variational Bayes</a>.
Second, we wanted to use the posterior <span class="math">\(Z|X=x_i\)</span> to maximize our
likelihood function <span class="math">\(P(X|Z;\theta)\)</span>,
but surely to find <span class="math">\(Z|X=x_i\)</span> we need to know <span class="math">\(X|Z\)</span> --
a circular dependence.  The solution to this is also novel, let's
<em>simultaneously</em>, fit our posterior, generate samples from it, <em>and</em> maximize
our original log-likelihood function!</p>
<p>First, we'll attempt to solve the first problem of finding the posterior
<span class="math">\(P(z|X=x_i)\)</span> and this will lead us to the solution to the second problem
of fitting our likelihood.  Let's dig into some math to see how this works.</p>
<p></p>
<h5> 3.1 Setting up the Variational Objective </h5>
<p>Since the posterior is so complex, we won't try to model it exactly.  Instead,
we'll use variational Bayesian methods (see my
<a class="reference external" href="../variational-bayes-and-the-mean-field-approximation/">previous post</a>)
to approximate it.  We'll denote our approximate distribution as
<span class="math">\(Q(Z|X)\)</span> and, as usual, we'll use
<a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>
as our measure of "closeness" to the actual distribution.
Writing the math down, we get:</p>
<div class="math">
\begin{align*}
\mathcal{D}(Q(z|X) || P(z|X)) &amp;= \int Q(z|X) \log\big[\frac{Q(z|X)}{P(z|X)}\big] dz \\
&amp;= E_{z\sim Q}[\log Q(z|X) - \log P(z|X)] \\
&amp;= E_{z\sim Q}[\log Q(z|X) + \log P(X) - \log P(X|z) - \log P(z)] &amp;&amp;&amp; \text{Bayes rule} \\
&amp;= E_{z\sim Q}[\log Q(z|X) - \log P(X|z) - \log P(z)] + \log P(X)
\tag{8}
\end{align*}
</div>
<p>We pulled <span class="math">\(P(X)\)</span> out of the expectation since it is not dependent on
<span class="math">\(z\)</span>.  Rearranging Equation 8:</p>
<div class="math">
\begin{equation*}
\log P(X) - \mathcal{D}(Q(z|X) || P(z|X)) =
    E_{z\sim Q}[\log P(X|z)] - \mathcal{D}(Q(z|X) || P(z)) \tag{9}
\end{equation*}
</div>
<p>where we use the definition of KL divergence on the RHS.
This is the core objective of variational autoencoders for which we wish to
maximize.  The LHS defines our higher level goal:</p>
<ul class="simple">
<li><p><span class="math">\(\log P(X)\)</span>: maximize the log-likelihood (Equation 7)</p></li>
<li><p><span class="math">\(\mathcal{D}(Q(z|X) || P(z|X))\)</span>: Minimize the KL divergence between our
approximate posterior <span class="math">\(Q(z|X)\)</span> to fit <span class="math">\(P(z|X)\)</span></p></li>
</ul>
<p>The RHS gives us an explicit objective where we know all the pieces, and we can
maximize via gradient descent (details in the next section):</p>
<ul class="simple">
<li><p><span class="math">\(P(X|z)\)</span>: original implicit generative model</p></li>
<li><p><span class="math">\(Q(z|X)\)</span>: approximate posterior (we'll define what this looks like below)</p></li>
<li><p><span class="math">\(P(z)\)</span>: prior distribution of latent variables (standard isotropic normal distribution)</p></li>
</ul>
<p>Notice, we now have what appears to be an "autoencoder".  <span class="math">\(Q(z|X)\)</span>
"encodes" <span class="math">\(X\)</span> to latent representation <span class="math">\(z\)</span>, and <span class="math">\(P(X|z)\)</span>
"decodes" <span class="math">\(z\)</span> to reconstruct <span class="math">\(X\)</span>.  We'll see how these equations
translate into a model that we can directly optimize.</p>
<p></p>
<h5> 3.2 Defining the Variational Autoencoder Model </h5>
<p>Now that we have a theoretical framework that gives us an objective to optimize,
we need to explicitly define <span class="math">\(Q(z|X)\)</span>.  In the same way we implicitly
defined the generative model, we'll use the same idea to define the approximate
posterior.  We'll assume that the <cite>Q(z|X)</cite> is normally distributed with
mean and co-variance matrix defined by a neural network with parameters
<span class="math">\(\phi\)</span>.  The co-variance matrix is usually constrained to be diagonal to
simplify things a bit.  Formally, we'll have:</p>
<div class="math">
\begin{align*}
z|X &amp;\approx \mathcal{N}(\mu_{z|X}, \Sigma_{z|X}) \\
\mu_{z|X}, \Sigma_{z|X} &amp;= g_{z|X}(X; \phi) \\
\tag{10}
\end{align*}
</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math">\(z|X\)</span> is our approximated posterior distribution as a multivariate
normal distribution</p></li>
<li><p><span class="math">\(\mu_{z|X}\)</span> is a vector of means for our normal distribution</p></li>
<li><p><span class="math">\(\Sigma_{z|X}\)</span> is a diagonal co-variance matrix for our normal distribution</p></li>
<li><p><span class="math">\(g_{z|X}\)</span> is our function approximator (neural network)</p></li>
<li><p><span class="math">\(\phi\)</span> are the parameters to <span class="math">\(g_{z|X}\)</span></p></li>
</ul>
<p>As a first attempt, we might try to put Equations 5 and 10 to form a model
as shown in Figure 3.  The red boxes show our variational loss (objective)
function from Equation 9 (Note: the squared error comes from the <span class="math">\(\log\)</span>
of the normal distribution and <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Kullback.E2.80.93Leibler_divergence_for_multivariate_normal_distributions">KL divergence between two multivariate normals</a>
is well known); the input and the sampling of the <span class="math">\(z\)</span> values
is shown in blue.</p>
<div class="figure align-center">
<img alt="Variational Autoencoder Diagram" src="../../images/variational_autoencoder2.png" style="width: 550px;"><p class="caption">Figure 3: An initial attempt at a variational autoencoder without
the "reparameterization trick".  Objective functions shown in red.
We cannot back-propagate through the stochastic sampling operation
because it is not a continuous deterministic function.</p>
</div>
<p>Using this model, we can perform a "forward pass":</p>
<ol class="arabic simple">
<li><p>Inputting values of <span class="math">\(X=x_i\)</span></p></li>
<li><p>Computing <span class="math">\(\mu_{z|X}\)</span> and <span class="math">\(\Sigma_{z|X}\)</span> from <span class="math">\(g_{z|X}(X;\phi)\)</span></p></li>
<li><p>Sample a <span class="math">\(z\)</span> value from <span class="math">\(\mathcal{N}(\mu_{z|X}, \Sigma_{z|X})\)</span></p></li>
<li><p>Compute <span class="math">\(\mu_{X|z}\)</span> from <span class="math">\(g_{X|z}(z;\theta)\)</span> to produce the
(mean of the) reconstructed output.</p></li>
</ol>
<p>Remember, our goal is to learn the parameters for our two networks: <span class="math">\(\theta\)</span>
and <span class="math">\(\phi\)</span>.  We can attempt to do this through back-propagation but
we hit a road-block when we back-propagate the <span class="math">\(||X-\mu_{X|z}||^2\)</span> error
through the sampling operation.  Since the sampling operation isn't a
continuous deterministic function, it has no gradient, thus we can't do any
form of back-propagation.  Fortunately, we can use the "reparameterization
trick" to circumvent this sampling problem.  This is shown in Figure 4.</p>
<div class="figure align-center">
<img alt="Variational Autoencoder Diagram" src="../../images/variational_autoencoder3.png" style="width: 550px;"><p class="caption">Figure 4: A variational autoencoder with the "reparameterization trick".
Notice that all operations between the inputs and objectives are
continuous deterministic functions, allowing back-propagation to occur.</p>
</div>
<p>The key insight is that <span class="math">\(\mathcal{N}(\mu_{z|X}, \Sigma_{z|X})\)</span>
is equivalent to <span class="math">\(\mathcal{N}(0, I) * \Sigma_{z|X} + \mu_{z|X}\)</span>.
That is, we can just sample from an standard isotropic normal distribution and then
scale and shift our sample to transform it to a sample from our desired
<span class="math">\(z|X\)</span> distribution.  Shifting the sampling operation to an input
of our model (for each sample <span class="math">\(x_i\)</span>, we can sample an arbitrary
number of <span class="math">\(\mathcal{N}(0,I)\)</span> samples to pair with it).</p>
<p>With the "reparameterization trick", this new model allows us to
take the gradient of the loss function with respect to the target parameters
(<span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span>).  In particular, we would want to take
these gradients:</p>
<div class="math">
\begin{align*}
&amp;\frac{\partial}{\partial\theta}(X-\mu_{X|z})^2 \\
&amp;\frac{\partial}{\partial\phi}(X-\mu_{X|z})^2 \\
&amp;\frac{\partial}{\partial\phi} \mathcal{D}(Q(z|X) || P(z)) \\
\tag{11}
\end{align*}
</div>
<p>and iteratively update the weights of our neural network using
back-propagation.  As seen in Figure 4, <span class="math">\(Q(z|X) = N(\mu_{z|X},
\Sigma_{z|X})\)</span> is parameterized by <span class="math">\(\phi\)</span> via <span class="math">\(g_{z|X}(X;\phi)\)</span>,
whereas <span class="math">\(\mu_{X|z}\)</span> is implicitly parameterized by both <span class="math">\(\theta\)</span>
through via <span class="math">\(g_{X|z}(z;\theta)\)</span>, and <span class="math">\(\phi\)</span> through
<span class="math">\(z\)</span> and <span class="math">\(g_{z|X}(X;\phi)\)</span>.</p>
<p></p>
<h5> 3.3 Training a Variational Autoencoder </h5>
<p>Now that we have the basic structure of our network and understand the relationship
between the "encoder" and "decoder", let's figure out how to train this sucker.
Recall Equation 9 shows us how define our objective in terms of distributions:
<span class="math">\(z, z|X, X|z\)</span>.  When we're optimizing, we don't directly work with
distributions, instead we have samples and a single objective function.
We can translate Equation 9 into this goal by taking the expectation over
the relevant variables like so:</p>
<div class="math">
\begin{align*}
E_{X\sim D}[\log P(X) - \mathcal{D}(Q(z|X) || P(z|X))] =
    E_{X\sim D}[E_{z\sim Q}[\log P(X|z)] - \mathcal{D}(Q(z|X) || P(z))]  \\
\tag{12}
\end{align*}
</div>
<p>To be a bit pedantic, we still don't have a function we can't optimize directly
because we don't know how to take the expectation.  Of course, we'll just
make the usual approximations by replacing the expectation with summations.
I'll show it step by step, starting from the RHS of Equation 12:</p>
<div class="math">
\begin{align*}
&amp;E_{X\sim D}[E_{z\sim Q}[\log P(X|z)] - \mathcal{D}(Q(z|X) || P(z))] \\
&amp;= E_{X\sim D}[E_{\epsilon \sim \mathcal{N}(0, I)}[
    \log P(X|z=\mu_{z|X}(X) + \Sigma_{z|X}^{1/2}(X)*\epsilon)
] - \mathcal{D}(Q(z|X) || P(z))]  \\
&amp;\approx \frac{1}{N}\sum_{x_i \in X}
   E_{\epsilon \sim \mathcal{N}(0, I)}[
    \log P(x_i|z=\mu_{z|X}(x_i) + \Sigma_{z|X}^{1/2}(x_i)*\epsilon)
   ] - \mathcal{D}(Q(z|x_i) || P(z))  \\
&amp;\approx \frac{1}{N}\sum_{x_i \in X}
    \log P(x_i|z=\mu_{z|X}(x_i) + \Sigma_{z|X}^{1/2}(x_i)*\epsilon)
   - \mathcal{D}(Q(z|x_i) || P(z))  \\
\end{align*}
</div>
<div class="math">
\begin{equation*}
\tag{13}
\end{equation*}
</div>
<p>Going line by line: first, we use our "reparameterization trick" by just
sampling from our isotropic normal distribution instead of our prior <span class="math">\(z\)</span>.
Next, let's approximate the outer expectation by taking our N observations of
the <span class="math">\(X\)</span> values and averaging them.  This is what we implicitly do in most
learning algorithms when we assume i.i.d. Finally, we'll simplify the inner
expectation by just using a single sample paired with each observation
<span class="math">\(x_i\)</span>.  This requires a bit more explanation.</p>
<p>We want to simplify the inner expectation <span class="math">\(E_{\epsilon \sim
\mathcal{N}(0, I)}[\cdot]\)</span>.  To compute this, each time we evaluate the network, we
must explicitly sample a <em>new</em> value of <span class="math">\(\epsilon\)</span> from our isotropic
normal distribution.  That means we can just pair each observation
<span class="math">\(x_i\)</span> with a bunch of samples from <span class="math">\(\mathcal{N}(0, I)\)</span>
to make a "full input".</p>
<p>However, instead of doing that explicitly, let's just pair each <span class="math">\(x_i\)</span>
with a single sample from <span class="math">\(\mathcal{N}(0, I)\)</span>.  If we're training over
many epochs (large loop over all <span class="math">\(x_i\)</span> values), it's as if we are pairing
each observation with a bunch of new values sampled from <span class="math">\(\mathcal{N}(0,
I)\)</span>.
According to stochastic gradient descent theory, these two methods should
converge to the same place and it simplifies life for us a bit.</p>
<p>As a final note, we can simplify the last line in Equation 13 to something
like:</p>
<div class="math">
\begin{equation*}
\frac{1}{N} \sum_{x_i \in X} -\frac{1}{2\sigma^2}(x_i-\mu_{X|z})^2 - \frac{1}{2}\big(tr(\Sigma_{z|X}(x_i)) + (\mu_{z|X}(x_i))^T(\mu_{z|X}(x_i)) - k - \log \text{det}(\Sigma_{z|X}(x_i))\big) \tag{14}
\end{equation*}
</div>
<p>Each of the two big terms corresponds to <span class="math">\(\log P(x_i|z)\)</span> and
<span class="math">\(\mathcal{D}(Q(z|x_i) || P(z)\)</span> respectively, where I dropped out some of
the constants from <span class="math">\(\log P(x_i|z)\)</span> (since they're not needed in the gradient)
and used the formula for <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Kullback.E2.80.93Leibler_divergence_for_multivariate_normal_distributions">KL divergence between two multivariate normals</a>.</p>
<p>And now that we have a concrete formula for the overall objective, it's straight
forward to perform stochastic gradient descent either by manually working the
derivatives or using some form of automatic differentiation.</p>
<p></p>
<h5> 3.4 Generating New Samples </h5>
<p>Finally, we get to the interesting part of our model.  After all that training
we can finally try to generate some new observations using our implicit
generative model.  Even though we did all that work with the
"reparameterization trick" and KL divergence, we still only need our implicit
generative model from Section 2.1.</p>
<div class="figure align-center">
<img alt='Variational "Decoder" Diagram' src="../../images/variational_autoencoder4.png" style="width: 250px;"><p class="caption">Figure 5: The generative model component of a variational autoencoder
in test mode.</p>
</div>
<p>Figure 5 shows our generative model.  To generate a new observation, all
we have to do is sample from our isotropic normal distribution (our prior
for our latent variables), and then run it through our neural network.
The network should have learned how to transform our latent variables into
the mean of what our training data looks like <a class="footnote-reference brackets" href="#id5" id="id2">2</a>.</p>
<p>Note, that our network now only outputs the mean of our generative output,
we can additionally sample from our actual output distribution if we sample a
normal distribution with this mean.  In most cases, the mean is probably what
we want though (e.g. when generating an image, the mean values are just the
values for the pixels).</p>
<p></p>
<h5> 4. A Basic Example: MNIST Variational Autoencoder </h5>
<p>The nice thing about many of these modern ML techniques is that implementations are
widely available.  I put together a
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb">notebook</a>
that uses <a class="reference external" href="https://keras.io/">Keras</a> to build a variational autoencoder <a class="footnote-reference brackets" href="#id6" id="id3">3</a>.
The code is from the Keras convolutional variational autoencoder example and
I just made some small changes to the parameters.  I also added some annotations
that make reference to the things we discussed in this post.</p>
<p>Figure 6 shows a sample of the digits I was able to generate with 64 latent
variables in the above Keras example.</p>
<div class="figure align-center">
<img alt="MNIST digits generated from a variational autoencoder model" src="../../images/generated_digits.png" style="height: 350px;"><p class="caption">Figure 6: MNIST digits generated from a variational autoencoder model.</p>
</div>
<p>Not the prettiest hand writing =) We definitely got some decent looking digits
but also some really weird ones.  Usually the explanation for the weird ones
are that they're in between two digits or two styles of writing the same digit
(or maybe I didn't train the network well?).
An example might be the top left digit.  Is it a "3", "5" or "6"?
Kind of hard to tell.</p>
<p>Anyways take a look at the
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb">notebook</a>,
I find it really interesting to see the connection between theory and
implementation.  It's often that you see an implementation and it's very
difficult to reverse-engineer it back to the theory because of all the
simplifications that have been done.  Hopefully this post and the accompanying
notebook will help.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>Variational autoencoders are such a cool idea: it's a full blown probabilistic
latent variable model which you don't need explicitly specify!  On top of that,
it builds on top of modern machine learning techniques, meaning that it's also
quite scalable to large datasets (if you have a GPU).  I'm a big fan of
probabilistic models but an even bigger fan of practical things, which is why
I'm so enamoured with the idea of these variational techniques in ML.  I plan
on continuing in this direction of exploring more of these techniques in ML
that have a solid basis in probability.  Look out for future posts!</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li><p>Previous Posts:
<a class="reference external" href="../variational-bayes-and-the-mean-field-approximation/">Variational Bayes and The Mean-Field Approximation</a>, <a class="reference external" href="../the-calculus-of-variations/">Variational Calculus</a></p></li>
<li><p>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Bayesian methods</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">Generative Models</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoders</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a></p></li>
<li><p>"Tutorial on Variational Autoencoders", Carl Doersch, <a class="reference external" href="https://arxiv.org/abs/1606.05908">https://arxiv.org/abs/1606.05908</a></p></li>
</ul>
<p><br></p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd>
<p>Said another way, the posterior distribution for each sample <span class="math">\(p(z|X=x_i)\)</span> has some distinctive shape that is probably very different from the prior <span class="math">\(p(z)\)</span>.  If we sample any given <span class="math">\(z_m\sim Z\)</span> (which has distribution <span class="math">\(p(z)\)</span>), then most of those samples will have low <span class="math">\(p(Z=z_m|X=x_i)\)</span> because the distributions have vastly different shapes.  This implies that, <span class="math">\(p(X=x_i|Z=z_m)\)</span> (recall Bayes theorem) will also be low, which implies little contribution to our likelihood in Equation 7.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd>
<p>If you find this a bit confusing, here's another explanation.  The only reason we did all the work on the "encoder" part was to generate a good distribution for <span class="math">\(z|X\)</span>.  That is given an <span class="math">\(x_i\)</span>, find the likely <span class="math">\(z\)</span> values.  However, we made sure that when we average over all the <span class="math">\(X\)</span> observations, our average <span class="math">\(z\)</span> values would still match our prior <span class="math">\(p(z)\)</span> isotropic normal distribution via the KL divergence.  That means, sampling for our isotropic normal distribution should still give us likely values for <span class="math">\(z\)</span>.</p>
</dd>
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd>
<p>I initially just tried to use this example with just my CPU but it was painfully slow (~ 5+ min/epoch).  So I embarked on a multi-week journey to buy a modern GPU, re-build my computer and dual-boot Linux (vs. using a virtual machine).  The speed-up was quite dramatic, now it's around ~15 secs/epoch.</p>
</dd>
</dl>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/autoencoders/" rel="tag">autoencoders</a></li>
            <li><a class="tag p-category" href="../../categories/generative-models/" rel="tag">generative models</a></li>
            <li><a class="tag p-category" href="../../categories/kullback-leibler/" rel="tag">Kullback-Leibler</a></li>
            <li><a class="tag p-category" href="../../categories/variational-calculus/" rel="tag">variational calculus</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../variational-bayes-and-the-mean-field-approximation/" rel="prev" title="Variational Bayes and The Mean-Field Approximation">Previous post</a>
            </li>
            <li class="next">
                <a href="../a-variational-autoencoder-on-the-svnh-dataset/" rel="next" title="A Variational Autoencoder on the SVHN dataset">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            Hi, I'm <a href="http://www.briankeng.com/about">Brian Keng</a>.  This is
            <a href="../../">the place</a> where I write about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL">Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents  2023         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
