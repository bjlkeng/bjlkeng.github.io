<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A short post on showing what happens if you keep summarizing a piece of text.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Iterative Summarization using LLMs | Bounded Rationality</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/iterative-summarization-using-llms/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Brian Keng">
<link rel="prev" href="../a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/" title="A Look at The First Place Solution of a Dermatology Classification Kaggle Competition" type="text/html">
<link rel="next" href="../the-logic-behind-entropy/" title="The Logic Behind the Maximum Entropy Principle" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Iterative Summarization using LLMs">
<meta property="og:url" content="http://bjlkeng.github.io/posts/iterative-summarization-using-llms/">
<meta property="og:description" content="A short post on showing what happens if you keep summarizing a piece of text.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-06-03T20:21:43-04:00">
<meta property="article:tag" content="blog">
<meta property="article:tag" content="fixed point">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="OpenAI">
<meta property="article:tag" content="summarization">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="http://bjlkeng.github.io/">

            <span id="blog-title">Bounded Rationality</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <div class="row">
        <!--Body content-->
            <div class="col-lg-9">
                
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Iterative Summarization using LLMs</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2024-06-03T20:21:43-04:00" itemprop="datePublished" title="2024-06-03 20:21">2024-06-03 20:21</time></a>
            </p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>After being busy for the first part of the year, I finally have a bit of time
to work on this blog.  After a lot of thinking about how to best fit it into my
schedule, I've decided to <em>attempt</em> to write shorter posts.  Although I do get
a lot of satisfaction writing long posts, it's not practical because of the
time commitment.  Better to break it up into smaller parts to be able to
"ship" often rather than perfect each post.
This also allows me to experiment with smaller scoped topics, which hopefully
will keep more more motivated as well.  Speaking of which...</p>
<p>This post is about answering a random thought I had the other day: what would
happen if I kept passing an LLM's output back to itself?  I ran a few
experiments of trying to get the LLM to iteratively summarize or rephrase a
piece of text and the results are...  pretty much what you would expect.  But
if you don't know what to expect, then read on and find out what happened!</p>
<!-- TEASER_END -->
<div class="card card-body bg-light">
<h2>Table of Contents</h2>
<div class="contents local topic" id="contents">
<ul class="auto-toc simple">
<li><p><a class="reference internal" href="#setup" id="id1"><span class="sectnum">1</span> Setup</a></p></li>
<li>
<p><a class="reference internal" href="#experiments" id="id2"><span class="sectnum">2</span> Experiments</a></p>
<ul class="auto-toc">
<li><p><a class="reference internal" href="#blog-data" id="id3"><span class="sectnum">2.1</span> Blog Data</a></p></li>
<li><p><a class="reference internal" href="#random-fact-data" id="id4"><span class="sectnum">2.2</span> Random Fact Data</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#discussion" id="id5"><span class="sectnum">3</span> Discussion</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id6"><span class="sectnum">4</span> Conclusion</a></p></li>
</ul>
</div>
</div>
<p></p>
<div class="section" id="setup">
<h2><a class="toc-backref" href="#id1"><span class="sectnum">1</span> Setup</a></h2>
<p>The setup for experiments is really simple.  Start with one of two pieces of
data: A long summary of all my blog posts from my <a class="reference external" href="http://www.briankeng.com">personal site</a>, or an LLM generated list of 100 random facts.  Next, run
an LLM asking it to either: <code>Summarize the following: {data}</code> or
<code>Rephrase the following: {data}</code>.  Take the output response, repeat 50
times and observe!</p>
<p>I also used this system prompt to help not lose too much data:</p>
<blockquote>
<p>The following is a conversation with an AI assistant. The assistant is helpful,
creative, clever, and very friendly. The assistant will attempt to give a
response that is concise but ensures that all the key points are included when
relevant.</p>
</blockquote>
<p>I ran combinations of the above using <code>gpt-4o</code> and <code>gpt-3.5-turbo</code>,
and temperatures of <span class="math">\(0.0, 0.5, 1.0\)</span>.  That's it!  You can find my hacky
code on <a class="reference external" href="https://github.com/bjlkeng/sandbox/tree/master/llm_fixed_point">Github</a> for these
experiments.  Let's take a look to see what happened.</p>
</div>
<div class="section" id="experiments">
<h2><a class="toc-backref" href="#id2"><span class="sectnum">2</span> Experiments</a></h2>
<div class="section" id="blog-data">
<h3><a class="toc-backref" href="#id3"><span class="sectnum">2.1</span> Blog Data</a></h3>
<p>First up are the blog post experiments shown in Figure 1 (never mind the 42
in the label, that's just the random seed) where the X-axis is the iteration
number and the Y-axis is the length of the response.  For these experiments, I only
ran the "Summarize" prompt.  The input data to this is
a summary by <code>gpt-4o</code> of a scrape of my blog, which you can look at
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/llm_fixed_point/blog_gpt4o_summary.txt">here</a>.
It's about 7.6 KB in size compared to all my blog posts which are closer to
500KB.</p>
<div class="figure align-center">
<img alt="Blog post experiments" src="../../images/llm_fixed_point_blog.png" style="height: 350px;"><p class="caption"><strong>Figure 1: Length of response from LLM's for iterative summarization across {gpt-4o, gpt-3.5-turbo} and temperatures of {0.0, 0.5, 1.0}.</strong></p>
</div>
<p>In the first iteration, the LLM drops a lot of bytes cutting the size down
by more than half in all cases.  Across almost all runs, the LLM's converged to
a pretty short response.  Here's an example from <code>gpt-4o_42_0.5</code> run:</p>
<blockquote>
<p>The author, an AI and data science expert affiliated with BorealisAI and
the University of Toronto, blends their professional knowledge with
interests in sports, music, and technology to offer varied reading
suggestions. They are a respected speaker, patent holder, and researcher
whose reflections from 2020-2022 emphasize family, career growth, humility,
and mentorship. At Rubikloud, they prioritized people and learning,
contributing to its acquisition by Kinaxis. Throughout the pandemic, they
maintained routines and optimized their work environment. Their 2019 review
highlights professional achievements, personal health, team management, and
includes an inspiring story about Mozart.</p>
</blockquote>
<p>I mean as a summary of my entire blog in less than 1000 bytes, it's not
terrible.  But the original summary, which was already greatly condensed from
all my writing, is 7x the size.  The only outlier is <code>gpt-4o</code> at
<code>temperature=0.0</code>, here's the summary it gave:</p>
<blockquote>
<p>The author is a Research Director at BorealisAI and an Adjunct Professor at the University of Toronto, specializing in Electrical and Computer Engineering. He enjoys sports, music, technology, and economics, and values simplicity and flexibility. His expertise includes AI for hardware design automation and teaching data structures and algorithms. He recommends "Outliers" by Malcolm Gladwell and "The Intelligent Investor" by Benjamin Graham.</p>
<p>His contributions to data science and AI include talks, patents, and papers, focusing on machine learning, decision automation, and AI in retail, with notable patents in user behavioral segmentation and influencer analysis.</p>
<p>In his yearly reviews, he reflects on professional and personal milestones:</p>
<ul class="simple">
<li><p><strong>2022</strong>: A stable year with family time and professional highlights, emphasizing mindfulness and quality time.</p></li>
<li><p><strong>2021</strong>: Adaptation to the pandemic, starting at BorealisAI, and hopes for better social connections and fitness.</p></li>
<li><p><strong>2020</strong>: Significant changes like his daughter's birth, buying a house, and Rubikloud's acquisition by Kinaxis, with challenges in maintaining hobbies and fitness.</p></li>
</ul>
<p>Reflecting on six years at Rubikloud, he emphasizes the importance of people, learning, and resilience, with the acquisition by Kinaxis marking a new chapter. Lessons from the pandemic include adjusting routines, managing stress, and improving remote work efficiency.</p>
<p>The 2019 review covers professional highs and emotional lows, contributions to Rubikloud's growth, and challenges in work-life balance and burnout. Teaching was fulfilling, and personal hobbies progressed, with aims to improve health and fitness in 2020.</p>
<p>He reviews books like "Tuesdays with Morrie" and "Radical Candor," offering insights on cherishing life, honest feedback, and learning strategies. A humorous story about Mozart highlights self-sufficiency and the importance of self-motivation and independence.</p>
</blockquote>
<p>It's a respectable 2000 characters long and still maintains some of the
original format.  Perhaps what is most interesting here is that it looks like
that <code>gpt-4o</code> reached a fixed point in this configuration.  The plateau in
Figure 1 in fact did produce the same summary over and over again.  This obviously
could only really happen at temperature 0 where each run is (mostly) deterministic.
<code>gpt-3.5-turbo</code> showed similar behavior with a shorter summary at this
temperature but it did drop a few words at iteration 14 and again at iteration
49. This is likely because of the slight non-determinism of how OpenAI runs
the models even with a fixed random seed.</p>
<p>At any other larger temperature, we would expect deviation, which explains all
the wiggles on the other runs.  We also see that the other <code>gpt-4o</code> runs
with larger temperature did have similar length summaries in the first few
iterations, but quickly devolved into much shorter ones where I assume it could
not make it more concise.  The <code>gpt-3.5-turbo</code> run's response length drops
pretty quickly after the first iteration though.</p>
<p>All these results aren't too surprising except for the "near" fixed point that
we saw with <cite>gpt-4o</cite>, temperature 0.  It just seems unlikely to me that it
would reproduce the exact same text instead of modifying a word here or there.
These LLM's are mysterious blackboxes indeed.</p>
</div>
<div class="section" id="random-fact-data">
<h3><a class="toc-backref" href="#id4"><span class="sectnum">2.2</span> Random Fact Data</a></h3>
<p>In this set of experiments, I first asked <code>gpt-4o</code> to generate 100 random facts
that I would then use as input data, which I put on
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/llm_fixed_point/100_facts.txt">Github</a>.
Similar to above, I then asked the LLM to either summarize or rephrase the input data
over and over again.  Figure 2 shows the results in terms of lines (where each
fact is on a line).</p>
<div class="figure align-center">
<img alt="Random facts experiments" src="../../images/llm_fixed_point-random_facts.png" style="height: 350px;"><p class="caption"><strong>Figure 2: Number of lines of response from LLM's for iterative summarization across a sample of experiments from {gpt-4o, gpt-3.5-turbo} and temperatures of {0.0, 0.5, 1.0} and two different prompts.</strong></p>
</div>
<p>In this chart I only included a sample because all of the other experiments
were pretty boring -- they just mirrored the majority, which were able to
retain all 100 lines of facts.  The two outliers were <code>gpt-3.5-turbo</code>
with the rephrasing prompt and <code>gpt-4o</code> with the summary prompt, both at
temperature <span class="math">\(1.0\)</span>.  The <code>gpt-3.5-turbo</code> at iteration 35 randomly
decided to drop half of the facts and spit out a list of only 50 lines long.
Not sure what happened here, but I guess it just randomly decided to stop!
The <code>gpt-4o</code> run decided to drop the list format entirely on iteration 3
and just summarize the list with a short paragraph, which obviously dropped a
lot of information.</p>
<p>Unsurprisingly both happened at temperature <span class="math">\(1.0\)</span>, and maybe slightly
surprisingly, not all runs at <span class="math">\(1.0\)</span> had this issue.  There were two other
runs paralleling the ones above but with the opposite prompt that kept all 100
facts.  This is just another good reminder that LLM behavior is indeed random
and the randomness scales with temperature.  Consequently, they are not easy to
control at all.</p>
</div>
</div>
<div class="section" id="discussion">
<h2><a class="toc-backref" href="#id5"><span class="sectnum">3</span> Discussion</a></h2>
<p>Here are a couple of other random thoughts I had:</p>
<ul>
<li>
<p>I kept getting an error at temperature <span class="math">\(2.0\)</span>:</p>
<blockquote>
<p>Error code: 500 - {'error': {'message': 'Failed to create completion as the model generated invalid Unicode output. Unfortunately, this can happen in rare situations. Consider reviewing your prompt or reducing the temperature of your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID req_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX in your message.)', 'type': None, 'param': None, 'code': None}}</p>
</blockquote>
<p>I guess at that temperature the output token distribution gets really flat
and you get weird non-Unicode characters being selected?  I probably should
have tried to do some runs at temperature <span class="math">\(1.5\)</span> or something like that
but I'll leave that to someone else if they're curious.</p>
</li>
<li><p>I used <a class="reference external" href="https://wandb.ai/">Weights &amp; Biases</a> for all the experimentation
(maybe you can tell from the charts?), and I like it!  I played around with
it a bit before, and decided that I should keep using it to get more familiar
with it.  It's a nice time saver to not have to manage all the logged data
and code up the right visualizations.  Especially with Github CoPilot helping
smooth the coding part, I was able to do most of what I wanted pretty easily.
For organizations, there are obvious lock-in problems.  From an enterprise
point of view, it's also pretty expensive (so I hear) but I guess at that
scale you can afford it.</p></li>
</ul>
</div>
<div class="section" id="conclusion">
<h2><a class="toc-backref" href="#id6"><span class="sectnum">4</span> Conclusion</a></h2>
<p>That's it!  Possibly my shortest post yet, and kind of fun to just randomly
play around without doing anything too grand.  I haven't given up on ML or math
heavy stuff though, just wanted to prove to myself that I could write a short
post and ease myself back into it.  See you next time!</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/blog/" rel="tag">blog</a></li>
            <li><a class="tag p-category" href="../../categories/fixed-point/" rel="tag">fixed point</a></li>
            <li><a class="tag p-category" href="../../categories/llm/" rel="tag">LLM</a></li>
            <li><a class="tag p-category" href="../../categories/openai/" rel="tag">OpenAI</a></li>
            <li><a class="tag p-category" href="../../categories/summarization/" rel="tag">summarization</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/" rel="prev" title="A Look at The First Place Solution of a Dermatology Classification Kaggle Competition">Previous post</a>
            </li>
            <li class="next">
                <a href="../the-logic-behind-entropy/" rel="next" title="The Logic Behind the Maximum Entropy Principle">Next post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'left', // Change this to 'center' to center equations.
    displayIndent: '2em',
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": "0em 0em 1em 0em"}}
    }
});
</script></article>
</div>
            <div class="col-md-3 ">
            <div class="card card-body bg-light">
            <p>
            Hi, I'm <a href="http://www.briankeng.com/about">Brian Keng</a>.  This is
            <a href="../../">the place</a> where I write about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br>
</div>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
            </div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2025         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>



        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
