<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A look at regularization through the lens of probability.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>A Probabilistic Interpretation of Regularization | Bounded Rationality</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="A look at regularization through the lens of probability.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../beyond-collaborative-filtering/" title="Beyond Collaborative Filtering" type="text/html">
<link rel="next" href="../the-expectation-maximization-algorithm/" title="The Expectation-Maximization Algorithm" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="A Probabilistic Interpretation of Regularization">
<meta property="og:url" content="http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/">
<meta property="og:description" content="A look at regularization through the lens of probability.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-08-29T08:52:33-04:00">
<meta property="article:tag" content="Bayesian">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="probability">
<meta property="article:tag" content="regularization">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">A Probabilistic Interpretation of Regularization</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2016-08-29T08:52:33-04:00" itemprop="datePublished" title="2016-08-29 08:52">2016-08-29 08:52</time></a></p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This post is going to look at a probabilistic (Bayesian) interpretation of
regularization.  We'll take a look at both L1 and L2 regularization in the
context of ordinary linear regression.  The discussion will start off
with a quick introduction to regularization, followed by a back-to-basics
explanation starting with the maximum likelihood estimate (MLE), then on to the
maximum a posteriori estimate (MAP), and finally playing around with priors to
end up with L1 and L2 regularization.</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Regularization </h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Regularization</a>
is the process of introducing additional information in order to solve
ill-posed problems or prevent overfitting.  A trivial example is when trying to
fit a simple linear regression but you only have one point.  In this case, you can't
estimate both the slope and intercept (you need at least two points) so any MLE
estimate (which <em>only</em> uses the data) will be ill-formed.  Instead, if you
provide some "additional information" (i.e. prior information <a class="footnote-reference" href="#id12" id="id1">[1]</a>), you can
get a much more reasonable estimate.</p>
<p>To make things a bit more concrete, let's talk about things in the context of a
<a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">ordinary linear regression</a>.
Recall from my previous post on
<a class="reference external" href="../a-probabilistic-view-of-regression">linear regression</a>
(Equation 11 in that post)
that the maximum likelihood estimate for ordinary linear regression is given by:</p>
<div class="math">
\begin{align*}
{\bf \hat{\beta}_{\text{MLE}}}
&amp;= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2 \\
&amp;= \arg\min_{\bf \beta} \sum_{i=1}^{n} (y_i-\hat{y_i})^2
\tag{1}
\end{align*}
</div>
<p>The estimate is quite intuitive: pick the coefficients (<span class="math">\(\beta_j\)</span>) that
minimizes the squared error between the observed values (<span class="math">\(y_i\)</span>) and those
generated by our linear model (<span class="math">\(\hat{y_i}\)</span>).</p>
<p>In a similar vein as above, consider what happens when we only have one data
point <span class="math">\((y_0, {\bf x_0})\)</span> but more than one coefficient.  There are any
number of possible "lines" or equivalently coefficients that we could draw to
minimize Equation 1.  Thinking back to high school math, this is analogous to
estimating the slope and intercept for a line but with only one point.
Definitely a problem they didn't teach you in high school.
I'm using examples where we don't have enough data
but there could other types of issues such as
<a class="reference external" href="https://en.wikipedia.org/wiki/Multicollinearity">colinearity</a> that may
not outright prevent fitting of the model but will probably produce an
unreasonable estimate.</p>
<p>Two common schemes for regularization add a simple modification to Equation 1:</p>
<div class="math">
\begin{align*}
{\bf \hat{\beta}_{\text{L1}}}
= \arg\min_{\bf \beta} \big( \sum_{i=1}^{n} (y_i- (\beta_0 + \beta_1 x_{i, 1} + ... + \beta_p x_{i,p}))^2
  + \lambda \sum_{j=0}^{p} | \beta_j | \big)
\\
\tag{2}
\end{align*}
</div>
<div class="math">
\begin{equation*}
{\bf \hat{\beta}_{\text{L2}}}
= \arg\min_{\bf \beta} \big( \sum_{i=1}^{n} (y_i-(\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2
  + \lambda \sum_{j=0}^{p} | \beta_j | ^2 \big)
\tag{3}
\end{equation*}
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#Regularizers_for_sparsity">L1 regularization</a>
(also known as LASSO in the context of linear regression) promotes sparsity of coefficients.
Sparsity translates to some coefficients having values, while others are zero
(or closer to zero).  This can be seen as a form of feature selection.
<a class="reference external" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)#Tikhonov_regularization">L2 regularization</a>
(also known as ridge regression in the context of linear regression and
generally as Tikhonov regularization) promotes smaller coefficients (i.e. no one
coefficient should be too large).  This type of regularization is pretty common
and typically will help in producing reasonable estimates.  It also has a simple probabilistic
interpretation (at least in the context of linear regression) which we will see below.
(You can skip the next two sections if you're already familiar with the basics
of MLE and MAP estimates.)</p>
<p><br></p>
<h4> The Likelihood Function </h4>
<p>Recall Equation 1 can be derived from the likelihood function (without
<span class="math">\(\log(\cdot)\)</span>) for ordinary linear regression:</p>
<div class="math">
\begin{align*}
\mathcal{L({\bf \beta}|{\bf y})} &amp;:= P({\bf y} | {\bf \beta}) \\
    &amp;= \prod_{i=1}^{n} P_Y(y_i|{\bf \beta}, \sigma^2) \\
    &amp;= \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2}{2\sigma^2}}
\tag{4}
\end{align*}
</div>
<p>where <span class="math">\({\bf y}\)</span> are our observed data points (<span class="math">\(y_1, \ldots, y_N\)</span>) and
<span class="math">\(P_Y(y_i|\mu, \sigma^2)\)</span> is the probability of observing the data point <span class="math">\(y_i\)</span>.
The implicit assumption in linear regression is that the data points are normally
distributed about the regression line (see my past post on
<a class="reference external" href="../a-probabilistic-view-of-regression">linear regression</a> for more on this).</p>
<p>Classical statistics focuses on maximizing this likelihood function,
which <em>usually</em> provides a pretty good estimate -- except when it doesn't like
in the case of "small data".  However, looking at the problem from a more
probabilistic point of view (i.e. Bayesian), we don't just want to maximize
the likelihood function but rather the posterior probability.  Let's see how
that works.</p>
<p><br></p>
<h4> The Posterior </h4>
<p>We'll start by reviewing
<a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference#Formal">Bayes Theorem</a>
where we usually denote the parameters we're trying to estimate by
<span class="math">\(\theta\)</span> and the data as <span class="math">\(y\)</span>:</p>
<div class="math">
\begin{align*}
P(\theta | y) &amp;= \frac{P(y | \theta) P(\theta)}{P(y)} \\
\text{posterior} &amp;= \frac{\text{likelihood} \cdot \text{prior}}{\text{evidence}}
\tag{5}
\end{align*}
</div>
<p>In Bayesian inference, we're primarily concerned with the posterior: "the
probability of the parameters given the data".  Put in another way, we're looking
to estimate the probability distribution of the parameters (<span class="math">\(\theta\)</span>)
given the data we have observed (<span class="math">\(y\)</span>).  Contrast this with classical methods
which instead try to find the best parameters to maximize likelihood: the
probability of observing data (<span class="math">\(y\)</span>) given a different values of the
parameters.  Definitely a subtle difference but I think most would agree the Bayesian
interpretation is much more natural <a class="footnote-reference" href="#id13" id="id3">[2]</a>.</p>
<p>Looking at Equation 5 in more detail, we already know how to compute the likelihood but
the two new parts are the prior and the evidence.  This is where proponents of
frequentist statistics usually have a philosophical dilemma.  The prior is actually
something we (the modeler) explicitly choose that is <strong>not</strong> based on the data <a class="footnote-reference" href="#id14" id="id4">[3]</a>
(<span class="math">\(y\)</span>)  *gasp*!</p>
<p>Without getting into a whole philosophical spiel, adding some additional prior
information is exactly what we want in certain situations!  For example when
we don't have enough data, we probably have some idea about what is reasonable
given our knowledge of the problem.  This prior allows us to encode this
knowledge.  Even in cases where we don't explicitly have this problem, we can
choose a <a class="reference external" href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors">"weak" prior</a>
which will only bias the result slightly from the MLE estimate.  In cases where
we have lots of the data, the likelihood dominates Equation 5 anyways so the
result will be similar in these cases.</p>
<p>From Equation 5, a full Bayesian analysis would look at the distribution of the
parameters (<span class="math">\(\theta\)</span>).  However, most people will settle for something a
bit less involved: finding the maximum of the posterior (which turns out to be
an easier problem in most cases).  This is known as the
<a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori probability estimate</a>,
usually abbreviated by MAP.  This simplifies the analysis and in particular
allows us to ignore the evidence (<span class="math">\(P(y)\)</span>), which is constant relative to
the parameters we're trying to estimate (<span class="math">\(\theta\)</span>).</p>
<p>Formalizing the MAP estimate, we can write it as:</p>
<div class="math">
\begin{align*}
{\bf \hat{\theta}_{\text{MAP}}} &amp;= \arg\max_{\bf \theta} P(\theta | y) \\
&amp;= \arg\max_{\bf \theta} \frac{P(y | \theta) P(\theta)}{P(y)} \\
&amp;= \arg\max_{\bf \theta} P(y | \theta) P(\theta) \\
&amp;= \arg\max_{\bf \theta} \log(P(y | \theta) P(\theta)) \\
&amp;= \arg\max_{\bf \theta} \log P(y | \theta) + \log P(\theta)
\tag{6}
\end{align*}
</div>
<p>Notice that we can get rid of the evidence term (<span class="math">\(P(y)\)</span>) because it's
constant with respect to the maximization and also take the <span class="math">\(\log\)</span> of the
inner function because it's monotonically increasing.  Contrast this with the
MLE estimate:</p>
<div class="math">
\begin{equation*}
{\bf \hat{\theta}_{\text{MLE}}} = \arg\max_{\bf \theta} \log P(y | \theta)
\tag{7}
\end{equation*}
</div>
<p><br></p>
<h4> Selecting Priors for Linear Regression </h4>
<p>The main idea is to select Bayesian priors on the coefficients of linear
regression that get us to L1 and L2 regularization (Equation 2 and 3).  Let's
see how this works.</p>
<p></p>
<h5> Normally Distributed Priors </h5>
<p>We'll start with our good old friend the normal distribution and place a
zero-mean normally distributed prior on <em>each</em> <span class="math">\(\beta_i\)</span> value, all with
identical variance <span class="math">\(\tau^2\)</span>.  From Equation 6 and filling in the
likelihood function from Equation 4 and our prior:</p>
<div class="math">
\begin{align*}
 &amp;\arg\max_{\bf \beta} \Big[ \log \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2}{2\sigma^2}}
 + \log \prod_{j=0}^{p} \frac{1}{\tau\sqrt{2\pi}}e^{-\frac{\beta_j^2}{2\tau^2}} \Big] \\
&amp;= \arg\max_{\bf \beta} \Big[- \sum_{i=1}^{n} {\frac{(y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2}{2\sigma^2}}
 - \sum_{j=0}^{p} {\frac{\beta_j^2}{2\tau^2}} \Big]\\
&amp;= \arg\min_{\bf \beta} \frac{1}{2\sigma^2} \big[ \sum_{i=1}^{n} (y_i-(\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2
 + \frac{\sigma^2}{\tau^2} \sum_{j=0}^{p} \beta_j^2 \big] \\
&amp;= \arg\min_{\bf \beta} \big[ \sum_{i=1}^{n} (y_i-(\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2 + \lambda \sum_{j=0}^{p} \beta_j^2 \big]
 \tag{8}
\end{align*}
</div>
<p>Notice that we dropped many of the constants (with respect to <span class="math">\(\beta\)</span>)
and factored a bit to simplify the expression.
You can see this is the same expression as Equation 3 (L2 Regularization)
with <span class="math">\(\lambda = \sigma^2/\tau^2\)</span> (remember <span class="math">\(\sigma\)</span> is
assumed to be constant in ordinary linear regression, and we get to pick
<span class="math">\(\tau\)</span> for our prior).  We can adjust the amount of regularization we
want by changing <span class="math">\(\lambda\)</span>.  Equivalently, we can adjust how much we want
to weight the priors carry on the coefficients (<span class="math">\(\beta\)</span>).  If we have a
very small variance (large <span class="math">\(\lambda\)</span>) then the coefficients will be very
close to 0; if we have a large variance (small <span class="math">\(\lambda\)</span>) then the
coefficients will not be affected much (similar to as if we didn't have any
regularization).</p>
<p></p>
<h5> Laplacean Priors </h5>
<p>Let's first review the density of the <a class="reference external" href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace distribution</a> (something that's
usually not introduced in beginner probability classes):</p>
<div class="math">
\begin{equation*}
Laplace(\mu, b) = \frac{1}{2b} e^{-\frac{|x-\mu|}{b}}
\end{equation*}
</div>
<p>This is sometimes called the "double exponential" distribution because it looks
like two exponential distributions placed back to back (appropriately scaled
with a location parameter).  It's also quite similar to our Gaussian in form,
perhaps you can see how we can get to L1 regularization already?</p>
<p>Starting with a zero-mean Laplacean prior on all the coefficients like we did
in the previous subsection:</p>
<div class="math">
\begin{align*}
 &amp;\arg\max_{\bf \beta} \Big[ \log \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2}{2\sigma^2}}
 + \log \prod_{j=0}^{p} \frac{1}{2b}e^{-\frac{|\beta_j|}{2b}} \Big] \\
&amp;= \arg\max_{\bf \beta} \Big[- \sum_{i=1}^{n} {\frac{(y_i- (\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2}{2\sigma^2}}
 - \sum_{j=0}^{p} {\frac{|\beta_j|}{2b}} \Big]\\
&amp;= \arg\min_{\bf \beta} \frac{1}{2\sigma^2} \big[ \sum_{i=1}^{n} (y_i-(\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2
 + \frac{\sigma^2}{b} \sum_{j=0}^{p} |\beta_j| \big] \\
&amp;= \arg\min_{\bf \beta} \big[ \sum_{i=1}^{n} (y_i-(\beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p}))^2 + \lambda \sum_{j=0}^{p} |\beta_j| \big]
 \tag{9}
\end{align*}
</div>
<p>Again we can see that Equation 9 contains the same expression as L1
Regularization in Equation 2.</p>
<p>The Laplacean prior has a slightly different effect compared to L2
regularization.  Instead of preventing any of the coefficients from being too
large (due to the squaring), L1 promotes sparsity.  That is, zeroing out some
of the coefficients.  This makes some sense if you look at the density of a
Laplacean prior where there is a sharp increase in the density at its mean.</p>
<p>Another way to intuitively see this is to compare two solutions <a class="footnote-reference" href="#id15" id="id5">[4]</a>.  Let's
imagine we are estimating two coefficients in a regression.  In L2
regularization, the solution <span class="math">\({\bf \beta} = (1, 0)\)</span> has the same weight
as <span class="math">\({\bf \beta} = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})\)</span> so they are
both treated equally.  In L1 regularization, the same two solutions
favor the sparse one:</p>
<div class="math">
\begin{equation*}
 ||(1, 0)||_1 = 1 &lt; ||(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})||_1 = \sqrt{2}
\tag{10}
\end{equation*}
</div>
<p>So L2 regularization doesn't have any specific built in mechanisms to favor
zeroed out coefficients, while L1 regularization actually favors these sparser
solutions.</p>
<p><br></p>
<h4> Conclusion </h4>
<p>L1 and L2 regularization are such intuitive techniques when viewed shallowly
as just extra terms in the objective function (i.e. "shrink the coefficients").
However, I was delighted to find out that it also has a Bayesian interpretation,
it just seems so much more elegant that way.  As I have mentioned before, I'm a
big fan probabilistic interpretations of machine learning and you can expect
many more posts on this subject!</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li>Wikipedia:
<a class="reference external" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Regularization</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">ordinary linear regression</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference#Formal">Bayes Theorem</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors">"weak" prior</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori probability estimate</a>
</li>
<li>A previous post on <a class="reference external" href="../a-probabilistic-view-of-regression">linear regression</a>
</li>
<li>Machine Learning: A Probabilistic Perspective, Kevin P. Murphy</li>
</ul>
<p><br></p>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>One philosophical counterpoint is that we should "let the data speak for itself".  Although superficially satisfying, it is almost always the case where you inject "prior" knowledge into interpreting the data.  For example, selecting a linear regression model already adds some prior knowledge or intuition to the data.  In the same way, if we have some vague idea that the mean of the data should be close to zero, why not add that information into the problem?  If there's enough data and we've coded things right, then the prior isn't that impactful anyways.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id3">[2]</a></td>
<td>As you might have guessed, I'm fall into the Bayesian camp.  Although I would have to say that I'm much more of a pragmatist above all else.  I'll use whatever works, frequentist, Bayesian, no theoretical basis, doesn't really matter as long as I can solve the desired problem in a reasonable manner.  It just so happens Bayesian methods produce reasonable estimates very often.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id4">[3]</a></td>
<td>Well that's not exactly true.  As a modeler, we can pick a prior that does depend on the data, although that's a bit of "double dipping".  These methods are generally known as <a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_Bayes_method">empirical Bayes methods</a>.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id5">[4]</a></td>
<td>I got this example from Machine Learning: A Probabilistic Perspective.  It has great explanations on both L1 and L2 regularization as long as you have some moderate fluency in probability.  I highly recommend this textbook if you want to dig into the meat of ML techniques.  It's very math (probability) heavy but really provides good high level explanations that help with intuition.</td>
</tr></tbody>
</table>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/bayesian/" rel="tag">Bayesian</a></li>
            <li><a class="tag p-category" href="../../categories/probability/" rel="tag">probability</a></li>
            <li><a class="tag p-category" href="../../categories/regularization/" rel="tag">regularization</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../beyond-collaborative-filtering/" rel="prev" title="Beyond Collaborative Filtering">Previous post</a>
            </li>
            <li class="next">
                <a href="../the-expectation-maximization-algorithm/" rel="next" title="The Expectation-Maximization Algorithm">Next post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
