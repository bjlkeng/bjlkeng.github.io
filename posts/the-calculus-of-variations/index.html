<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A primer on variational calculus.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>The Calculus of Variations | Bounded Rationality</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/the-calculus-of-variations/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="A primer on variational calculus.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../maximum-entropy-distributions/" title="Maximum Entropy Distributions" type="text/html">
<link rel="next" href="../variational-bayes-and-the-mean-field-approximation/" title="Variational Bayes and The Mean-Field Approximation" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="The Calculus of Variations">
<meta property="og:url" content="http://bjlkeng.github.io/posts/the-calculus-of-variations/">
<meta property="og:description" content="A primer on variational calculus.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-02-26T10:08:38-05:00">
<meta property="article:tag" content="differentials">
<meta property="article:tag" content="entropy">
<meta property="article:tag" content="lagrange multipliers">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="probability">
<meta property="article:tag" content="variational calculus">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">The Calculus of Variations</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-02-26T10:08:38-05:00" itemprop="datePublished" title="2017-02-26 10:08">2017-02-26 10:08</time></a></p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This post is going to describe a specialized type of calculus called
variational calculus.
Analogous to the usual methods of calculus that we learn in university,
this one deals with functions <em>of functions</em> and how to
minimize or maximize them.  It's used extensively in physics problems such as
finding the minimum energy path a particle takes under certain conditions.  As
you can also imagine, it's also used in machine learning/statistics where you
want to find a density that optimizes an objective <a class="footnote-reference" href="#id4" id="id1">[1]</a>.  The explanation I'm
going to use (at least for the first part) is heavily based upon Svetitsky's
<a class="reference external" href="http://julian.tau.ac.il/bqs/functionals/functionals.html">Notes on Functionals</a>, which so far is
the most intuitive explanation I've read.  I'll try to follow Svetitsky's
notes to give some intuition on how we arrive at variational calculus from
regular calculus with a bunch of examples along the way.  Eventually we'll
get to an application that relates back to probability.  I think with the right
intuition and explanation, it's actually not too difficult, enjoy!</p>
<!-- TEASER_END -->
<p><br></p>
<h4> From multivariable functions to functionals </h4>
<p>Consider a regular scalar function <span class="math">\(F(y)\)</span>, it maps a single value <span class="math">\(y\)</span> to a
single value <span class="math">\(F(y)\)</span>.
You can differentiate it to get <span class="math">\(\frac{dF}{dy} = F'(y)\)</span>.
Another way to think about it is starting at <span class="math">\(y_1\)</span>, move a tiny
step away, call it <span class="math">\(dy\)</span>, then <span class="math">\(F\)</span> will change by its <a class="reference external" href="https://en.wikipedia.org/wiki/Differential_of_a_function">differential</a>:</p>
<div class="math">
\begin{equation*}
dF = F(y_1 + dy) - F(y_1) = \frac{dF}{dy}|_{y_1} dy \tag{1}
\end{equation*}
</div>
<p>Now let's consider the same situation with a multivariable function
<span class="math">\(F(y_1, y_2, \ldots, y_n)\)</span>.  It maps a set of values <span class="math">\(y_1, \ldots,
y_n\)</span> to a single value <span class="math">\(F(y_1, y_2, \ldots, y_n)\)</span>.
You can also differentiate it by taking partial derivatives:
<span class="math">\(\frac{dF}{dy_1}, \ldots, \frac{dF}{dy_n}\)</span>.  Similarly, if I move
a tiny step away in each direction <span class="math">\(dy_1, \ldots, dy_n\)</span>, starting from
points <span class="math">\(y_1^1, \ldots, y_n^1\)</span>, the
<a class="reference external" href="https://en.wikipedia.org/wiki/Differential_of_a_function#Differentials_in_several_variables">total differential</a>
of the function is:</p>
<div class="math">
\begin{equation*}
dF = \frac{\partial F}{\partial y_1}\Big|_{y_1^1} dy_1 + \ldots + \frac{\partial F}{\partial y_n}\Big|_{y_n^1}dy_n \tag{2}
\end{equation*}
</div>
<p>So far we have only touched upon derivatives and differentials of a function
<span class="math">\(F(y_1, y_2, \ldots, y_n)\)</span> with independent variables <span class="math">\(y_1, y_2,
\ldots, y_n\)</span>.  If you squint hard enough, you may already start to see where
we're going with this: we can view the input to the function not just as
independent variables, but collectively as a group of variables derived
from a function <span class="math">\(y\)</span> (i.e. <span class="math">\(y_i = y(i), i \in \mathcal{N}\)</span>).  If
we have an infinite set of these variables, then <span class="math">\(F\)</span> is no longer just a
function of independent variables but a function of a <em>function</em> <span class="math">\(y(i)\)</span>.</p>
<p>Of course this is just some intuition and not at all precise but let's keep
going to see where this can take us.  Our independent points can be seen
as sampling some function <span class="math">\(y\)</span> on some interval <span class="math">\([a, b]\)</span>.  Let's say
we want to sample <span class="math">\(N\)</span> points, each <span class="math">\(\epsilon\)</span> distance apart,
i.e. <span class="math">\(N\epsilon = b - a\)</span>.  The <span class="math">\(n^{th}\)</span> point will be at
<span class="math">\(x_n=a + n\epsilon\)</span>, so our sample point becomes
<span class="math">\(y_n = y(x_n) = y(a + n\epsilon)\)</span>.
As <span class="math">\(N \rightarrow \infty, \epsilon \rightarrow 0\)</span>, our sampled points
<span class="math">\(y_n\)</span> become an increasingly accurate representation of our original
function <span class="math">\(y(x)\)</span>.</p>
<p>Our original "function" <span class="math">\(F\)</span> can now be thought of as a function of a set
of variables <span class="math">\(\{y_n\}\)</span> resulting in <span class="math">\(F(\{y_n\})\)</span>.  As
<span class="math">\(N \rightarrow \infty\)</span>, the original
function <span class="math">\(F\)</span> transforms into a function of an infinite set of variables,
or in another way, a function of a <em>function</em>.  The mapping is called a
<em>functional</em>, which we generally write with square brackets <span class="math">\(F[y]\)</span>.  So
instead of taking a fixed number of independent variables and outputting a
value, it takes an <em>infinite</em> number of variables, defined by <span class="math">\(y(x)\)</span> on
the interval <span class="math">\([a,b]\)</span>, and outputs a value!</p>
<p>Let's pause for a second and summarize:</p>
<blockquote>
<ul class="simple">
<li>A function <span class="math">\(y(x)\)</span> takes as input a number <span class="math">\(x\)</span> and returns a number.</li>
<li>A functional <span class="math">\(F[y]\)</span> takes as input a <em>function</em> <span class="math">\(y(x)\)</span> and returns
a number.</li>
</ul>
</blockquote>
<p>Another way to say this is <span class="math">\(F[y(x)]\)</span> takes <em>all</em> the values of <span class="math">\(y(x)\)</span>
at all the values of <span class="math">\(x\)</span> (for that <span class="math">\(y\)</span>) and maps it to a single
number.  Let's take a look at a few examples to make things concrete.</p>
<div class="admonition admonition-example-1-a-simple-functional">
<p class="first admonition-title">Example 1: A simple functional</p>
<p>The simplest functional just evaluates the input function at a particular value.</p>
<p>Define <span class="math">\(F[y] = y(3)\)</span>.</p>
<ul class="last simple">
<li><span class="math">\(F[y=x] = 3\)</span></li>
<li><span class="math">\(F[y=x^2] = (3)^2 = 9\)</span></li>
<li><span class="math">\(F[y=\ln_3(x)] = \ln_3(3) = 1\)</span></li>
</ul>
</div>
<div class="admonition admonition-example-2-an-integral-functional">
<p class="first admonition-title">Example 2: An integral functional</p>
<p>Many useful functionals will take a definite integral of the input function
as a means to map it to a number.</p>
<p>Define <span class="math">\(F[y] = \int_0^1  y(x) dx\)</span>.</p>
<ul class="last simple">
<li><span class="math">\(F[y=x] = \int_0^1  x dx = \frac{1}{2}\)</span></li>
<li><span class="math">\(F[y=x^2] = \int_0^1  x^2 dx = \frac{1}{3}\)</span></li>
<li><span class="math">\(F[y=e^x] = \int_0^1 e^x dx = e - 1\)</span></li>
</ul>
</div>
<div class="admonition admonition-example-3-functionals-with-derivatives">
<p class="first admonition-title">Example 3: Functionals with derivatives</p>
<p>Since we're dealing with functions as inputs, the functional can also
involve the derivative of an input function.</p>
<p>A functional that defines the <a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcII/ArcLength.aspx">arc length of a curve</a>:</p>
<div class="math">
\begin{equation*}
L[y] = \int_a^b \sqrt{1 + (\frac{dy}{dx})^2} dx
\end{equation*}
</div>
<p>Notice that we're using the derivative of <span class="math">\(y(x)\)</span> instead of the
function itself.  Solving for <span class="math">\(a=0, b=1, y=x\)</span>, we get:</p>
<div class="last math">
\begin{align*}
L[y=x] &amp;= \int_0^1 \sqrt{1 + (\frac{d(x)}{dx})^2} dx \\
       &amp;= \sqrt{2} \int_0^1 dx \\
       &amp;= \sqrt{2}
\end{align*}
</div>
</div>
<p>A common form of functionals that in appear in many contexts is:</p>
<div class="math">
\begin{align*}
J[y] &amp;= \int_a^b F(x, y(x), y'(x)) dx \\
&amp;\text{ for } x=[a,b], \text{   }a\leq b, \text{   }y(a)=\hat{y}_a, \text{   }y(b)=\hat{y}_b \tag{3}
\end{align*}
</div>
<p>Which is mostly just saying that <span class="math">\(y(x)\)</span> is well behaved over <span class="math">\(x\in [a,b]\)</span>.
In more detail, we want these conditions to be satisfied for any <span class="math">\(y(x)\)</span> we plug in:
<span class="math">\(y(x)\)</span> being a single-valued function,
smooth so that <span class="math">\(y'(x)\)</span> exists as well as the integral defined in Equation 3,
and the boundary conditions (<span class="math">\(x=[a,b], a\leq b, y(a)=\hat{y}_a, y(b)=\hat{y}_b\)</span>)
are satisfied.</p>
<p><br></p>
<h4> Functional Derivatives </h4>
<p>Now it's finally time to do something useful with functionals!  As with
regular calculus, whose premier application is finding minima and maxima,
we also want to be able to find the extrema of functionals.  It turns out we can define
something analogous to a derivative unsurprisingly called a <em>functional derivative</em>.
Let's see how we can intuitively build it up from the same multivariable
function from above.</p>
<p>Let's take another look at the total differential in Equation 2 again, but re-write
it this time as a sum:</p>
<div class="math">
\begin{equation*}
dF = \sum_{i=1}^N \frac{\partial F}{\partial y_i}\Big|_{y_i^1} dy_i \tag{4}
\end{equation*}
</div>
<p>Again, if you squint hard enough, as <span class="math">\(N \rightarrow \infty\)</span>,
<span class="math">\(y_i\)</span> approximates our <span class="math">\(y(x)\)</span>, and our sum turns into an integral
of a continuous function (recall the domain of our function <span class="math">\(y(x)\)</span> was <span class="math">\(x \in [a,b]\)</span>).</p>
<div class="math">
\begin{equation*}
dF = \int_{a}^b \frac{\delta F}{\delta y(x)}\Big|_{y^1(x)} \delta y(x) dx \tag{5}
\end{equation*}
</div>
<p>The meaning of Equation 5 is the same as Equation 4: a small change in <span class="math">\(F\)</span>
is proportional to a sum of small changes in each direction <span class="math">\(\delta y(x)\)</span>
(step size) multiplied by the derivative for each direction <span class="math">\(\frac{\delta
F}{\delta y(x)}\)</span> (slope), where we can think of <span class="math">\(x\)</span> as a continuous index
(analogous to <span class="math">\(i\)</span>).  As a result, the <em>functional derivative</em> is defined
by:</p>
<div class="math">
\begin{equation*}
\frac{\delta F}{\delta y(x)} \tag{6}
\end{equation*}
</div>
<p>This is analogous to the derivative at each of the "independent variables" <span class="math">\(y(x)\)</span>,
which is the same concept as the <a class="reference external" href="https://en.wikipedia.org/wiki/Gradient">gradient</a>
for multivariate functions.</p>
<p>Equation 5 then becomes a
<a class="reference external" href="https://en.wikipedia.org/wiki/Directional_derivative">directional derivative</a>,
where we can interpret as the rate of change of <span class="math">\(F\)</span> as we are
moving through "point" <span class="math">\(y^1(x)\)</span> in the direction of <span class="math">\(\delta y(x)\)</span>
(check out this <a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx">tutorial</a>
on directional derivatives for a good intuitive refresher on the subject).</p>
<p>This explanation takes us from gradients to functional derivatives
but we can also define it in terms of limits.  Using the analogy of directional
derivatives from above, if we have the functional derivative at the multivariate
"point" <span class="math">\(y(x)\)</span> moving in the multivariate "direction" of an arbitrary
function <span class="math">\(\eta(x)\)</span> then we can formulate the limit as:</p>
<div class="math">
\begin{equation*}
\lim_{\epsilon \to 0} \frac{F[y(x) + \epsilon \eta(x)] - F[y(x)]}{\epsilon}
= \int \frac{\delta F}{\delta y(x)} \eta(x) dx
\tag{7}
\end{equation*}
</div>
<p>which, if you think hard enough about, results in the same integral as Equation 5.
This also means the functional derivative is a function (natural extension from the
multivariate point analogy where the number of points is infinite within an interval).
We also define <span class="math">\(\delta y\)</span> as <span class="math">\(\epsilon \eta(x)\)</span> and call it the variation of
<span class="math">\(y\)</span>.</p>
<p>Of course, there's no guarantee that the functional derivative exists.  That's
where formal definitions and rigorous mathematics comes in, which is beyond
the scope of this post.  Also important to mention is that we can have higher order
functional derivatives that can be defined in a very similar way.  For now,
let's just focus on simple cases where everything plays nicely.</p>
<div class="admonition admonition-why-the-name-variational-calculus">
<p class="first admonition-title">Why the name <em>variational</em> calculus?</p>
<p>A variation of a functional is the small change in a functional's value
due to a small change in the functional's input.  It's the analogous concept
to a <a class="reference external" href="https://en.wikipedia.org/wiki/Differential_of_a_function">differential</a> for
regular calculus.</p>
<p>We've already seen an example of a variation in Equation 5, which is the first
variation of the functional <span class="math">\(F\)</span>:</p>
<div class="math">
\begin{equation*}
\delta F(y, \eta) = \int \frac{\delta F}{\delta y(x)} \eta(x) dx \tag{8}
\end{equation*}
</div>
<p>As mentioned above the term <span class="math">\(\epsilon \eta(x)\)</span> is also called a
variation of input <span class="math">\(y\)</span>, which is analogous to the infinitesimally small
<span class="math">\(\epsilon\)</span> in regular calculus.</p>
<p class="last">The first variation and higher order variations define the respective
functional derivatives and can be derived by taking the coefficients of the
Taylor series expansion of the functional.  More details can be found
here <a class="reference external" href="http://www.colorado.edu/engineering/CAS/courses.d/AVMM.d/AVMM.Ch01.d/AVMM.Ch01.pdf">Advanced Variational Methods In Mechanics Chapter 1: Variational
Calculus Overview</a>.</p>
</div>
<div class="admonition admonition-example-4-computing-a-simple-functional-derivative">
<p class="first admonition-title">Example 4: Computing a simple functional derivative</p>
<p>Let's try to find the functional derivative of a simple functional:</p>
<div class="math">
\begin{equation*}
F[y(x)] = \int_0^1 y(x)^2 dx \tag{9}
\end{equation*}
</div>
<p>We can calculate this by going back to Equation 5 and computing its first variation:
<span class="math">\(dF = F[y + \delta y] - F[y]\)</span>. Start by computing <span class="math">\(F[y + \delta y]\)</span>:</p>
<div class="math">
\begin{align*}
F[y + \delta y] &amp;= \int_0^1 [y(x) + \delta y(x)]^2 dx \\
 &amp;= \int_0^1 y(x)^2 + 2y(x)\delta y(x) + \delta y(x)^2 dx \\
 &amp;= \int_0^1 y(x)^2 + 2y(x)\delta y(x) + \delta y(x)^2 dx \\
 &amp;= F[y] + \int_0^1 2y(x)\delta y(x) dx + \int_0^1 \delta y(x)^2 dx
\tag{10}
\end{align*}
</div>
<p>From Equation 10, we know that in the limit <span class="math">\(\delta y \rightarrow 0\)</span> so
we can drop the last term. Finally, computing <span class="math">\(dF\)</span> we have:</p>
<div class="math">
\begin{align*}
dF &amp;= F[y + \delta y] - F[y] \\
&amp;= F[y] + \int_0^1 2y(x)\delta y(x) dx  - F[y] \\
&amp;= \int_0^1 2y(x)\delta y(x) dx
\tag{11}
\end{align*}
</div>
<p class="last">By inspection, we can see Equation 11 resembles Equation 5, thus our functional
derivative is <span class="math">\(\frac{\delta F}{\delta y(x)} = 2y(x)\)</span>.</p>
</div>
<p><br></p>
<h4> Euler-Lagrange Equation </h4>
<p>Now armed with the definition of a functional derivative, we now know how to
compute it from first principles.  However, as with regular calculus,
computing a derivative by definition can get tedious.  Fortunately, there is a
result that can help us compute the functional derivative called the
Euler-Lagrange equation, which states (roughly):</p>
<blockquote>
<p>For a given function <span class="math">\(y(x)\)</span> with a real argument <span class="math">\(x\)</span>, the
functional:</p>
<div class="math">
\begin{equation*}
F[y] = \int_a^b L(x, y(x), y'(x)) dx \tag{12}
\end{equation*}
</div>
<p>has functional derivative given by:</p>
<div class="math">
\begin{equation*}
\frac{\delta F}{\delta y(x)}
= \frac{\partial L}{\partial y} - \frac{d}{dx} \frac{\partial L}{\partial y'} \tag{13}
\end{equation*}
</div>
</blockquote>
<p>You can derive this equation using the method we used above (and a few extra
tricks) but I'll leave it as an exercise :)
For simple functionals like Equation 12, this is a very handy way to compute functional
derivatives.  Let's take a look at a couple more complicated examples.</p>
<div class="admonition admonition-example-5-use-the-euler-lagrange-equation-to-find-the-functional-derivative-of-math-f-y-x-int-0-1-x-3-e-y-x-dx">
<p class="first admonition-title">Example 5: Use the Euler-Lagrange Equation to find the
functional derivative of <span class="math">\(F[y(x)] = \int_0^1 x^3 e^{-y(x)} dx\)</span></p>
<p>Notice the second term in Equation 13 involves only <span class="math">\(y'\)</span>, which we
doesn't appear in our functional so that means it's 0.  Thus, the functional
derivative in this case just treats <span class="math">\(y\)</span> as a variable and the usual rules
of differentiation apply:</p>
<div class="last math">
\begin{equation*}
\frac{\delta F}{\delta y(x)} = \frac{\partial L}{\partial y}
= \frac{\partial ( x^3 e^{-y(x)})}{\partial y}
= -x^3 e^{-y(x)} \tag{14}
\end{equation*}
</div>
</div>
<div class="admonition admonition-example-6-use-the-euler-lagrange-equation-to-find-the-functional-derivative-of-math-f-y-x-int-0-1-x-2-y-3-y-4-dx">
<p class="first admonition-title">Example 6: Use the Euler-Lagrange Equation to find the
functional derivative of <span class="math">\(F[y(x)] = \int_0^1 x^2 y^3 y'^4 dx\)</span></p>
<p>Here we have to use all terms from Equation 13 and treat <span class="math">\(y\)</span>
and <span class="math">\(y'\)</span> as "independent" variables:</p>
<div class="last math">
\begin{align*}
\frac{\delta F}{\delta y(x)} &amp;= \frac{\partial L}{\partial y}
        - \frac{d}{dx} \frac{\partial L}{\partial y'}\\
&amp;= 3x^2 y^2 y'^4 - 4\frac{d (x^2y^3y'^3)}{dx} \\
&amp;= 3x^2 y^2 y'^4 - 8xy^3y'^3 \tag{15}
\end{align*}
</div>
</div>
<p><br></p>
<h4> Extrema of Functionals </h4>
<p>As an exercise this is interesting enough, but the real application is when we
want to minimize or maximize a functional.  In a similar way to how we find a
point that is an extremum of a function, we can also find a function that
is an extremum a functional.</p>
<p>It turns out that it's pretty much what you would expect: if we set the
functional derivative to zero, we'll find a
<a class="reference external" href="https://en.wikipedia.org/wiki/Critical_point_(mathematics)">stationary point</a>
of the functional where we possibly have a local minimum or maximum (i.e. a
necessary condition for extrema, sometimes we might find a
<a class="reference external" href="https://en.wikipedia.org/wiki/Saddle_point">saddle point</a> though).  In other
words, this is a place where the "slope" is zero.  Let's take a look at a
classic example.</p>
<div class="admonition admonition-example-7-find-the-shortest-possible-curve-between-the-points-math-a-c-and-math-b-d-for-which-the-path-length-along-the-curve-is-defined-by-math-ell-f-int-a-b-sqrt-1-f-x-2-dx">
<p class="first admonition-title">Example 7: Find the shortest possible curve between
the points <span class="math">\((a,c)\)</span> and <span class="math">\((b,d)\)</span> for which the path length
along the curve is defined by <span class="math">\(\ell(f) = \int_a^b \sqrt{1 + f'(x)^2} dx\)</span></p>
<p>First define our integrand functional:</p>
<div class="math">
\begin{equation*}
L(x,y,y') = \sqrt{1 + f'(x)^2} \tag{16}
\end{equation*}
</div>
<p>where <span class="math">\((x, y, y') = (x, f(x), f'(x))\)</span>.  Pre-computing the partial derivatives
of <span class="math">\(L\)</span>:</p>
<div class="math">
\begin{align*}
\frac{\partial L}{\partial y} &amp;= 0  \\
\frac{\partial L}{\partial y'} &amp;= \frac{f'(x)}{\sqrt{1 + f'(x)^2}}  \tag{17}
\end{align*}
</div>
<p>Plugging them into Equation 13, we can simplify the resulting differential
equation:</p>
<div class="math">
\begin{align*}
\frac{d}{dx} \frac{f'(x)}{\sqrt{1 + f'(x)^2}} &amp;= 0 \\
\frac{f'(x)}{\sqrt{1 + f'(x)^2}} &amp;= C \\
f'(x) &amp;= C\sqrt{1 + f'(x)^2} \\
f'(x)^2 &amp;= \frac{C^2}{1 - C^2} \\
f'(x) &amp;= \frac{C}{\sqrt{1 - C^2}} := A \\
f(x) &amp;= Ax + B \tag{18}
\end{align*}
</div>
<p>where we introduce a constant <span class="math">\(C\)</span> after integrating, define
a new constant <span class="math">\(A\)</span> to be the result of a constant expression with <span class="math">\(C\)</span>,
and introduce a new constant <span class="math">\(B\)</span> from the second integration.
As you would expect the shortest distance between two points is a straight line
(but now you can prove it!).</p>
<p>We can find the actual values of constants <span class="math">\(A\)</span> and <span class="math">\(B\)</span> by using
our initial conditions <span class="math">\((a,c)\)</span> and <span class="math">\((b,d)\)</span> since we know the function
has to pass through our points (i.e. compute the slope and intercept of the line):</p>
<div class="last math">
\begin{align*}
A = \frac{d-c}{b-a} \\
B = \frac{ad-bc}{a-b} \tag{19}
\end{align*}
</div>
</div>
<p>Now that we have a method to solve the general problem of finding extrema for a
functional, we can add constraints in the mix.  As you may have guessed, we
can use the concept of
<a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>
here (see my previous post on <a class="reference external" href="../lagrange-multipliers">Lagrange Multipliers</a>).</p>
<p>Given a functional in the form of Equation 12, we can add different types of constraints.
The simplest type of constraint we can have is a functional constraint of the form:</p>
<div class="math">
\begin{equation*}
G[y] = \int_a^b M(x, y, y') dx = C \tag{20}
\end{equation*}
</div>
<p>In this case, the solution resembles the usual method for Lagrange multipliers.
We can solve this problem by building a new functional in the same vein of a
Lagrangian:</p>
<div class="math">
\begin{equation*}
H[y] = \int_a^b (L(x,y,y') - \lambda M(x, y, y')) dx \tag{21}
\end{equation*}
</div>
<p>Using the Euler-Lagrange equation, we can solve Equation 21 for a function
<span class="math">\(y\)</span> and constant <span class="math">\(\lambda\)</span>, keeping in mind we are given boundary
conditions at <span class="math">\(a\)</span> and <span class="math">\(b\)</span> as well as Equation 20 to help us solve
for all the constants.  This method also naturally extends to multiple
constraints as you would expect.</p>
<p>The other type of constraint is just a constraint on the actual function
(similar to regular Lagrange multipliers):</p>
<div class="math">
\begin{equation*}
g(x, y) = 0 \tag{22}
\end{equation*}
</div>
<p>Here, a Lagrange multiplier <em>function</em> needs to be introduced and the Lagrangian
becomes:</p>
<div class="math">
\begin{equation*}
H[y] = \int_a^b (L(x,y,y') - \lambda(x) g(x,y)) dx \tag{23}
\end{equation*}
</div>
<p>Again, we can use the Euler-Lagrange equation to solve Equation 23, except
we'll get a system of differential equations to solve (you need to take the functional
derivative with respect to both <span class="math">\(y(x)\)</span> and <span class="math">\(\lambda(x)\)</span>).</p>
<p>And at long last, we can finally get to solving some interesting problems in
probability!  Let's take a look at a couple of examples of finding
<a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">maximum entropy distributions</a>
under different constraints (check out my previous
post on <a class="reference external" href="../maximum-entropy-distributions">Maximum Entropy Distributions</a>).</p>
<div class="admonition admonition-example-8-find-the-continuous-maximum-entropy-distribution-with-support-math-a-b">
<p class="first admonition-title">Example 8: Find the continuous maximum entropy distribution
with support <span class="math">\([a,b]\)</span>.</p>
<p>This is actually the same example as that appeared in my post on <a class="reference external" href="../maximum-entropy-distributions">Maximum Entropy Distributions</a>, but let's take another look.</p>
<p>First since we're finding the maximum entropy distribution, we define the
<a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#Differential_entropy">differential entropy</a>
functional (we use <span class="math">\(H\)</span> here to denote entropy):</p>
<div class="math">
\begin{equation*}
H[f] := -\int_{a}^{b} f(x)\log(f(x)) dx \tag{24}
\end{equation*}
</div>
<p>Next, we define a functional constraint that our density must sum to 1:</p>
<div class="math">
\begin{equation*}
G[f] := \int_{a}^{b} f(x) dx = 1 \tag{25}
\end{equation*}
</div>
<p>Now put together the Lagrangian equivalent:</p>
<div class="math">
\begin{align*}
F[f] &amp;= \int_{a}^{b} L(x, f(x)) dx \\
     &amp;= \int_{a}^{b} -f(x)\log(f(x)) - \lambda f(x) dx \tag{26}
\end{align*}
</div>
<p>Using the Euler-Lagrange equation to find the maximum and noticing we have
no derivatives of <span class="math">\(f(x)\)</span>, we get:</p>
<div class="math">
\begin{align*}
\frac{\delta L}{\delta f(x)} = -\log(f(x)) - 1 - \lambda &amp;= 0 \\
-\log(f(x)) = 1 + \lambda \\
f(x) = e^{-\lambda - 1} \tag{27}
\end{align*}
</div>
<p>Plugging this into our constraint in Equation 25:</p>
<div class="math">
\begin{align*}
G[f] = \int_{a}^{b} e^{-\lambda - 1} dx &amp;= 1 \\
e^{-\lambda - 1} \int_{a}^{b} dx &amp;= 1 \\
e^{-\lambda - 1} &amp;= \frac{1}{b-a} \tag{28}
\end{align*}
</div>
<p>Now substituting back into Equation 27, we get:</p>
<div class="math">
\begin{equation*}
f(x) = \frac{1}{b-a} \tag{29}
\end{equation*}
</div>
<p class="last">This is nothing more than a uniform distribution on the interval
<span class="math">\([a,b]\)</span>. This means that given no other knowledge of a distribution
(except its support), the principle of maximum entropy says we should
assume it's a uniform distribution.</p>
</div>
<div class="admonition admonition-example-9-find-the-continuous-maximum-entropy-distribution-with-support-math-infty-infty-math-e-x-mu-and-math-e-x-mu-2-sigma-2">
<p class="first admonition-title">Example 9: Find the continuous maximum entropy distribution
with support <span class="math">\([-\infty,\infty]\)</span>, <span class="math">\(E[x] = \mu\)</span> and <span class="math">\(E[(x-\mu)^2] = \sigma^2\)</span>.</p>
<p>You may already be able to guess what kind of distribution we should end up with
when we have the mean and variance specified, let's see if you're right.</p>
<p>We'll just transform the variance constraint into the second moment to make
this a bit more symmetric:</p>
<div class="math">
\begin{align*}
\int_{-\infty}^{\infty} f(x) (x-\mu)^2 dx &amp;= \sigma^2  \\
\int_{-\infty}^{\infty} f(x)x^2 dx - \mu^2 &amp;= \sigma^2  \\
\int_{-\infty}^{\infty} f(x)x^2 dx &amp;= \sigma^2 + \mu^2 \tag{30}
\end{align*}
</div>
<p>Given this objective functional and associated constraints:</p>
<div class="math">
\begin{align*}
H[f] &amp;:= -\int_{-\infty}^{\infty} f(x)\log(f(x)) dx \\
G_0[f] &amp;:= \int_{-\infty}^{\infty} f(x) dx = 1 \\
G_1[f] &amp;:= \int_{-\infty}^{\infty} f(x) x  dx = \mu  \\
G_2[f] &amp;:= \int_{-\infty}^{\infty} f(x) x^2 dx = \sigma^2 + \mu^2 \tag{31}
\end{align*}
</div>
<p>we can put together the Lagrangian functional:</p>
<div class="math">
\begin{align*}
F[f] &amp;:= \int_{-\infty}^{\infty} L(x, f(x)) dx \\
 &amp;= \int_{-\infty}^{\infty} -f(x)\log(f(x))
- \lambda_0 f(x)
- \lambda_1 f(x) x
- \lambda_2 f(x) x^2 dx \tag{32}
\end{align*}
</div>
<p>Using the Euler-Lagrange equation again and setting it to 0:</p>
<div class="math">
\begin{align*}
-\log(f(x)) - 1 - \lambda_0 - \lambda_1 x - \lambda_2 x^2 &amp;= 0 \\
f(x) &amp;= e^{-(1 + \lambda_0 + \lambda_1 x + \lambda_2 x^2)} \tag{33}
\end{align*}
</div>
<p>Now this is not going to work out so nicely in terms of plugging it back into
our constraints from Equation 30 because integrals involving <span class="math">\(e^{x^2}\)</span>
usually don't have nice anti-derivatives.  But one thing to notice is that this is
basically the form of a
<a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_function">Gaussian function</a>
(you'll have to do some legwork to complete the square though):</p>
<div class="math">
\begin{align*}
f(x) &amp;= e^{-(1 + \lambda_0 + \lambda_1 x + \lambda_2 x^2)} \\
     &amp;= ae^{-\frac{(x-b)^2}{2c^2}} \tag{34}
\end{align*}
</div>
<p>Further, we know from the constraints in Equation 31 that the function
is normalized to <span class="math">\(1\)</span>, making this a normal distribution.
Thus we can determine the values of the missing coefficients by just
matching them against the definition of a normal distribution:</p>
<div class="math">
\begin{align*}
a &amp;= \frac{1}{\sqrt{2\pi \sigma^2}} \\
b &amp;= \mu \\
c &amp;= \sigma^2  \tag{35}
\end{align*}
</div>
<p class="last">So by the principle of maximum entropy, if we only know the mean and variance
of a distribution with support along the real line, we should assume the distribution
is normal.</p>
</div>
<p><br></p>
<h4> Conclusion </h4>
<p>For those of us who aren't math or physics majors (<cite>*</cite> <em>cough</em> <cite>*</cite> computer engineers),
variational calculus is an important topic that we missed out on.  Not only
does it have a myriad of applications in physical domains (it's the most common
type of problem when searching for "variational calculus"), it also has many
applications in statistics and machine learning (you can expect a future post
using this topic!).  As with most things, once you know enough about the individual
parts (multivariable calculus, Lagrange multipliers etc.) the actual topic (variational calculus)
isn't too much of a stretch (at least when you're not trying to prove things
formally!).  I hope this post helps all the non-mathematicians and non-physicists
out there.</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li>Previous Posts: <a class="reference external" href="../lagrange-multipliers">Lagrange Multipliers</a>, <a class="reference external" href="../maximum-entropy-distributions">Max Entropy Distributions</a>
</li>
<li>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Calculus_of_variations">Calculus of Variations</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Functional_derivative">Functional Derivative</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Directional_derivative">Directional Derivative</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Differential_of_a_function">Differential of a function</a>,
<a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>
</li>
<li>
<a class="reference external" href="http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx">Directional Derivatives</a>, Paul Dawkins, Paul's Online Math Notes.</li>
<li>
<a class="reference external" href="http://math.stackexchange.com/questions/23902/what-is-the-practical-difference-between-a-differential-and-a-derivative">What is the practical difference between a differential and a derivative?</a>, Arturo Magidin, Math.Stack Exchange.</li>
<li>"<a class="reference external" href="http://julian.tau.ac.il/bqs/functionals/functionals.html">Notes on Functionals</a>", B. Svetitsky</li>
<li>"Advanced Variational Methods In Mechanics", <a class="reference external" href="http://www.colorado.edu/engineering/CAS/courses.d/AVMM.d/AVMM.Ch01.d/AVMM.Ch01.pdf">Chapter 1: Variational Calculus Overview</a>, University of Colorado at Boulder</li>
<li>
<a class="reference external" href="http://www.vgu.edu.vn/fileadmin/pictures/studies/master/compeng/study_subjects/modules/math/notes/chapter-06.pdf">Variational Problems</a>, Vietnamese-German University.</li>
</ul>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>As you have probably guessed, this is the primary reason I'm interested in this area of mathematics.  A lot of popular ML/statistics techniques have the word "variational", which they get because they are somehow related to variational calculus.</td>
</tr></tbody>
</table>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/differentials/" rel="tag">differentials</a></li>
            <li><a class="tag p-category" href="../../categories/entropy/" rel="tag">entropy</a></li>
            <li><a class="tag p-category" href="../../categories/lagrange-multipliers/" rel="tag">lagrange multipliers</a></li>
            <li><a class="tag p-category" href="../../categories/probability/" rel="tag">probability</a></li>
            <li><a class="tag p-category" href="../../categories/variational-calculus/" rel="tag">variational calculus</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../maximum-entropy-distributions/" rel="prev" title="Maximum Entropy Distributions">Previous post</a>
            </li>
            <li class="next">
                <a href="../variational-bayes-and-the-mean-field-approximation/" rel="next" title="Variational Bayes and The Mean-Field Approximation">Next post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
