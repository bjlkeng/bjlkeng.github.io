<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A introduction to maximum entropy distributions.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Maximum Entropy Distributions | Bounded Rationality</title>
<link href="../../assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/posts/maximum-entropy-distributions/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="description" itemprop="description" content="A introduction to maximum entropy distributions.">
<meta name="author" content="Brian Keng">
<link rel="prev" href="../lagrange-multipliers/" title="Lagrange Multipliers" type="text/html">
<link rel="next" href="../the-calculus-of-variations/" title="The Calculus of Variations" type="text/html">
<meta property="og:site_name" content="Bounded Rationality">
<meta property="og:title" content="Maximum Entropy Distributions">
<meta property="og:url" content="http://bjlkeng.github.io/posts/maximum-entropy-distributions/">
<meta property="og:description" content="A introduction to maximum entropy distributions.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-01-27T09:05:00-05:00">
<meta property="article:tag" content="entropy">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="probability">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>

                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Maximum Entropy Distributions</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Brian Keng
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-01-27T09:05:00-05:00" itemprop="datePublished" title="2017-01-27 09:05">2017-01-27 09:05</time></a></p>
            
        <p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This post will talk about a method to find the probability distribution that best
fits your given state of knowledge.  Using the principle of maximum
entropy and some testable information (e.g. the mean), you can find the
distribution that makes the fewest assumptions about your data (the one with maximal
information entropy).  As you may have guessed, this is used often in Bayesian
inference to determine prior distributions and also (at least implicitly) in
natural language processing applications with maximum entropy (MaxEnt)
classifiers (i.e. a multinomial logistic regression).  As usual, I'll go through
some intuition, some math, and some examples.  Hope you find this topic as
interesting as I do!</p>
<!-- TEASER_END -->
<p><br></p>
<h4> Information Entropy and Differential Entropy </h4>
<p>There are plenty of ways to intuitively understand information entropy,
I'll try to describe one that makes sense to me.  If it doesn't quite make
sense to you, I encourage you to find a few different sources until you can
piece together a picture that you can internalize.</p>
<p>Let's first clarify two important points about terminology.  First, information
entropy is a distinct idea from the physics concept of thermodynamic entropy.
There are parallels, and connections have been made between the two ideas, but
it's probably best to initially to treat them as separate things.  Second, the
"information" part refers to information theory, which deals with sending
messages (or symbols) over a channel.  One crucial point for our explanation is
that the "information" (of a data source) is modelled as a probability
distribution.  So everything we talk about is with respect to a probabilistic
model of the data.</p>
<p>Now let's start from the basic idea of <a class="reference external" href="https://en.wikipedia.org/wiki/Self-information#Definition">information</a>.  Wikipedia has a good
article on <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#Rationale">Shannon's rationale</a>
for information, check it out for more details.  I'll simplify it a bit to
pick out the main points.</p>
<p>First, <em>information</em> was originally defined in the context of sending a message
between a transmitter and receiver over a (potentially noisy) channel.
Think about a situation where you are shouting messages to your friend
across a large field.  You are the transmitter, your friend the receiver, and
the channel is this large field.  We can model what your friend is hearing
using probability.</p>
<p>For simplicity, let's say you are only shouting (or transmitting) letters of
the alphabet (A-Z).  We'll also assume that the message always transmits
clearly (if not this will affect your probability distribution by adding noise).
Let's take a look at a couple examples to get a feel for how information works:</p>
<ol class="arabic simple">
<li>Suppose you and your friend agree that you will always shout "A" ahead of
time (or a priori).  So when you actually do start shouting, how much
information is being transmitted?  None, because your friend knows exactly
what you are saying.  This is akin to modelling the probability of receiving
"A" as 1, and all other letters as 0.</li>
<li>Suppose you and your friend agree, a priori, that you will being shouting
letters in order from some English text.  Which letter do you think would
have more information, "E" or "Z"?  Since we know "E" is the most common letter
in the English language, we can usually guess when the next character is an
"E".  So we'll be less surprised when it happens, thus it has a relatively
low amount of information that is being transmitted.  Conversely, "Z" is an
uncommon letter.  So we would probably not guess that it's coming next and be
surprised when it does, thus "Z" conveys more information than "E" in this
situation.  This is akin to modelling a probability distribution over the
alphabet with probabilities proportional to the relative frequencies of letters
occurring in the English language.</li>
</ol>
<p>Another way of describing information is a measure of "surprise".  If you are more
surprised by the result then it has more information.
Based on some desired mathematical properties shown in the box below, we can
generalize this idea to define information as:</p>
<div class="math">
\begin{equation*}
I(p) := \log(1/p) = -\log(p) \tag{1}
\end{equation*}
</div>
<p>The base of the logarithm isn't too important since it will just adjust the
value by a constant.  A usual choice is base 2 which we'll usually call a
"bit", or base <span class="math">\(e\)</span>, which we'll call a "nat".</p>
<div class="admonition admonition-properties-of-information">
<p class="first admonition-title">Properties of Information</p>
<p>The definition of information came about based on certain reasonable
properties we ideally have:</p>
<ol class="last arabic simple">
<li>
<span class="math">\(I(p_i)\)</span> is anti-monotonic - information increases when the probability of an
event decreases, and vice versa.  If something almost always happens (e.g. the
sun will rise tomorrow), then there is no surprise and you really
haven't gained much information; or if something very rarely happens
(e.g. a gigantic earth quake), then you will be surprised and more
information is gained.</li>
<li>
<span class="math">\(I(p_i=0)\)</span> is undefined - for infinitesimally small probability events,
you have a infinitely large amount of information.</li>
<li>
<span class="math">\(I(p_i)\geq 0\)</span> - information is non-negative.</li>
<li>
<span class="math">\(I(p_i=1)=0\)</span> - sure things don't give you any information.</li>
<li>
<span class="math">\(I(p_i, p_j)=I(p_i) + I(p_j)\)</span> - for independent events <span class="math">\(i\)</span> and
<span class="math">\(j\)</span>, information should be additive.  That is, getting the information
(for independent events) together, or separately, should be the same.</li>
</ol>
</div>
<p>Now that we have an idea about the information of a single event, we can define
entropy in the context of a probability distribution (over a set of events).
For a given discrete probability distribution for random variable <span class="math">\(X\)</span>,
we define entropy of <span class="math">\(X\)</span> (denoted by <span class="math">\(H(X)\)</span>) as the expected value
of the information of <span class="math">\(X\)</span>:</p>
<div class="math">
\begin{align*}
H(X) := E[I(X)] &amp;= \sum_{i=1}^n P(x_i)I(x_i) \\
&amp;= \sum_{i=1}^n p_i \log(1/p_i) \\
&amp;= -\sum_{i=1}^n p_i \log(p_i) \tag{2}
\end{align*}
</div>
<p>Eh voila!  The usual (non-intuitive) definition of entropy we all know and
love.  Note: When any of the probabilities are <span class="math">\(p_i=0\)</span>, you replace
<span class="math">\(0\log(0)\)</span> with <span class="math">\(0\)</span>, which is consistent with the limit as
<span class="math">\(p\)</span> approaches to 0 from the right.</p>
<p>Entropy, then, is the <em>average</em> amount of information or surprise for an event
in a probability distribution.  Going back to our example above, when
transmitting only "A"s, the information transmitted is 0 (because
<span class="math">\(P(\text{"A"})=1\)</span> and <span class="math">\(0\)</span> for other letters), so the entropy is naturally
0.  When transmitting English text, the entropy will be
the average entropy using <a class="reference external" href="https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_the_English_language">letter frequencies</a>
<a class="footnote-reference" href="#id2" id="id1">[1]</a>.</p>
<div class="admonition admonition-example-1-entropy-of-a-fair-coin">
<p class="first admonition-title">Example 1: Entropy of a fair coin.</p>
<p>For a random variable X corresponding to the toss of a fair coin we have,
<span class="math">\(P(X=H)=p\)</span> and <span class="math">\(P(X=T)=1-p\)</span> with <span class="math">\(p=0.5\)</span>.  Using Equation
2 (using base 2):</p>
<div class="math">
\begin{align*}
H(X) &amp;= p\log_2(1/p) + (1-p)\log_2(1/p) \\
     &amp;= \log_2(1/p) \\
     &amp;= \log_2(1/0.5) \\
     &amp;= 1 \tag{3}
\end{align*}
</div>
<p>So one bit of information is transmitted with every observation of a fair coin toss.
If we vary the value of <span class="math">\(p\)</span>, we get a symmetric curve shown in Figure
1.  The more biased towards H or T, the less entropy (information/surprise)
we get (on average).</p>
<div class="last figure align-center">
<img alt="Entropy with varying :math:`p` (source: Wikipedia)" src="../../images/binary_entropy.png" style="height: 300px;"><p class="caption">Figure 1: Entropy with varying <span class="math">\(p\)</span> (source: Wikipedia)</p>
</div>
</div>
<p>A continuous analogue to (discrete) entropy is called <em>differential entropy</em>
(or continuous entropy).  It has a very similar equation using integrals
instead of sums:</p>
<div class="math">
\begin{equation*}
H(X) := - \int_{-\infty}^{\infty} p(x)\log(p(x)) dx \tag{4}
\end{equation*}
</div>
<p>where it is understood that <span class="math">\(p(x)\log(p(x))=0\)</span> when <span class="math">\(p(x)=0\)</span>.  We
have to be careful with differential entropy because some of the properties of
(discrete) entropy do not apply to differential entropy, for example,
differential entropy can be negative.</p>
<p><br></p>
<h4> Principle of Maximum Entropy </h4>
<p>The <em>principle of maximum entropy</em> states that given precisely stated prior data,
the probability distribution that best represents the current state of knowledge
is the one with the largest (information) entropy.  In other words, if we only
know certain statistics about the distribution, such as its mean, then this
principle tells us that the best distribution to use is the one with the most
surprise (more surprise, means fewer of your assumptions were satisfied).  This
rule can be thought of expressing epistemic modesty, or maximal ignorance,
because it makes the least strong claim on a distribution beyond being informed
by the prior data.</p>
<p>The precisely stated prior data should be in a testable form, which just means
that given a probability distribution you say whether the statement is true or
false.  The most common examples are moments of a distribution such as the
expected value or variance of a distribution, along with its support.</p>
<p>In terms of solving for these maximum entropy distributions, we can usually
formulate it as maximizing a function (entropy) in the presence of multiple
constraints.  This is typically solved using <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a> (see my <a class="reference external" href="../lagrange-multipliers">previous post</a>).  Let's take a look at a bunch of
examples to get a feel for how this works.</p>
<div class="admonition admonition-example-2-discrete-probability-distribution-with-support-math-a-a-1-ldots-b-1-b-with-math-b-a-and-math-a-b-in-mathbb-z">
<p class="first admonition-title">Example 2: Discrete Probability distribution with support
<span class="math">\(\{a, a+1, \ldots, b-1, b\}\)</span>
with <span class="math">\(b &gt; a\)</span> and <span class="math">\(a,b \in \mathbb{Z}\)</span>.</p>
<p>First the function we're maximizing:</p>
<div class="math">
\begin{equation*}
H(x) = - \sum_{i=a}^{b} p_i\log(p_i)   \tag{5}
\end{equation*}
</div>
<p>Next our constraints, which in this case is just our usual rule of probabilities
summing to 1:</p>
<div class="math">
\begin{equation*}
\sum_{i=a}^{b} p_i = 1   \tag{6}
\end{equation*}
</div>
<p>Using Lagrange multipliers, we can solve the Lagrangian by taking its
partial derivatives and setting them to zero:</p>
<div class="math">
\begin{align*}
\mathcal{L}(p_a, \ldots, p_b, \lambda) &amp;= -\sum_{i=a}^{b} p_i\log(p_i)
        - \lambda(\sum_{i=a}^{b} p_i - 1) \tag{7} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\frac{\partial \mathcal{L}(p_a, \ldots, p_b, \lambda)}{\partial p_i} &amp;= 0 \\
-\log(p_i) -1 -\lambda &amp;= 0 \tag{8} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\frac{\partial \mathcal{L}(p_a, \ldots, p_b, \lambda)}{\partial \lambda} &amp;= 0 \\
- \sum_{i=a}^{b} p_i + 1 &amp;= 0 \tag{9}
\end{align*}
</div>
<p>Solving for <span class="math">\(p_i\)</span> and <span class="math">\(\lambda\)</span>:</p>
<div class="math">
\begin{align*}
p_i &amp;= \frac{1}{b-a+1} \\
\lambda &amp;= \lg(b-a+1) -1 \tag{10}
\end{align*}
</div>
<p class="last">So given no information about a discrete distribution, the maximal entropy distribution
is just a uniform distribution.  This matches with Laplace's <a class="reference external" href="https://en.wikipedia.org/wiki/Principle_of_indifference">principle of
indifference</a> which
states that given mutually exclusive and exhaustive indistinguishable
possibilities, each possibility should be assigned equal probability of
<span class="math">\(\frac{1}{n}\)</span>.</p>
</div>
<div class="admonition admonition-example-3-jaynes-dice-https-arxiv-org-abs-1408-6803">
<p class="first admonition-title">Example 3: <a class="reference external" href="https://arxiv.org/abs/1408.6803">Jaynes' Dice</a></p>
<blockquote>
A die has been tossed a very large number N of times, and we are told
that the average number of spots per toss was not 3.5, as we might
expect from an honest die, but 4.5. Translate this information into
a probability assignment <span class="math">\(p_n, n = 1, 2, \ldots, 6\)</span>, for the
<span class="math">\(n\)</span>-th face to come up on the next toss.</blockquote>
<p>This problem is similar to the above except for two changes:
our support is <span class="math">\(\{1,\ldots,6\}\)</span> and the expectation of the die roll is
<span class="math">\(4.5\)</span>.  We can formulate the problem in a similar way with the following
Lagrangian with an added term for the expected value (<span class="math">\(B\)</span>):</p>
<div class="math">
\begin{equation*}
\mathcal{L}(p_1, \ldots, p_6, \lambda_0, \lambda_1) =
    -\sum_{k=1}^{6} p_k\log(p_k)
    - \lambda_0(\sum_{k=1}^{6} p_k - 1)
    - \lambda_1(\sum_{k=1}^{6} k p_k - B)
    \tag{11}
\end{equation*}
</div>
<p>Taking the partial derivatives and setting them to zero, we get:</p>
<div class="math">
\begin{align*}
\log(p_k) = - 1 - \lambda_0 - k\lambda_1 &amp;= 0 \\
\log(p_k) &amp;= - 1 - \lambda_0 - k\lambda_1 \\
p_k &amp;= e^{- 1 - \lambda_0 - k\lambda_1} \tag{12} \\
\sum_{k=1}^{6} p_k &amp;= 1 \tag{13} \\
\sum_{k=1}^{6} k p_k &amp;= B \tag{14}
\end{align*}
</div>
<p>Define a new quantity <span class="math">\(Z(\lambda_1)\)</span> by substituting Equation 12 into 13:</p>
<div class="math">
\begin{equation*}
Z(\lambda_1) := e^{-1-\lambda_0} = \frac{1}{\sum_{k=1}^6 e^{-k\lambda_1}} \tag{15}
\end{equation*}
</div>
<p>Substituting Equation 12, and dividing Equation 14 by 13</p>
<div class="math">
\begin{align*}
\frac{\sum_{k=1}^{6} k e^{- 1 - \lambda_0 - k\lambda_1}}{\sum_{k=1}^{6} e^{- 1 - \lambda_0 - k\lambda_1}} =&amp; B \\
\frac{\sum_{k=1}^{6} k e^{- k\lambda_1}}{\sum_{k=1}^{6} e^{- k\lambda_1}} =&amp; B \tag{16}
\end{align*}
</div>
<p>Going back to Equation 12 and defining it in terms of <span class="math">\(Z\)</span>:</p>
<div class="math">
\begin{equation*}
p_k = \frac{1}{Z(\lambda_1)}e^{- k\lambda_1} \tag{17}
\end{equation*}
</div>
<p>Unfortunately, now we're at an impasse because there is no closed form solution.
Interesting to note that the solution is just an exponential-like distribution
with parameter <span class="math">\(\lambda_1\)</span> and <span class="math">\(Z(\lambda_1)\)</span> as a
normalization constant to make sure the probabilities sum to 1.  Equation 16
gives us the desired value of <span class="math">\(\lambda_1\)</span>.  We can easily find a solution
using any root solver, such as the code below:</p>
<pre class="code python"><a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-1"></a><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">exp</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-2"></a><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">newton</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-3"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-4"></a><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">4.5</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-5"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-6"></a><span class="c1"># Equation 15</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-7"></a><span class="k">def</span> <span class="nf">z</span><span class="p">(</span><span class="n">lamb</span><span class="p">):</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-8"></a>    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k</span><span class="o">*</span><span class="n">lamb</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-9"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-10"></a><span class="c1"># Equation 16</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-11"></a><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">lamb</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">B</span><span class="p">):</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-12"></a>    <span class="n">y</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k</span><span class="o">*</span><span class="n">lamb</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-13"></a>    <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">(</span><span class="n">lamb</span><span class="p">)</span> <span class="o">-</span> <span class="n">B</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-14"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-15"></a><span class="c1"># Equation 17</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-16"></a><span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lamb</span><span class="p">):</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-17"></a>    <span class="k">return</span> <span class="n">z</span><span class="p">(</span><span class="n">lamb</span><span class="p">)</span> <span class="o">*</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k</span> <span class="o">*</span> <span class="n">lamb</span><span class="p">)</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-18"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-19"></a><span class="n">lamb</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-20"></a><span class="k">print</span><span class="p">(</span><span class="s2">"Lambda = </span><span class="si">%.4f</span><span class="s2">"</span> <span class="o">%</span> <span class="n">lamb</span><span class="p">)</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-21"></a><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-22"></a>    <span class="k">print</span><span class="p">(</span><span class="s2">"p_</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%.4f</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">lamb</span><span class="p">)))</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-23"></a>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-24"></a><span class="c1"># Output:</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-25"></a><span class="c1">#   Lambda = -0.3710</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-26"></a><span class="c1">#   p_1 = 0.0544</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-27"></a><span class="c1">#   p_2 = 0.0788</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-28"></a><span class="c1">#   p_3 = 0.1142</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-29"></a><span class="c1">#   p_4 = 0.1654</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-30"></a><span class="c1">#   p_5 = 0.2398</span>
<a name="rest_code_4d9494c125e941d39c5d30cdbe43e912-31"></a><span class="c1">#   p_6 = 0.3475</span>
</pre>
<p class="last">The distribution is skewed much more towards <span class="math">\(6\)</span>.  If you re-run the
program with <span class="math">\(B=3.5\)</span>, you'll get a uniform distribution, which is
what we would expect from a fair die.</p>
</div>
<div class="admonition admonition-example-4-continuous-probability-distribution-with-support-math-a-b-with-math-b-a">
<p class="first admonition-title">Example 4: Continuous probability distribution with support
<span class="math">\([a, b]\)</span> with <span class="math">\(b &gt; a\)</span>.</p>
<p>This is the continuous analogue to Example 2, so we'll use differential entropy
instead of the discrete version along with the corresponding probability
constraint of summing to <span class="math">\(1\)</span> (<span class="math">\(p(x)\)</span> is our density function):</p>
<div class="math">
\begin{align*}
H(x) = &amp;- \int_{a}^{b} p(x)\log(p(x))dx \tag{18} \\
&amp;\int_{a}^{b} p(x) dx = 1   \tag{19}
\end{align*}
</div>
<p>Gives us the continuous analogue to the Lagrangian:</p>
<div class="math">
\begin{equation*}
\mathcal{L}(p(x), \lambda) = -\int_{a}^{b} p(x)\log(p(x)) dx
        - \lambda\big(\int_{a}^{b} p(x)dx - 1\big) \tag{20}
\end{equation*}
</div>
<p>Notice that the problem is different from Example 1: we're trying to find
<em>a function</em> that maximizes Equation 20, not just a discrete set of values.
To solve this, we have to use the
<a class="reference external" href="https://en.wikipedia.org/wiki/Calculus_of_variations">calculus of variations</a>,
which basically is the analogue to the value-maximization mathematics of regular
calculus.</p>
<p>Describing variational calculus is a bit beyond the scope of this post
(that's for next time!) but in this specific case, it turns out the equations
look almost identical to Example 2.  Taking the partial functional
derivatives of Equation 20 and solving for the function:</p>
<div class="math">
\begin{align*}
\frac{\partial \mathcal{L}(p(x), \lambda)}{\partial p(x)} &amp;= 0 \\
\log(p(x)) &amp;= - 1 - \lambda \\
p(x) &amp;= e^{- 1 - \lambda} \tag{22} \\
\end{align*}
</div>
<div class="math">
\begin{align*}
\frac{\partial \mathcal{L}(p(x), \lambda)}{\partial \lambda} &amp;= 0 \\
\int_{a}^{b} p(x) dx &amp;= 1 \\
e^{-1 - \lambda} \int_{a}^{b} dx &amp;= 1 \\
p(x) &amp;= e^{-1 - \lambda} = \frac{1}{b-a} \tag{23}
\end{align*}
</div>
<p class="last">So no surprises here, we get a uniform distribution on the interval
<span class="math">\([a,b]\)</span>, analogous to the discrete version.</p>
</div>
<p>Wikipedia has a table of some common <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution#Other_examples">maximum entropy distributions</a>,
here are few you might encounter:</p>
<ul class="simple">
<li>Support <span class="math">\(\{0, 1\}\)</span> with <span class="math">\(E(x)=p\)</span>: Bernoulli distribution</li>
<li>Support <span class="math">\(\{1, 2, 3, \ldots\}\)</span> with <span class="math">\(E(x)=\frac{1}{p}\)</span>: geometric distribution</li>
<li>Support <span class="math">\((0, \infty)\)</span> with <span class="math">\(E(x)=b\)</span>: exponential distribution.</li>
<li>Support <span class="math">\((-\infty, \infty)\)</span> with <span class="math">\(E(|x-\mu|)=b\)</span>: Laplacian distribution</li>
<li>Support <span class="math">\((-\infty, \infty)\)</span> with <span class="math">\(E(x)=\mu, Var(x)=\sigma^2\)</span>: normal distribution</li>
<li>Support <span class="math">\((0, \infty)\)</span> with <span class="math">\(E(\log(x))=\mu, E((\log(x) - \mu)^2)=\sigma^2\)</span>: lognormal distribution</li>
</ul>
<p><br></p>
<h4> Conclusion </h4>
<p>The maximum entropy distribution is a very nice concept: if you don't know
anything except for the stated data, assume the least informative distribution.
Practically, it can be used for Bayesian priors but on a more philosophical
note the idea has been used by Jaynes to show that thermodynamic entropy (in
statistical mechanics) is the same concept as information entropy.  Even though
it's controversial, it's kind of reassuring to note that nature <em>may</em> be
Bayesian.  I don't know about you but this somehow makes me sleep more soundly
at night :)</p>
<p><br></p>
<h4> Further Reading </h4>
<ul class="simple">
<li>Wikipedia: <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Maximum Entropy Probability Distribution</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Principle of Maximum Entropy</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy</a>, <cite>Self-Information &lt;https://en.wikipedia.org/wiki/Self-information#Definition&gt;</cite>
</li>
<li>"The Brandeis Dice Problem &amp; Statistical Mechanics", Steven J. van Enk., <a class="reference external" href="https://arxiv.org/pdf/1408.6803">arxiv 1408.6803</a>.</li>
</ul>
<p><br></p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>This isn't exactly right because beyond the letter frequencies, we also can predict what the word is, which will change the information and entropy.  Natural language also has redundancies such as "q must always be followed by u", so this will change our probability distribution.  See <a class="reference external" href="http://people.seas.harvard.edu/~jones/cscie129/papers/stanford_info_paper/entropy_of_english_9.htm">Entropy and Redundancy in English</a> for more details.</td>
</tr></tbody>
</table>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/entropy/" rel="tag">entropy</a></li>
            <li><a class="tag p-category" href="../../categories/probability/" rel="tag">probability</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../lagrange-multipliers/" rel="prev" title="Lagrange Multipliers">Previous post</a>
            </li>
            <li class="next">
                <a href="../the-calculus-of-variations/" rel="next" title="The Calculus of Variations">Next post</a>
            </li>
        </ul></nav></aside><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script></article>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href="../../">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="../../archive.html">Archive</a>
            </p>
            <p>
            <a href="../../categories/index.html">Tags</a>
            </p>
            <p>
            <a href="../../rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/bootstrap.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><script src="../../assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
