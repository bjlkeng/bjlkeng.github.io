<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Understanding math, machine learning, and data to a satisfactory degree.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Bounded Rationality | Bounded Rationality</title>
<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/a-note-on-using-log-likelihood-for-generative-models/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                

<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/a-note-on-using-log-likelihood-for-generative-models/" class="u-url">A Note on Using Log-Likelihood for Generative Models</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/a-note-on-using-log-likelihood-for-generative-models/" rel="bookmark"><time class="published dt-published" datetime="2019-08-27T07:50:09-04:00" title="2019-08-27 07:50">2019-08-27 07:50</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>One of the things that I find is usually missing from many ML papers is how
they relate to the fundamentals.  There's always a throwaway line where it
assumes something that is not at all obvious (see my post on
<a class="reference external" href="posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders">Importance Sampling</a>).  I'm the kind of person who likes to
understand things to a satisfactory degree (it's literally in the subtitle of
the blog) so I couldn't help myself investigating a minor idea I read about in
a paper.</p>
<p>This post investigates how to use continuous density outputs (e.g. a logistic
or normal distribution) to model discrete image data (e.g. 8-bit RGB values).
It seems like it might be something obvious such as setting the loss as the
average log-likelihood of the continuous density and that's <em>almost</em> the
whole story.  But leaving it at that skips over so many (interesting) and
non-obvious things that you would never know if you didn't bother to look.  I'm
a curious fellow so come with me and let's take a look!</p>
<p class="more"><a href="posts/a-note-on-using-log-likelihood-for-generative-models/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/pixelcnn/" class="u-url">PixelCNN</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/pixelcnn/" rel="bookmark"><time class="published dt-published" datetime="2019-07-22T07:11:09-04:00" title="2019-07-22 07:11">2019-07-22 07:11</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>It's been a long time coming but I'm finally getting this post out!  I read
this paper a couple of years ago and wanted to really understand it because it
was state of the art at the time (still pretty close even now).  As usual
though, once I started down the variational autoencoder line of posts, there
was always <em>yet</em> another VAE paper to look into so I never got around to
looking at this one.</p>
<p>This post is all about a proper probabilistic generative model called Pixel
Convolutional Neural Networks or PixelCNN.  It was originally proposed
as a side contribution of Pixel Recurrent Neural Networks in [1] and later
expanded upon in [2,3] (and I'm sure many other papers).  The real cool thing
about it is that it's (a) probabilistic, and (b) autoregressive.  It's still
counter-intuitive to me that you can generate images one pixel at at time, but
I'm jumping ahead of myself here.  We'll go over some background material, the
method, and my painstaking attempts at an implementation (and what I learned
from it).  Let's get started!</p>
<p class="more"><a href="posts/pixelcnn/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/" class="u-url">Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/" rel="bookmark"><time class="published dt-published" datetime="2019-02-06T08:20:11-04:00" title="2019-02-06 08:20">2019-02-06 08:20</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>It took a while but I'm back!  This post is kind of a digression (which seems
to happen a lot) along my journey of learning more about probabilistic
generative models.  There's so much in ML that you can't help learning a lot
of random things along the way.  That's why it's interesting, right?</p>
<p>Today's topic is <em>importance sampling</em>.  It's a really old idea that you may
have learned in a statistics class (I didn't) but somehow is useful in deep learning,
what's old is new right?  How this is relevant to the discussion is that when
we have a large latent variable model (e.g. a variational
autoencoder), we want to be able to efficiently estimate the marginal likelihood
given data.  The marginal likelihood is kind of taken for granted in the
experiments of some VAE papers when comparing different models.  I was curious
how it was actually computed and it took me down this rabbit hole.  Turns out
it's actually pretty interesting!  As usual, I'll have a mix of background
material, examples, math and code to build some intuition around this topic.
Enjoy!</p>
<p class="more"><a href="posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/label-refinery/" class="u-url">Label Refinery: A Softer Approach</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/label-refinery/" rel="bookmark"><time class="published dt-published" datetime="2018-09-04T07:26:02-04:00" title="2018-09-04 07:26">2018-09-04 07:26</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to be about a really simple idea that is surprisingly effective
from a paper by Bagherinezhad et al. called <a class="reference external" href="https://arxiv.org/abs/1805.02641">Label Refinery: Improving ImageNet
Classification through Label Progression</a>.
The title pretty much says it all but I'll also discuss some intuition and show
some experiments on the CIFAR10 and SVHN datasets.  The idea is both simple and
surprising, my favourite kind of idea!  Let's take a look.</p>
<p class="more"><a href="posts/label-refinery/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/universal-resnet-the-one-neuron-approximator/" class="u-url">Universal ResNet: The One-Neuron Approximator</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/universal-resnet-the-one-neuron-approximator/" rel="bookmark"><time class="published dt-published" datetime="2018-08-03T08:03:28-04:00" title="2018-08-03 08:03">2018-08-03 08:03</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p><em>"In theory, theory and practice are the same. In practice, they are not."</em></p>
<p>I read a very interesting paper titled <em>ResNet with one-neuron hidden layers is
a Universal Approximator</em> by Lin and Jegelka [1].
The paper describes a simplified Residual Network as a universal approximator,
giving some theoretical backing to the wildly successful ResNet architecture.
In this post, I'm going to talk about this paper and a few of the related
universal approximation theorems for neural networks.
Instead of going through all the theoretical stuff, I'm simply going introduce
some theorems and play around with some toy datasets to see if we can get close
to the theoretical limits.</p>
<p>(You might also want to checkout my previous post where I played around with
ResNets: <a class="reference external" href="posts/residual-networks">Residual Networks</a>)</p>
<p class="more"><a href="posts/universal-resnet-the-one-neuron-approximator/">Read more…</a></p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href=".">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="archive.html">Archive</a>
            </p>
            <p>
            <a href="categories/index.html">Tags</a>
            </p>
            <p>
            <a href="rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/jquery.min.js"></script><script src="assets/js/bootstrap.min.js"></script><script src="assets/js/moment-with-locales.min.js"></script><script src="assets/js/fancydates.js"></script><script src="assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
