<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Understanding math, machine learning, and data to a satisfactory degree.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Bounded Rationality | Bounded Rationality</title>
<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/tensors-tensors-tensors/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                

<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/tensors-tensors-tensors/" class="u-url">Tensors, Tensors, Tensors</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/tensors-tensors-tensors/" rel="bookmark"><time class="published dt-published" datetime="2018-03-13T08:24:57-05:00" title="2018-03-13 08:24">2018-03-13 08:24</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to take a step back from some of the machine learning
topics that I've been writing about recently and go back to some basics: math!
In particular, tensors.  This is a topic that is casually mentioned in machine
learning papers but for those of us who weren't physics or math majors
(*cough* computer engineers), it's a bit murky trying to understand what's going on.
So on my most recent vacation, I started reading a variety of sources on the
interweb trying to piece together a picture of what tensors were all
about.  As usual, I'll skip the heavy formalities (partly because I probably
couldn't do them justice) and instead try to explain the intuition using my
usual approach of examples and more basic maths.  I'll sprinkle in a bunch of
examples and also try to relate it back to ML where possible.  Hope you like
it!</p>
<p class="more"><a href="posts/tensors-tensors-tensors/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/residual-networks/" class="u-url">Residual Networks</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/residual-networks/" rel="bookmark"><time class="published dt-published" datetime="2018-02-18T13:55:13-05:00" title="2018-02-18 13:55">2018-02-18 13:55</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>Taking a small break from some of the heavier math, I thought I'd write a post
(aka learn more about) a very popular neural network architecture called
Residual Networks aka ResNet.  This architecture is being very widely used
because it's so simple yet so powerful at the same time.  The architecture's
performance is due its ability to add hundreds of layers (talk about deep
learning!) without degrading performance or adding difficulty to training.  I
really like these types of robust advances where it doesn't require fiddling
with all sorts of hyper-parameters to make it work.  Anyways, I'll introduce
the idea and show an implementation of ResNet on a few runs of a variational
autoencoder that I put together on the CIFAR10 dataset.</p>
<p class="more"><a href="posts/residual-networks/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/variational-autoencoders-with-inverse-autoregressive-flows/" class="u-url">Variational Autoencoders with Inverse Autoregressive Flows</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/variational-autoencoders-with-inverse-autoregressive-flows/" rel="bookmark"><time class="published dt-published" datetime="2017-12-19T08:47:38-05:00" title="2017-12-19 08:47">2017-12-19 08:47</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>In this post, I'm going to be describing a really cool idea about how
to improve variational autoencoders using inverse autoregressive
flows.  The main idea is that we can generate more powerful posterior
distributions compared to a more basic isotropic Gaussian by applying a
series of invertible transformations.  This, in theory, will allow
your variational autoencoder to fit better by concentrating the
stochastic samples around a closer approximation to the true
posterior.  The math works out so nicely while the results are kind of
marginal <a class="footnote-reference" href="posts/variational-autoencoders-with-inverse-autoregressive-flows/#id3" id="id1">[1]</a>.  As usual, I'll go through some intuition, some math,
and have an implementation with few experiments I ran.  Enjoy!</p>
<p class="more"><a href="posts/variational-autoencoders-with-inverse-autoregressive-flows/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/autoregressive-autoencoders/" class="u-url">Autoregressive Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/autoregressive-autoencoders/" rel="bookmark"><time class="published dt-published" datetime="2017-10-14T10:02:15-04:00" title="2017-10-14 10:02">2017-10-14 10:02</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>You might think that I'd be bored with autoencoders by now but I still
find them extremely interesting!  In this post, I'm going to be explaining
a cute little idea that I came across in the paper <a class="reference external" href="https://arxiv.org/pdf/1502.03509.pdf">MADE: Masked Autoencoder
for Distribution Estimation</a>.
Traditional autoencoders are great because they can perform unsupervised
learning by mapping an input to a latent representation.  However, one
drawback is that they don't have a solid probabilistic basis
(of course there are other variants of autoencoders that do, see previous posts
<a class="reference external" href="posts/variational-autoencoders">here</a>,
<a class="reference external" href="posts/a-variational-autoencoder-on-the-svnh-dataset">here</a>, and
<a class="reference external" href="posts/semi-supervised-learning-with-variational-autoencoders">here</a>).
By using what the authors define as the <em>autoregressive property</em>, we can
transform the traditional autoencoder approach into a fully probabilistic model
with very little modification! As usual, I'll provide some intuition, math and
an implementation.</p>
<p class="more"><a href="posts/autoregressive-autoencoders/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/semi-supervised-learning-with-variational-autoencoders/" class="u-url">Semi-supervised Learning with Variational Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/semi-supervised-learning-with-variational-autoencoders/" rel="bookmark"><time class="published dt-published" datetime="2017-09-11T08:40:47-04:00" title="2017-09-11 08:40">2017-09-11 08:40</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>In this post, I'll be continuing on this variational autoencoder (VAE) line of
exploration
(previous posts: <a class="reference external" href="posts/variational-autoencoders">here</a> and
<a class="reference external" href="posts/a-variational-autoencoder-on-the-svnh-dataset">here</a>) by
writing about how to use variational autoencoders to do semi-supervised
learning.  In particular, I'll be explaining the technique used in
"Semi-supervised Learning with Deep Generative Models" by Kingma et al.
I'll be digging into the math (hopefully being more explicit than the paper),
giving a bit more background on the variational lower bound, as well as
my usual attempt at giving some more intuition.
I've also put some notebooks on Github that compare the VAE methods
with others such as PCA, CNNs, and pre-trained models.  Enjoy!</p>
<p class="more"><a href="posts/semi-supervised-learning-with-variational-autoencoders/">Read more…</a></p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-1.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
            </script>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href=".">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="archive.html">Archive</a>
            </p>
            <p>
            <a href="categories/index.html">Tags</a>
            </p>
            <p>
            <a href="rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/jquery.min.js"></script><script src="assets/js/bootstrap.min.js"></script><script src="assets/js/moment-with-locales.min.js"></script><script src="assets/js/fancydates.js"></script><script src="assets/js/jquery.colorbox-min.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
