<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Understanding math, machine learning, and data to a satisfactory degree.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Bounded Rationality (old posts, page 3) | Bounded Rationality</title>
<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/index-3.html">
<link rel="prev" href="index-2.html" type="text/html">
<link rel="next" href="index-4.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                

<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/lagrange-multipliers/" class="u-url">Lagrange Multipliers</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/lagrange-multipliers/" rel="bookmark"><time class="published dt-published" datetime="2016-12-13T07:48:31-05:00" title="2016-12-13 07:48">2016-12-13 07:48</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to be about finding the maxima or minima of a function
subject to some constraints.  This is usually introduced in a multivariate
calculus course, unfortunately (or fortunately?) I never got the chance to take
a multivariate calculus course that covered this topic.  In my undergraduate class, computer
engineers only took three half year engineering calculus courses, and the
<a class="reference external" href="http://www.ucalendar.uwaterloo.ca/1617/COURSE/course-ECE.html#ECE206">fourth one</a>
(for electrical engineers) seems to have covered other basic multivariate
calculus topics such as all the various theorems such as Green's, Gauss', Stokes' (I
could be wrong though, I never did take that course!).  You know what I always imagined Newton
saying, "It's never too late to learn multivariate calculus!".</p>
<p>In that vein, this post will discuss one widely used method for finding optima
subject to constraints: Lagrange multipliers.  The concepts
behind it are actually quite intuitive once we come up with the right analogue
in physical reality, so as usual we'll start there.  We'll work through some
problems and hopefully by the end of this post, this topic won't seem as
mysterious anymore <a class="footnote-reference" href="posts/lagrange-multipliers/#id3" id="id1">[1]</a>.</p>
<p class="more"><a href="posts/lagrange-multipliers/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/the-expectation-maximization-algorithm/" class="u-url">The Expectation-Maximization Algorithm</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/the-expectation-maximization-algorithm/" rel="bookmark"><time class="published dt-published" datetime="2016-10-07T08:47:47-04:00" title="2016-10-07 08:47">2016-10-07 08:47</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to talk about a widely used method to find the
maximum likelihood (MLE) or maximum a posteriori (MAP) estimate of parameters
in latent variable models called the Expectation-Maximization algorithm.  You
have probably heard about the most famous variant of this algorithm called the
k-means algorithm for clustering.
Even though it's so ubiquitous, whenever I've tried to understand <em>why</em> this
algorithm works, I never quite got the intuition right.  Now that I've taken
the time to work through the math, I'm going to <em>attempt</em> to explain the
algorithm hopefully with a bit more clarity.  We'll start by going back to the
basics with latent variable models and the likelihood functions, then moving on
to showing the math with a simple Gaussian mixture model <a class="footnote-reference" href="posts/the-expectation-maximization-algorithm/#id5" id="id1">[1]</a>.</p>
<p class="more"><a href="posts/the-expectation-maximization-algorithm/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/probabilistic-interpretation-of-regularization/" class="u-url">A Probabilistic Interpretation of Regularization</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/probabilistic-interpretation-of-regularization/" rel="bookmark"><time class="published dt-published" datetime="2016-08-29T08:52:33-04:00" title="2016-08-29 08:52">2016-08-29 08:52</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to look at a probabilistic (Bayesian) interpretation of
regularization.  We'll take a look at both L1 and L2 regularization in the
context of ordinary linear regression.  The discussion will start off
with a quick introduction to regularization, followed by a back-to-basics
explanation starting with the maximum likelihood estimate (MLE), then on to the
maximum a posteriori estimate (MAP), and finally playing around with priors to
end up with L1 and L2 regularization.</p>
<p class="more"><a href="posts/probabilistic-interpretation-of-regularization/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/beyond-collaborative-filtering/" class="u-url">Beyond Collaborative Filtering</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/beyond-collaborative-filtering/" rel="bookmark"><time class="published dt-published" datetime="2016-06-11T18:00:34-04:00" title="2016-06-11 18:00">2016-06-11 18:00</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>I wrote a couple of posts about some of the work on recommendation systems and
collaborative filtering that we're doing at my job as a Data Scientist at
<a class="reference external" href="http://www.rubikloud.com">Rubikloud</a>:</p>
<ul class="simple">
<li><a class="reference external" href="http://rubikloud.com/labs/data-science/beyond-collaborative-filtering/">Beyond Collaborative Filtering (Part 1)</a></li>
<li><a class="reference external" href="http://rubikloud.com/labs/data-science/beyond-collaborative-filtering-part-2/">Beyond Collaborative Filtering (Part 2)</a></li>
</ul>
<p>Here's a blurb:</p>
<blockquote>
Here at Rubikloud, a big focus of our data science team is empowering retailers
in delivering personalized one-to-one communications with their customers. A
big aspect of personalization is recommending products and services that are
tailored to a customer’s wants and needs. Naturally, recommendation systems are
an active research area in machine learning with practical large scale
deployments from companies such as Netflix and Spotify. In Part 1 of this
series, I’ll describe the unique challenges that we have faced in building a
retail specific product recommendation system and outline one of the main
components of our recommendation system: a collaborative filtering algorithm.
In Part 2, I’ll follow up with several useful applications of collaborative
filtering and end by highlighting some of its limitations.</blockquote>
<p>Hope you like it!</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/a-probabilistic-view-of-regression/" class="u-url">A Probabilistic View of Linear Regression</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/a-probabilistic-view-of-regression/" rel="bookmark"><time class="published dt-published" datetime="2016-05-14T20:43:05-04:00" title="2016-05-14 20:43">2016-05-14 20:43</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>One thing that I always disliked about introductory material to linear
regression is how randomness is explained.  The explanations always
seemed unintuitive because, as I have frequently seen it, they appear as an
after thought rather than the central focus of the model.
In this post, I'm going to try to
take another approach to building an ordinary linear regression model starting
from a probabilistic point of view (which is pretty much just a Bayesian view).
After the general idea is established, I'll modify the model a bit and end up
with a Poisson regression using the exact same principles showing how
generalized linear models aren't any more complicated.  Hopefully, this will
help explain the "randomness" in linear regression in a more intuitive way.</p>
<p class="more"><a href="posts/a-probabilistic-view-of-regression/">Read more…</a></p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-2.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-4.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
            </script>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href=".">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="archive.html">Archive</a>
            </p>
            <p>
            <a href="categories/index.html">Tags</a>
            </p>
            <p>
            <a href="rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/jquery.min.js"></script><script src="assets/js/bootstrap.min.js"></script><script src="assets/js/moment-with-locales.min.js"></script><script src="assets/js/fancydates.js"></script><script src="assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
