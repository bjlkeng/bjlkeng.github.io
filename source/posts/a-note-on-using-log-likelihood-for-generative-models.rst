.. title: A Note on Using Log-Likelihood for Generative Models
.. slug: a-note-on-using-log-likelihood-for-generative-models
.. date: 2019-07-30 07:50:09 UTC-04:00
.. tags: log-likelihood, generative models, mathjax
.. category: 
.. link: 
.. description: A short exploration on the theory behind using log-likelihood to train and measure generative models using image-like data.
.. type: text

.. |br| raw:: html

   <br />

.. |H2| raw:: html

   <br/><h3>

.. |H2e| raw:: html

   </h3>

.. |H3| raw:: html

   <h4>

.. |H3e| raw:: html

   </h4>

.. |center| raw:: html

   <center>

.. |centere| raw:: html

   </center>

A bit of a digression back to fundamentals.  One of the things that I find is
usually missing from many ML papers is how they relate to the fundamentals.
And what's more fundamental than probability (maybe measure theory)?

This post is a short note on some of the "gotchas" when trying to understand
log-likelihoods, probability and generative model loss functions.  I'll give
some intuition, examples and some basic demonstrations.  Let's get started!


.. TEASER_END

|h2| A Digression into Fundamentals |h2e|

**TODO{rewrite this...}**
The first thing I want to explain is something *very* fundamental (inspired
from [2,3]): How you find probability from observations of continuous
distributions?  Seems like a simple problem, right?  Well, there are lots
of paradoxes lurking here, so let's clarify them before moving on.  

Bear with me for a bit here, we'll get to how this applies to generative
models.

|h3| Zero Probability Events |h3e|

For example, assume you have sampled some data from an 
`IID <https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables>`__
continuous distribution, what is the probability of that happening?
Let's reason it out.

To make things concrete, assume we have a :math:`U`
`continuous uniform distribution <https://en.wikipedia.org/wiki/Uniform_distribution_(continuous>`__ 
with support :math:`[0,1]`. Let's draw a sample :math:`x` from :math:`U`, what
is the probability that we drew :math:`x`?  Well we know a few things about
this distribution, in particular, it's probability density function and how to
compute probability:

.. math::

    f_U(y) &= \frac{1}{b-a} = \left\{
        \begin{array}{ll}
            1 && \text{for support } [a,b] \\
            0 && \text{otherwise}
        \end{array}
    \right. \tag{1} \\
    P(u \leq y \leq v) &= \int_u^v f_U(y) dy = y \big|_u^v = v - u && \text{for } u, v \in [0,1] \tag{2}

Very fundamental indeed!  So we know how to compute probability (Equation 2)
over a distribution if we have a continuous interval, but that doesn't
quite get us to a single data point.
Let's take the limit of Equation 2 to see how the interval behaves as it
approaches a single data point:

.. math::

    P(x|U) &= \lim_{\epsilon \to 0}{P(x - \epsilon \leq y \leq x + \epsilon)} \\
    &= \lim_{\epsilon \to 0} \int_{x-\epsilon}^{x+\epsilon} f_U(y) dy \\
    &= \lim_{\epsilon \to 0} y \big|_{x-\epsilon}^{x+\epsilon} \\
    &= \lim_{\epsilon \to 0} { (x + \epsilon) - (x - \epsilon) } \\
    &= 0 \\
    \tag{3}

So the probability of :math:`x` occurring is :math:`0`?!

Yes!  Which is a bit
of a paradox since we did indeed draw it from a distribution, so it definitely
"happened".  This is also not particular to the uniform distribution, if you do
the same with any (reasonable) distribution, you'll find the same thing.  How
can this be possible?

Another way to think about this: what if the probability of observing :math:`x`
wasn't :math:`0` but rather some positive number :math:`\epsilon`?  That means
every number in the range :math:`[0, 1]` would have some positive probability
of being observed.  However, all those numbers/events are mutually exclusive,
so the sum of them should add up to :math:`1` but this isn't possible because
you have an infinite amount of them.

|h3| Hypothesis Testing with Continuous Data and Log-Likelihoods |h3e|

So how can resolve this paradox? As usual, we're asking the wrong question!
When we have observed an event, what we're really asking is "*what is the
probability that this event is generated by this hypothesis?*".  In other
words, we're really doing hypothesis testing! (Here, we're talking about
*statistical hypotheses*, which is an assumption about a distribution
with a particular set of parameters.)

So let's try this again but a bit more generically.  Let's say we have observed
a real-valued datum :math:`x` (this can easily be extended given IID data), and
we have :math:`N` hypothesis, :math:`M_1, M_2, \ldots, M_N`, each representing
a fixed parameter distribution with PDFs :math:`f_i` and CDFs :math:`F_i`.

(Note: we'll use the notation ":math:`M`" here because it also implies that
our hypotheses are trained models.  That is, a set of assumptions about the
data with a particular set of trained parameters.  You can start to see
how this is going to lead us to generative models...)

Let's formulate our question: "*what is the probability of hypothesis* :math:`M_1`
*given datum* :math:`x`" (using Bayes rule):

.. math::

    P(M_1 | x) &= \frac{P(x | M_1)P(M_1)}{P(x)} \\
               &= \frac{P(x | M_1)P(M_1)}{\sum_{i=1}^N P(x | M_i)P(M_i)} \\
               \tag{4}

Here we're using the standard expansion of the denominator for :math:`P(x)`.
As with Bayesian analysis, we need some prior for how likely each model occurs.
Let's just assume we have no intuition about which model is better, so they're
equally likely.  Since we're also dealing with continuous values, we'll use the
":math:`\epsilon` trick" we used above:

.. math::

    P(M_1 | x) 
    &= \frac{P(x | M_1)P(M_1)}{\sum_{i=1}^N P(x | M_i)P(M_i)} \\
    &= \lim_{\epsilon \to 0} 
        \frac{[\int_{x-\epsilon}^{x+\epsilon} f_1(x) dx][\frac{1}{N}]}{
            \sum_{i=1}^N [\int_{x-\epsilon}^{x+\epsilon} f_i(x) dx][\frac{1}{N}]} \\
    &= \lim_{\epsilon \to 0} 
        \frac{F_1(x+\epsilon) - F_1(x-\epsilon)}{\sum_{i=1}^N \big(F_i(x+\epsilon) - F_i(x-\epsilon)\big)} \\
    &= \frac{\lim_{\epsilon \to 0} \frac{F_1(x+\epsilon) - F_1(x-\epsilon)}{\epsilon}}{
        \sum_{i=1}^N 
        \lim_{\epsilon \to 0} 
        \big(\frac{F_i(x+\epsilon) - F_i(x-\epsilon)}{\epsilon}\big)} 
        && \text{divide top and bottom by } \epsilon \text{ and distribute limit}
        \\
    &= \frac{f_1(x)}{\sum_{i=1}^N f_i(x)} && \text{definition of derivative} \\
    \tag{5}

You might have to brush off your calculus a bit with the comments above, I
think you should be able to follow along.  The last step is not the typical
definition of a derivative but it's should be equivalent. (Note: this derivative
probably only works for smooth functions.)

The interesting this here is that we've totally resolved the problem of dealing
with continuous data!  We're dealing only with PDFs now and removed the zero
probability case when looking at :math:`P(x|M_i)` in isolation.

.. admonition:: Example 1: Probability an Observation is from a Given Two 
    Gaussian Hypotheses

    Suppose we have a datum :math:`x=0` that we know is drawn from
    one of two Gaussian distributions:
    
    * :math:`M_1: \mathcal{N}(\mu=0, \sigma^2=1)`
    * :math:`M_2: \mathcal{N}(\mu=1, \sigma^2=1)`

    What is the probability of :math:`x` being drawn from each distribution
    (assuming our priors are equally likely)?
    Equivalently, what is :math:`P(M_1 | x)` and :math:`P(M_2 | x)`
    with :math:`P(M_1)= P(M_2)=0.5`?

    Using Equation 5 it is simply just the PDFs of the two normalized by
    our priors:

    .. math::
    
        P(M_1|x) &= \frac{f_{M_1}P(M_1)}{f_{M_1}P(M_1)+ f_{M_2}P(M_2)} \\
                 &= \frac{-e^{x^2/2}(0.5)}{e^{-x^2/2}(0.5) + e^{-(x-1)^2/2}(0.5)}
                 &\approx 0.6225 \\
        P(M_2|x) &= \frac{f_{M_2}P(M_2)}{f_{M_1}P(M_1)+ f_{M_2}P(M_2)} \\
                 &= \frac{-e^{(x-1)^2/2}(0.5)}{e^{-x^2/2}(0.5) + e^{-(x-1)^2/2}(0.5)}
                 &\approx 0.3775 \tag{6}

    Therefore, :math:`x` is more likely to be drawn from distribution :math:`M_1`.


Before we finish off this section, we should notice on thing.  Given a fixed
set of hypotheses (or models) the denominator in Equation 5 doesn't change.
That is, :math:`P(X)` is constant.  Therefore, assuming all models are equally
likely, we can do a relative comparison of models just by their PDFs.
From Equation 5:

.. math::

    P(M_i | x) = \frac{f_i(x)}{\sum_{i=1}^N f_i(x)} \propto f_i(x)
    \tag{7}

Further, given a IID data for :math:`x`, we can do a relative comparison
of (fitted) models by taking the logarithm:

.. math::

    P(M_i | x_1 \ldots x_n) &= \frac{f_i(x_1)\ldots f_i(x_n)}{\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)} \\
    \log{P(M_i | x_1 \ldots x_n)} &= \log\big[\frac{f_i(x_1)\ldots f_i(x_n)}{\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)}\big] \\
    &= \log{f_i(x_1)} + \ldots + \log{f_i(x_n)} - \log{[\sum_{i=1}^N f_i(x_1)\ldots f_i(x_n)]} \\
    &\propto \log{f_i(x_1)} + \ldots + \log{f_i(x_n)} \\
    &= \sum_{j=1}^n \log{f_i(x_j)} \\
    \tag{8}

Where the last line of Equation 8 should look very familiar: it's the standard
log likelihood that we maximize in many ML and statistical models. Thus,
we can directly compare how well a model represents some data using the loss
from the log-likelihood as you would expect. (However, keep in mind it is
*not* a probability.)

|h3| Cross Entropy and Expected Message Length |h3e|

I won't go too deep into the concept of `(information) entropy <https://en.wikipedia.org/wiki/Entropy>`__,
you can find a more detailed explanation in my previous post on
`Maximum Entropy Distributions <link://slug/maximum-entropy-distributions/>`__.
Information Entropy is defined as follows over a discrete probability
distribution :math:`p`:

.. math::

    H(p) = -\sum_{x \in \mathcal{X}} p(x) \log p(x) \tag{9}


One of the interesting properties for discrete random variables is that the
entropy provides a lower bound for how much (on average) those symbols can be
compressed 
(via the `Shannon Coding Theorem <https://en.wikipedia.org/wiki/Shannon's_source_coding_theorem>`__).
This is squarely in the domain of information theory and communications
systems.  

For example, a *very* simplified application might be you might be sending a
string of bytes to your friend over a Wifi channel, how should you encode each
byte (i.e. symbol) into bits so that it minimizes the length of the message?
Intuitively, you would want the most frequent byte values (symbols) to have
the shortest length, the next most frequent byte to have a slightly longer
length, etc. `Huffman Coding <https://en.wikipedia.org/wiki/Huffman_coding>`__
is an example of such a scheme that enables lossless compression that is
optimal.  Optimality (that is, symbol-to-symbol optimality) is precisely the
entropy lower bound.

A related concept is that of 
`cross entropy <https://en.wikipedia.org/wiki/Cross_entropy>`__:
given two probability distributions :math:`p` and :math:`q` defined over the
same set, we have:

.. math::

    H(p,q) &= -\sum_{x \in \mathcal{X}} p(x) \log q(x) \\
    &= H(p) + D_{KL}(p||q) \\ 
    \tag{10}

This looks almost identical to the definition of entropy in Equation 9 except
we replace :math:`p` with :math:`q` inside the logarithm. As you would
expect, this also has an equivalent interpretation: cross entropy gives 
the average number of bits (using base 2 logarithm) needed to code a symbol
drawn from :math:`p` using the optimal code for :math:`q`. 
The second line of Equation 10 also shows us another interpretation of cross
entropy: :math:`H(p,q)` is equal to entropy of :math:`p` plus the KL divergence
between :math:`q` from :math:`p`.

Cross entropy can also be defined for the continuous case too:

.. math::

    H(p, q) = -\int_{\mathcal{X}} P(x)\log Q(x) dx \tag{11}

Note: We should be careful distinguishing between information entropy defined in
Equation 9 on discrete variables and the continuous version called 
`differential entropy <https://en.wikipedia.org/wiki/Differential_entropy>`__.
Differential entropy has a similar form but doesn't have the same nice
intuitive meaning of encoding into bits.  It also doesn't have nice properites,
for example, differential entropy can be negative.  A more interpretable
quantity is the KL divergence, which is the "number of extra bits to encode
P using Q".  See this
`Stackoverflow question <https://stats.stackexchange.com/questions/256203/how-to-interpret-differential-entropy>`__ 
for more details.


|h2| Image Data, Log-Likelihood and Generative Models |h2e|

Talk about how you don't have the issue directly if you
model the discrete data (a la PixelCNN, Pixel CNN++).

Explain Equation 3 from [1] in more detail

* Start by explaining the expected log-likelihood used in training
* Then show that this is a lower bound on the actual NLL (bits/pixel)

|h2| Log-Likelihood and Image Quality |h2e|

Not the best measure, evidenced by PixelCNN vs. RealNVP

|h2| Conclusion |h2e|


|h2| Further Reading |h2e|

* [1] "A note on the evaluation of generative models", Lucas Theis, Aäron van den Oord, Matthias Bethge, `<http://arxiv.org/abs/1511.01844>`__
* Stack Exchange Questions:
    * [2] `Probability that a Continuous Event is Generated from a Distribution <https://math.stackexchange.com/questions/2818318/probability-that-a-sample-is-generated-from-a-distribution>`__
    * [3] `Zero Probability Event <https://math.stackexchange.com/questions/920241/can-an-observed-event-in-fact-be-of-zero-probability>`__
* Previous posts: `Autoregressive Autoencoders <link://slug/autoregressive-autoencoders>`__, `Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders <link://slug/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders>`__, `PixelCNN <link://slug/pixelcnn>`__
