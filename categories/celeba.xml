<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (Posts about CELEBA)</title><link>http://bjlkeng.github.io/</link><description></description><atom:link href="http://bjlkeng.github.io/categories/celeba.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 09 Feb 2023 02:29:18 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Normalizing Flows with Real NVP</title><link>http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post has been a long time coming.  I originally started working on it several posts back but
hit a roadblock in the implementation and then got distracted with some other ideas, which took
me down various rabbit holes (&lt;a class="reference external" href="http://bjlkeng.github.io/posts/hamiltonian-monte-carlo/"&gt;here&lt;/a&gt;,
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/"&gt;here&lt;/a&gt;, and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/"&gt;here&lt;/a&gt;).
It feels good to finally get back on track to some core ML topics.
The other nice thing about not being an academic researcher (not that I'm
really researching anything here) is that there is no pressure to do anything!
If it's just for fun, you can take your time with a topic, veer off track, and
the come back to it later.  It's nice having the freedom to do what you want (this applies to
more than just learning about ML too)!&lt;/p&gt;
&lt;p&gt;This post is going to talk about a class of deep probabilistic generative
models called normalizing flows.  Alongside &lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;Variational Autoencoders&lt;/a&gt;
and autoregressive models &lt;a class="footnote-reference brackets" href="http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/#id3" id="id1"&gt;1&lt;/a&gt; (e.g. &lt;a class="reference external" href="http://bjlkeng.github.io/posts/pixelcnn/"&gt;Pixel CNN&lt;/a&gt; and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/autoregressive-autoencoders/"&gt;Autoregressive autoencoders&lt;/a&gt;),
normalizing flows have been one of the big ideas in deep probabilistic generative models (I don't count GANs because they are not quite probabilistic).
Specifically, I'll be presenting one of the earlier normalizing flow
techniques named &lt;em&gt;Real NVP&lt;/em&gt; (circa 2016).
The formulation is simple but surprisingly effective, which makes it a good
candidate to understand more about normalizing flows.
As usual, I'll go over some background, the method, an implementation
(with commentary on the details), and some experimental results.  Let's get into the flow!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/"&gt;Read moreâ€¦&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>CELEBA</category><category>CIFAR10</category><category>generative models</category><category>mathjax</category><category>MNIST</category><category>normalizing flows</category><guid>http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/</guid><pubDate>Sat, 23 Apr 2022 23:36:05 GMT</pubDate></item></channel></rss>