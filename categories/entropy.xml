<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (Posts about entropy)</title><link>http://bjlkeng.github.io/</link><description></description><atom:link href="http://bjlkeng.github.io/categories/entropy.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 03 May 2025 19:51:43 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>The Logic Behind the Maximum Entropy Principle</title><link>http://bjlkeng.github.io/posts/the-logic-behind-entropy/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;For a while now, I've really enjoyed diving deep to understand
probability and related fundamentals (see
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/probability-the-logic-of-science/"&gt;here&lt;/a&gt;,
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/maximum-entropy-distributions/"&gt;here&lt;/a&gt;, and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/an-introduction-to-stochastic-calculus/"&gt;here&lt;/a&gt;).
Entropy is a topic that comes up all over the place from physics to information
theory, and of course, machine learning.  I written about it in various
different forms but always taken it as a given as the "expected information".
Well I found a few of good explanations about how to "derive" it and thought
that I should share.&lt;/p&gt;
&lt;p&gt;In this post, I'll be showing a few of derivations of the maximum entropy
principle, where entropy appears as part of the definition.  These derivations
will show why it is a reasonable and natural thing to maximize, and how it is
determined from some well thought out reasoning.  This post will be more math
heavy but hopefully it will give you more insight into this wonderfully
surprising topic.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/the-logic-behind-entropy/"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>entropy</category><category>information</category><category>Jaynes</category><category>mathjax</category><category>Shannon</category><category>Wallis</category><guid>http://bjlkeng.github.io/posts/the-logic-behind-entropy/</guid><pubDate>Sat, 03 Aug 2024 00:44:59 GMT</pubDate></item><item><title>Lossless Compression with Asymmetric Numeral Systems</title><link>http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;During my undergraduate days, one of the most interesting courses I took was on
coding and compression.  Here was a course that combined algorithms,
probability and secret messages, what's not to like? &lt;a class="footnote-reference brackets" href="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/#id2" id="id1"&gt;1&lt;/a&gt; I ended up not going
down that career path, at least partially because communications systems had
its heyday around the 2000s with companies like Nortel and Blackberry and its
predecessors (some like to joke that all the major theoretical breakthroughs
were done by Shannon and his discovery of information theory around 1950).  Fortunately, I
eventually wound up studying industrial applications of classical AI techniques
and then machine learning, which has really grown like crazy in the last 10
years or so.  Which is exactly why I was so surprised that a &lt;em&gt;new&lt;/em&gt; and &lt;em&gt;better&lt;/em&gt;
method of lossless compression was developed in 2009 &lt;em&gt;after&lt;/em&gt; I finished my
undergraduate degree when I was well into my PhD.  It's a bit mind boggling that
something as well-studied as entropy-based lossless compression still had
(have?) totally new methods to discover, but I digress.&lt;/p&gt;
&lt;p&gt;In this post, I'm going to write about a relatively new entropy based encoding
method called Asymmetrical Numeral Systems (ANS) developed by Jaroslaw (Jarek)
Duda [2].  If you've ever heard of Arithmetic Coding (probably best known for
its use in JPEG compression), ANS runs in a very similar vein.  It can
generate codes that are close to the theoretical compression limit
(similar to Arithmetic coding) but is &lt;em&gt;much&lt;/em&gt; more efficient.  It's been used in
modern compression algorithms since 2014 including compressors developed
by Facebook, Apple and Google [3].  As usual, I'm going to go over some
background, some math, some examples to help with intuition, and finally some
experiments with a toy ANS implementation I wrote.  I hope you're as
excited as I am, let's begin!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/"&gt;Read more…&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Arithmetic Coding</category><category>asymmetric numeral systems</category><category>compression</category><category>entropy</category><category>Huffman coding</category><category>mathjax</category><guid>http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/</guid><pubDate>Sat, 26 Sep 2020 14:37:43 GMT</pubDate></item><item><title>The Calculus of Variations</title><link>http://bjlkeng.github.io/posts/the-calculus-of-variations/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to describe a specialized type of calculus called
variational calculus.
Analogous to the usual methods of calculus that we learn in university,
this one deals with functions &lt;em&gt;of functions&lt;/em&gt; and how to
minimize or maximize them.  It's used extensively in physics problems such as
finding the minimum energy path a particle takes under certain conditions.  As
you can also imagine, it's also used in machine learning/statistics where you
want to find a density that optimizes an objective &lt;a class="footnote-reference brackets" href="http://bjlkeng.github.io/posts/the-calculus-of-variations/#id4" id="id1"&gt;1&lt;/a&gt;.  The explanation I'm
going to use (at least for the first part) is heavily based upon Svetitsky's
&lt;a class="reference external" href="http://julian.tau.ac.il/bqs/functionals/functionals.html"&gt;Notes on Functionals&lt;/a&gt;, which so far is
the most intuitive explanation I've read.  I'll try to follow Svetitsky's
notes to give some intuition on how we arrive at variational calculus from
regular calculus with a bunch of examples along the way.  Eventually we'll
get to an application that relates back to probability.  I think with the right
intuition and explanation, it's actually not too difficult, enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/the-calculus-of-variations/"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>differentials</category><category>entropy</category><category>lagrange multipliers</category><category>mathjax</category><category>probability</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/the-calculus-of-variations/</guid><pubDate>Sun, 26 Feb 2017 15:08:38 GMT</pubDate></item><item><title>Maximum Entropy Distributions</title><link>http://bjlkeng.github.io/posts/maximum-entropy-distributions/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post will talk about a method to find the probability distribution that best
fits your given state of knowledge.  Using the principle of maximum
entropy and some testable information (e.g. the mean), you can find the
distribution that makes the fewest assumptions about your data (the one with maximal
information entropy).  As you may have guessed, this is used often in Bayesian
inference to determine prior distributions and also (at least implicitly) in
natural language processing applications with maximum entropy (MaxEnt)
classifiers (i.e. a multinomial logistic regression).  As usual, I'll go through
some intuition, some math, and some examples.  Hope you find this topic as
interesting as I do!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/maximum-entropy-distributions/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>entropy</category><category>mathjax</category><category>probability</category><guid>http://bjlkeng.github.io/posts/maximum-entropy-distributions/</guid><pubDate>Fri, 27 Jan 2017 14:05:00 GMT</pubDate></item></channel></rss>