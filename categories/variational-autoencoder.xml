<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (Posts about variational autoencoder)</title><link>http://bjlkeng.github.io/</link><description></description><atom:link href="http://bjlkeng.github.io/categories/variational-autoencoder.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 24 Oct 2023 01:51:43 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Lossless Compression with Latent Variable Models using Bits-Back Coding</title><link>http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;A lot of modern machine learning is related to this idea of "compression", or
maybe to use a fancier term "representations".  Taking a huge dimensional space
(e.g. images of 256 x 256 x 3 pixels = 196608 dimensions) and somehow compressing it into
a 1000 or so dimensional representation seems like pretty good compression to
me!  Unfortunately, it's not a lossless compression (or representation).
Somehow though, it seems intuitive that there must be a way to use what is learned in
these powerful lossy representations to help us better perform &lt;em&gt;lossless&lt;/em&gt;
compression, right?  Of course there is! (It would be too anti-climatic of a
setup otherwise.)&lt;/p&gt;
&lt;p&gt;This post is going to introduce a method to perform lossless compression that
leverages the learned "compression" of a machine learning latent variable
model using the Bits-Back coding algorithm.  Depending on how you first think
about it, this &lt;em&gt;seems&lt;/em&gt; like it should either be (a) really easy or (b) not possible at
all.  The reality is kind of in between with an elegant theoretical algorithm
that is brought down by the realities of discretization and imperfect learning
by the model.  In today's post, I'll skim over some preliminaries (mostly
referring you to previous posts), go over the main Bits-Back coding algorithm
in detail, and discuss some of the implementation details and experiments that
I did while trying to write a toy version of the algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/"&gt;Read moreâ€¦&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>asymmetric numeral systems</category><category>Bits-Back</category><category>compression</category><category>lossless</category><category>mathjax</category><category>MNIST</category><category>variational autoencoder</category><guid>http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/</guid><pubDate>Tue, 06 Jul 2021 16:00:00 GMT</pubDate></item></channel></rss>