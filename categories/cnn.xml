<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (Posts about CNN)</title><link>http://bjlkeng.github.io/</link><description></description><atom:link href="http://bjlkeng.github.io/categories/cnn.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 03 Aug 2024 01:42:52 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>A Look at The First Place Solution of a Dermatology Classification Kaggle Competition</title><link>http://bjlkeng.github.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;One interesting thing I often think about is the gap between academic and real-world
solutions.  In general academic solutions play in the realm of idealized problem
spaces, removing themselves from needing to care about the messiness of the real-world.
&lt;a class="reference external" href="https://www.kaggle.com/competitions"&gt;Kaggle&lt;/a&gt;
competitions are a (small) step in the right direction towards dealing with messiness,
usually providing a true blind test set (vs. overused benchmarks), and opening a
few degrees of freedom in terms the techniques that can be used, which
usually eschews novelty in favour of more robust methods.  To this end, I
thought it would be useful to take a look at a more realistic problem (via a
Kaggle competition) and understand the practical details that result in a
superior solution.&lt;/p&gt;
&lt;p&gt;This post will cover the &lt;a class="reference external" href="https://arxiv.org/abs/2010.05351"&gt;first place solution&lt;/a&gt; [&lt;a class="reference internal" href="http://bjlkeng.github.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#id2"&gt;1&lt;/a&gt;] to the
&lt;a class="reference external" href="https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview"&gt;SIIM-ISIC Melanoma Classification&lt;/a&gt; [&lt;a class="reference internal" href="http://bjlkeng.github.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/#id1"&gt;0&lt;/a&gt;] challenge.
In addition to using tried and true architectures (mostly EfficientNets), they
have some interesting tactics they use to formulate the problem, process the
data, and train/validate the model.  I'll cover background on the
ML techniques, competition and data, architectural details, problem formulation, and
implementation.  I've also run some experiments to better understand the
benefits of certain choices they made.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/"&gt;Read more…&lt;/a&gt; (36 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>augmentation</category><category>CNN</category><category>data</category><category>dermatology</category><category>EfficientNet</category><category>mathjax</category><category>MobileNet</category><category>Noisy Student</category><category>validation set</category><guid>http://bjlkeng.github.io/posts/a-look-at-the-first-place-solution-of-a-dermatology-classification-kaggle-competition/</guid><pubDate>Sat, 23 Dec 2023 00:09:46 GMT</pubDate></item><item><title>Semi-supervised Learning with Variational Autoencoders</title><link>http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I'll be continuing on this variational autoencoder (VAE) line of
exploration
(previous posts: &lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;here&lt;/a&gt; and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/"&gt;here&lt;/a&gt;) by
writing about how to use variational autoencoders to do semi-supervised
learning.  In particular, I'll be explaining the technique used in
"Semi-supervised Learning with Deep Generative Models" by Kingma et al.
I'll be digging into the math (hopefully being more explicit than the paper),
giving a bit more background on the variational lower bound, as well as
my usual attempt at giving some more intuition.
I've also put some notebooks on Github that compare the VAE methods
with others such as PCA, CNNs, and pre-trained models.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/"&gt;Read more…&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>CIFAR10</category><category>CNN</category><category>generative models</category><category>inception</category><category>Kullback-Leibler</category><category>mathjax</category><category>PCA</category><category>semi-supervised learning</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/</guid><pubDate>Mon, 11 Sep 2017 12:40:47 GMT</pubDate></item></channel></rss>