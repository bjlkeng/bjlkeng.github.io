<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bounded Rationality (Posts about generative models)</title><link>http://bjlkeng.github.io/</link><description></description><atom:link href="http://bjlkeng.github.io/categories/generative-models.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 16 May 2023 00:49:51 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Normalizing Flows with Real NVP</title><link>http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post has been a long time coming.  I originally started working on it several posts back but
hit a roadblock in the implementation and then got distracted with some other ideas, which took
me down various rabbit holes (&lt;a class="reference external" href="http://bjlkeng.github.io/posts/hamiltonian-monte-carlo/"&gt;here&lt;/a&gt;,
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/lossless-compression-with-asymmetric-numeral-systems/"&gt;here&lt;/a&gt;, and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/lossless-compression-with-latent-variable-models-using-bits-back-coding/"&gt;here&lt;/a&gt;).
It feels good to finally get back on track to some core ML topics.
The other nice thing about not being an academic researcher (not that I'm
really researching anything here) is that there is no pressure to do anything!
If it's just for fun, you can take your time with a topic, veer off track, and
the come back to it later.  It's nice having the freedom to do what you want (this applies to
more than just learning about ML too)!&lt;/p&gt;
&lt;p&gt;This post is going to talk about a class of deep probabilistic generative
models called normalizing flows.  Alongside &lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;Variational Autoencoders&lt;/a&gt;
and autoregressive models &lt;a class="footnote-reference brackets" href="http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/#id3" id="id1"&gt;1&lt;/a&gt; (e.g. &lt;a class="reference external" href="http://bjlkeng.github.io/posts/pixelcnn/"&gt;Pixel CNN&lt;/a&gt; and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/autoregressive-autoencoders/"&gt;Autoregressive autoencoders&lt;/a&gt;),
normalizing flows have been one of the big ideas in deep probabilistic generative models (I don't count GANs because they are not quite probabilistic).
Specifically, I'll be presenting one of the earlier normalizing flow
techniques named &lt;em&gt;Real NVP&lt;/em&gt; (circa 2016).
The formulation is simple but surprisingly effective, which makes it a good
candidate to understand more about normalizing flows.
As usual, I'll go over some background, the method, an implementation
(with commentary on the details), and some experimental results.  Let's get into the flow!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/"&gt;Read more…&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>CELEBA</category><category>CIFAR10</category><category>generative models</category><category>mathjax</category><category>MNIST</category><category>normalizing flows</category><guid>http://bjlkeng.github.io/posts/normalizing-flows-with-real-nvp/</guid><pubDate>Sat, 23 Apr 2022 23:36:05 GMT</pubDate></item><item><title>A Note on Using Log-Likelihood for Generative Models</title><link>http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;One of the things that I find is usually missing from many ML papers is how
they relate to the fundamentals.  There's always a throwaway line where it
assumes something that is not at all obvious (see my post on
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/"&gt;Importance Sampling&lt;/a&gt;).  I'm the kind of person who likes to
understand things to a satisfactory degree (it's literally in the subtitle of
the blog) so I couldn't help myself investigating a minor idea I read about in
a paper.&lt;/p&gt;
&lt;p&gt;This post investigates how to use continuous density outputs (e.g. a logistic
or normal distribution) to model discrete image data (e.g. 8-bit RGB values).
It seems like it might be something obvious such as setting the loss as the
average log-likelihood of the continuous density and that's &lt;em&gt;almost&lt;/em&gt; the
whole story.  But leaving it at that skips over so many (interesting) and
non-obvious things that you would never know if you didn't bother to look.  I'm
a curious fellow so come with me and let's take a look!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/"&gt;Read more…&lt;/a&gt; (15 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>generative models</category><category>log-likelihood</category><category>mathjax</category><guid>http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/</guid><pubDate>Tue, 27 Aug 2019 11:50:09 GMT</pubDate></item><item><title>PixelCNN</title><link>http://bjlkeng.github.io/posts/pixelcnn/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;It's been a long time coming but I'm finally getting this post out!  I read
this paper a couple of years ago and wanted to really understand it because it
was state of the art at the time (still pretty close even now).  As usual
though, once I started down the variational autoencoder line of posts, there
was always &lt;em&gt;yet&lt;/em&gt; another VAE paper to look into so I never got around to
looking at this one.&lt;/p&gt;
&lt;p&gt;This post is all about a proper probabilistic generative model called Pixel
Convolutional Neural Networks or PixelCNN.  It was originally proposed
as a side contribution of Pixel Recurrent Neural Networks in [1] and later
expanded upon in [2,3] (and I'm sure many other papers).  The real cool thing
about it is that it's (a) probabilistic, and (b) autoregressive.  It's still
counter-intuitive to me that you can generate images one pixel at at time, but
I'm jumping ahead of myself here.  We'll go over some background material, the
method, and my painstaking attempts at an implementation (and what I learned
from it).  Let's get started!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/pixelcnn/"&gt;Read more…&lt;/a&gt; (23 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoregressive</category><category>CIFAR10</category><category>generative models</category><category>mathjax</category><guid>http://bjlkeng.github.io/posts/pixelcnn/</guid><pubDate>Mon, 22 Jul 2019 11:11:09 GMT</pubDate></item><item><title>Importance Sampling and Estimating Marginal Likelihood in Variational Autoencoders</title><link>http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;It took a while but I'm back!  This post is kind of a digression (which seems
to happen a lot) along my journey of learning more about probabilistic
generative models.  There's so much in ML that you can't help learning a lot
of random things along the way.  That's why it's interesting, right?&lt;/p&gt;
&lt;p&gt;Today's topic is &lt;em&gt;importance sampling&lt;/em&gt;.  It's a really old idea that you may
have learned in a statistics class (I didn't) but somehow is useful in deep learning,
what's old is new right?  How this is relevant to the discussion is that when
we have a large latent variable model (e.g. a variational
autoencoder), we want to be able to efficiently estimate the marginal likelihood
given data.  The marginal likelihood is kind of taken for granted in the
experiments of some VAE papers when comparing different models.  I was curious
how it was actually computed and it took me down this rabbit hole.  Turns out
it's actually pretty interesting!  As usual, I'll have a mix of background
material, examples, math and code to build some intuition around this topic.
Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/"&gt;Read more…&lt;/a&gt; (22 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>autoregressive</category><category>CIFAR10</category><category>generative models</category><category>importance sampling</category><category>mathjax</category><category>MNIST</category><category>Monte Carlo</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/</guid><pubDate>Wed, 06 Feb 2019 12:20:11 GMT</pubDate></item><item><title>Variational Autoencoders with Inverse Autoregressive Flows</title><link>http://bjlkeng.github.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I'm going to be describing a really cool idea about how
to improve variational autoencoders using inverse autoregressive
flows.  The main idea is that we can generate more powerful posterior
distributions compared to a more basic isotropic Gaussian by applying a
series of invertible transformations.  This, in theory, will allow
your variational autoencoder to fit better by concentrating the
stochastic samples around a closer approximation to the true
posterior.  The math works out so nicely while the results are kind of
marginal &lt;a class="footnote-reference brackets" href="http://bjlkeng.github.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/#id3" id="id1"&gt;1&lt;/a&gt;.  As usual, I'll go through some intuition, some math,
and have an implementation with few experiments I ran.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/"&gt;Read more…&lt;/a&gt; (18 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>autoregressive</category><category>CIFAR10</category><category>generative models</category><category>Kullback-Leibler</category><category>MADE</category><category>mathjax</category><category>MNIST</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/variational-autoencoders-with-inverse-autoregressive-flows/</guid><pubDate>Tue, 19 Dec 2017 13:47:38 GMT</pubDate></item><item><title>Autoregressive Autoencoders</title><link>http://bjlkeng.github.io/posts/autoregressive-autoencoders/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;You might think that I'd be bored with autoencoders by now but I still
find them extremely interesting!  In this post, I'm going to be explaining
a cute little idea that I came across in the paper &lt;a class="reference external" href="https://arxiv.org/pdf/1502.03509.pdf"&gt;MADE: Masked Autoencoder
for Distribution Estimation&lt;/a&gt;.
Traditional autoencoders are great because they can perform unsupervised
learning by mapping an input to a latent representation.  However, one
drawback is that they don't have a solid probabilistic basis
(of course there are other variants of autoencoders that do, see previous posts
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;here&lt;/a&gt;,
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/"&gt;here&lt;/a&gt;, and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/"&gt;here&lt;/a&gt;).
By using what the authors define as the &lt;em&gt;autoregressive property&lt;/em&gt;, we can
transform the traditional autoencoder approach into a fully probabilistic model
with very little modification! As usual, I'll provide some intuition, math and
an implementation.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/autoregressive-autoencoders/"&gt;Read more…&lt;/a&gt; (17 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>autoregressive</category><category>generative models</category><category>MADE</category><category>mathjax</category><category>MNIST</category><guid>http://bjlkeng.github.io/posts/autoregressive-autoencoders/</guid><pubDate>Sat, 14 Oct 2017 14:02:15 GMT</pubDate></item><item><title>Semi-supervised Learning with Variational Autoencoders</title><link>http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I'll be continuing on this variational autoencoder (VAE) line of
exploration
(previous posts: &lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;here&lt;/a&gt; and
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/"&gt;here&lt;/a&gt;) by
writing about how to use variational autoencoders to do semi-supervised
learning.  In particular, I'll be explaining the technique used in
"Semi-supervised Learning with Deep Generative Models" by Kingma et al.
I'll be digging into the math (hopefully being more explicit than the paper),
giving a bit more background on the variational lower bound, as well as
my usual attempt at giving some more intuition.
I've also put some notebooks on Github that compare the VAE methods
with others such as PCA, CNNs, and pre-trained models.  Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/"&gt;Read more…&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>CIFAR10</category><category>CNN</category><category>generative models</category><category>inception</category><category>Kullback-Leibler</category><category>mathjax</category><category>PCA</category><category>semi-supervised learning</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/</guid><pubDate>Mon, 11 Sep 2017 12:40:47 GMT</pubDate></item><item><title>A Variational Autoencoder on the SVHN dataset</title><link>http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I'm going to share some notes on implementing a variational
autoencoder (VAE) on the
&lt;a class="reference external" href="http://ufldl.stanford.edu/housenumbers/"&gt;Street View House Numbers&lt;/a&gt;
(SVHN) dataset.  My last post on
&lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;variational autoencoders&lt;/a&gt;
showed a simple example on the MNIST dataset but because it was so simple I
thought I might have missed some of the subtler points of VAEs -- boy was I
right!  The fact that I'm not really a computer vision guy nor a deep learning
guy didn't help either.  Through this exercise, I picked up some of the basics
in the "craft" of computer vision/deep learning area; there are a lot of subtle
points that are easy to gloss over if you're just reading someone else's
tutorial.  I'll share with you some of the details in the math (that I
initially got wrong) and also some of the implementation notes along with a
notebook that I used to train the VAE.  Please check out my previous post
on &lt;a class="reference external" href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;variational autoencoders&lt;/a&gt; to
get some background.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 2017-08-09: I actually found a bug in my original code where I was
only using a small subset of the data!  I fixed it up in the notebooks and
I've added some inline comments below to say what I've changed.  For the most
part, things have stayed the same but the generated images are a bit blurry
because the dataset isn't so easy anymore.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/"&gt;Read more…&lt;/a&gt; (19 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>generative models</category><category>Kullback-Leibler</category><category>mathjax</category><category>svhn</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/</guid><pubDate>Thu, 13 Jul 2017 12:13:03 GMT</pubDate></item><item><title>Variational Autoencoders</title><link>http://bjlkeng.github.io/posts/variational-autoencoders/</link><dc:creator>Brian Keng</dc:creator><description>&lt;div&gt;&lt;p&gt;This post is going to talk about an incredibly interesting unsupervised
learning method in machine learning called variational autoencoders.  It's main
claim to fame is in building generative models of complex distributions like
handwritten digits, faces, and image segments among others.  The really cool
thing about this topic is that it has firm roots in probability but uses a
function approximator (i.e.  neural networks) to approximate an otherwise
intractable problem.  As usual, I'll try to start with some background and
motivation, include a healthy does of math, and along the way try to convey
some of the intuition of why it works.  I've also annotated a
&lt;a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb"&gt;basic example&lt;/a&gt;
so you can see how the math relates to an actual implementation.  I based much
of this post on Carl Doersch's &lt;a class="reference external" href="https://arxiv.org/abs/1606.05908"&gt;tutorial&lt;/a&gt;,
which has a great explanation on this whole topic, so make sure you check that
out too.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bjlkeng.github.io/posts/variational-autoencoders/"&gt;Read more…&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>autoencoders</category><category>generative models</category><category>Kullback-Leibler</category><category>mathjax</category><category>variational calculus</category><guid>http://bjlkeng.github.io/posts/variational-autoencoders/</guid><pubDate>Tue, 30 May 2017 12:19:36 GMT</pubDate></item></channel></rss>