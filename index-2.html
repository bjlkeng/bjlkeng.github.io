<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Understanding math, machine learning, and data to a satisfactory degree.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Bounded Rationality (old posts, page 2) | Bounded Rationality</title>
<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link href="assets/css/colorbox.css" rel="stylesheet" type="text/css">
<link href="assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="http://bjlkeng.github.io/index-2.html">
<link rel="prev" href="index-1.html" type="text/html">
<link rel="next" href="index-3.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://bjlkeng.github.io/">

                <span class="h1" id="blog-title">Bounded Rationality</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<p class="lead">Understanding math, machine learning, and data to a satisfactory degree.</p>
<!--
                
                <li><a href="/archive.html">Archive</a>
                <li><a href="/categories/">Tags</a>
                <li><a href="/rss.xml">RSS feed</a>

                 
-->
            </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            <div class="col-lg-9">
                
                

<div class="postindex">
    <article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/semi-supervised-learning-with-variational-autoencoders/" class="u-url">Semi-supervised Learning with Variational Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/semi-supervised-learning-with-variational-autoencoders/" rel="bookmark"><time class="published dt-published" datetime="2017-09-11T08:40:47-04:00" title="2017-09-11 08:40">2017-09-11 08:40</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>In this post, I'll be continuing on this variational autoencoder (VAE) line of
exploration
(previous posts: <a class="reference external" href="posts/variational-autoencoders">here</a> and
<a class="reference external" href="posts/a-variational-autoencoder-on-the-svnh-dataset">here</a>) by
writing about how to use variational autoencoders to do semi-supervised
learning.  In particular, I'll be explaining the technique used in
"Semi-supervised Learning with Deep Generative Models" by Kingma et al.
I'll be digging into the math (hopefully being more explicit than the paper),
giving a bit more background on the variational lower bound, as well as
my usual attempt at giving some more intuition.
I've also put some notebooks on Github that compare the VAE methods
with others such as PCA, CNNs, and pre-trained models.  Enjoy!</p>
<p class="more"><a href="posts/semi-supervised-learning-with-variational-autoencoders/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/the-hard-thing-about-machine-learning/" class="u-url">The Hard Thing about Machine Learning</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/the-hard-thing-about-machine-learning/" rel="bookmark"><time class="published dt-published" datetime="2017-08-22T08:32:55-04:00" title="2017-08-22 08:32">2017-08-22 08:32</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>I wrote a post on the hard parts about machine learning over
at Rubikloud:</p>
<ul class="simple">
<li><a class="reference external" href="https://rubikloud.com/labs/data-science/hard-thing-machine-learning/">The Hard Thing about Machine Learning</a></li>
</ul>
<p>Here's a blurb:</p>
<blockquote>
<p>Much of the buzz around machine learning lately has been around novel
applications of deep learning models. They have captured our imagination by
anthropomorphizing them, allowing them to dream, play games at superhuman
levels, and read x-rays better than physicians. While these deep learning
models are incredibly powerful with incredible ingenuity built into them,
they are not humans, nor are they much more than “sufficiently large
parametric models trained with gradient descent on sufficiently many
examples.” In my experience, this is not the hard part about machine
learning.</p>
<p>Beyond the flashy headlines, the high-level math, and the computation-heavy
calculations, the whole point of machine learning — as has been with
computing and software before it — has been its application to real-world
outcomes. Invariably, this means dealing with the realities of messy data,
generating robust predictions, and automating decisions.</p>
<p>...</p>
<p>Just as much of the impact of machine learning is beneath the surface, the
hard parts of machine learning are not usually sexy. I would argue that the
hard parts about machine learning fall into two areas: generating robust
predictions and building machine learning systems.</p>
</blockquote>
<p>Enjoy!</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/building-a-table-tennis-ranking-model/" class="u-url">Building A Table Tennis Ranking Model</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/building-a-table-tennis-ranking-model/" rel="bookmark"><time class="published dt-published" datetime="2017-07-19T08:51:41-04:00" title="2017-07-19 08:51">2017-07-19 08:51</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>I wrote a post about building a table tennis ranking model over at Rubikloud:</p>
<ul class="simple">
<li><a class="reference external" href="https://rubikloud.com/labs/building-table-tennis-ranking-model/">Building A Table Tennis Ranking Model</a></li>
</ul>
<p>It uses
<a class="reference external" href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry</a>
probability model to predict the outcome of pair-wise comparisons (e.g. games
or matches).  I describe an easy algorithm for fitting the model (via
MM-algorithms) as well as adding a simple Bayesian prior to handle ill-defined
cases.  I even have some
<a class="reference external" href="https://github.com/bjlkeng/Bradley-Terry-Model">code on Github</a>
so you can build your own ranking system using Google sheets.</p>
<p>Here's a blurb:</p>
<blockquote>
<p>Many of our Rubikrew are big fans of table tennis, in fact, we’ve held an
annual table tennis tournament for all the employees for three years
running (and I’m the reigning champion). It’s an incredibly fun event where
everyone in the company gets involved from the tournament participants to
the spectators who provide lively play-by-play commentary.</p>
<p>Unfortunately, not everyone gets to participate either due to travel and
scheduling issues, or by the fact that they miss the actual tournament
period in the case of our interns and co-op students. Another downside is
that the event is a single-elimination tournament, so while it has a clear
winner the ranking of the participants is not clear.</p>
<p>Being a data scientist, I identified this as a thorny issue for our
Rubikrew table tennis players. So, I did what any data scientist would do
and I built a model.</p>
</blockquote>
<p>Enjoy!</p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/a-variational-autoencoder-on-the-svnh-dataset/" class="u-url">A Variational Autoencoder on the SVHN dataset</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/a-variational-autoencoder-on-the-svnh-dataset/" rel="bookmark"><time class="published dt-published" datetime="2017-07-13T08:13:03-04:00" title="2017-07-13 08:13">2017-07-13 08:13</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>In this post, I'm going to share some notes on implementing a variational
autoencoder (VAE) on the
<a class="reference external" href="http://ufldl.stanford.edu/housenumbers/">Street View House Numbers</a>
(SVHN) dataset.  My last post on
<a class="reference external" href="posts/variational-autoencoders">variational autoencoders</a>
showed a simple example on the MNIST dataset but because it was so simple I
thought I might have missed some of the subtler points of VAEs -- boy was I
right!  The fact that I'm not really a computer vision guy nor a deep learning
guy didn't help either.  Through this exercise, I picked up some of the basics
in the "craft" of computer vision/deep learning area; there are a lot of subtle
points that are easy to gloss over if you're just reading someone else's
tutorial.  I'll share with you some of the details in the math (that I
initially got wrong) and also some of the implementation notes along with a
notebook that I used to train the VAE.  Please check out my previous post
on <a class="reference external" href="posts/variational-autoencoders">variational autoencoders</a> to
get some background.</p>
<p><em>Update 2017-08-09: I actually found a bug in my original code where I was
only using a small subset of the data!  I fixed it up in the notebooks and
I've added some inline comments below to say what I've changed.  For the most
part, things have stayed the same but the generated images are a bit blurry
because the dataset isn't so easy anymore.</em></p>
<p class="more"><a href="posts/a-variational-autoencoder-on-the-svnh-dataset/">Read more…</a></p>
</div>
    </div>
    </article><article class="h-entry post-text"><header><h1 class="p-name entry-title"><a href="posts/variational-autoencoders/" class="u-url">Variational Autoencoders</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                Brian Keng
            </span></p>
            <p class="dateline"><a href="posts/variational-autoencoders/" rel="bookmark"><time class="published dt-published" datetime="2017-05-30T08:19:36-04:00" title="2017-05-30 08:19">2017-05-30 08:19</time></a></p>
        </div>
    </header><div class="p-summary entry-summary">
    <div>
<p>This post is going to talk about an incredibly interesting unsupervised
learning method in machine learning called variational autoencoders.  It's main
claim to fame is in building generative models of complex distributions like
handwritten digits, faces, and image segments among others.  The really cool
thing about this topic is that it has firm roots in probability but uses a
function approximator (i.e.  neural networks) to approximate an otherwise
intractable problem.  As usual, I'll try to start with some background and
motivation, include a healthy does of math, and along the way try to convey
some of the intuition of why it works.  I've also annotated a
<a class="reference external" href="https://github.com/bjlkeng/sandbox/blob/master/notebooks/variational-autoencoder.ipynb">basic example</a>
so you can see how the math relates to an actual implementation.  I based much
of this post on Carl Doersch's <a class="reference external" href="https://arxiv.org/abs/1606.05908">tutorial</a>,
which has a great explanation on this whole topic, so make sure you check that
out too.</p>
<p class="more"><a href="posts/variational-autoencoders/">Read more…</a></p>
</div>
    </div>
    </article>
</div>

        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-1.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-3.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><script type="text/x-mathjax-config">
                    MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
                    </script>
</div>
            <div class="col-md-3 well">
            <p>
            I'm <a href="http://www.briankeng.com/about">Brian Keng</a>, 
            a former academic, current data scientist and engineer.  This is
            <a href=".">the place</a>
            where I write
            about all things technical.
            </p>
            <p>
            Twitter: <a href="http://www.twitter.com/bjlkeng">@bjlkeng</a>
            </p>

            <br><p>
            <a href="archive.html">Archive</a>
            </p>
            <p>
            <a href="categories/index.html">Tags</a>
            </p>
            <p>
            <a href="rss.xml">RSS feed</a>
            </p>

<!-- Begin MailChimp Signup Form -->
<hr>
<link href="//cdn-images.mailchimp.com/embedcode/classic-081711.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{clear:left; font:13px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
</style>
<div id="mc_embed_signup">
<form action="//briankeng.us10.list-manage.com/subscribe/post?u=cedf72ca8daa891e57f4379a0&amp;id=1f1563094f" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <b>Signup for Email Blog Posts</b>
    <div id="mc_embed_signup_scroll">
<div>
    <label for="mce-EMAIL"> Email Address </label>
    <input type="email" value="" name="EMAIL" class="required email form-control input-sm" id="mce-EMAIL">
</div>
    <div id="mce-responses" class="clear">
        <div class="response" id="mce-error-response" style="display:none"></div>
        <div class="response" id="mce-success-response" style="display:none"></div>
    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_cedf72ca8daa891e57f4379a0_1f1563094f" tabindex="-1" value=""></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="btn btn-default btn-xs"></div>
    </div>
</form>
</div>
<script type="text/javascript" src="//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js"></script><script type="text/javascript">(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script><!--End mc_embed_signup-->
</div>
        </div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2019         <a href="mailto:brian@briankeng.com">Brian Keng</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
        </footer>
</div>
</div>


            <script src="assets/js/jquery.min.js"></script><script src="assets/js/bootstrap.min.js"></script><script src="assets/js/moment-with-locales.min.js"></script><script src="assets/js/fancydates.js"></script><script src="assets/js/jquery.colorbox-min.js"></script><!-- <script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script> --><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-15314898-2', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
